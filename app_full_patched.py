# ============================================================
# üéØ STREAMLIT WEB APP V2.4 - UPLOAD + SUMMARIZE
# ============================================================
# ‚úÖ Upload danh s√°ch m√£ CK
# ‚úÖ T√≥m t·∫Øt extractive (t·ª´ V1.0)
# ‚úÖ Sentiment analysis
# ‚úÖ Risk detection
# ============================================================

import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime, timedelta, timezone
import time
import re
from urllib.parse import urljoin
import io

# ============================================================
# CONFIG
# ============================================================

st.set_page_config(
    page_title="C√†o Tin Ch·ª©ng Kho√°n V2.4",
    page_icon="üìà",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ============================================================
# CSS
# ============================================================

st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 1rem;
    }
    .upload-box {
        background-color: #e8f4f8;
        padding: 1.5rem;
        border-radius: 0.5rem;
        border: 2px dashed #1f77b4;
        margin: 1rem 0;
    }
    .severe-card {
        background-color: #ffe6e6;
        border-left: 5px solid #ff4444;
        padding: 1rem;
        margin: 0.5rem 0;
        border-radius: 0.3rem;
    }
    .warning-card {
        background-color: #fff8e6;
        border-left: 5px solid #ffaa00;
        padding: 1rem;
        margin: 0.5rem 0;
        border-radius: 0.3rem;
    }
    .positive-card {
        background-color: #e6ffe6;
        border-left: 5px solid #44ff44;
        padding: 1rem;
        margin: 0.5rem 0;
        border-radius: 0.3rem;
    }
</style>
""", unsafe_allow_html=True)

# ============================================================
# HELPER FUNCTIONS
# ============================================================

def load_default_stock_list():
    """Danh s√°ch m√£ m·∫∑c ƒë·ªãnh"""
    default_data = {
        'M√£ CK': ['SHS', 'PVS', 'NVB', 'VCS', 'BVS', 'CEO', 'VGC', 'PVC',
                  'LPB', 'EIB', 'BAB', 'OCB', 'HDG', 'PAN'],
        'S√†n': ['HNX']*8 + ['UPCoM']*6,
        'T√™n c√¥ng ty': ['Ch·ª©ng kho√°n SHS', 'Ch·ª©ng kho√°n PVS', 'Ng√¢n h√†ng NVB',
                        'Ch·ª©ng kho√°n VCS', 'Ch·ª©ng kho√°n BVS', 'T·∫≠p ƒëo√†n CEO',
                        'Viglacera', 'PVC', 'Ng√¢n h√†ng LPB', 'Ng√¢n h√†ng EIB',
                        'Ng√¢n h√†ng BAB', 'Ng√¢n h√†ng OCB', 'T·∫≠p ƒëo√†n HDG', 'PAN Group']
    }
    return pd.DataFrame(default_data)

def parse_stock_file(uploaded_file):
    """Parse Excel/CSV file"""
    try:
        if uploaded_file.name.endswith('.csv'):
            df = pd.read_csv(uploaded_file)
        else:
            df = pd.read_excel(uploaded_file)
        
        df.columns = df.columns.str.strip().str.lower()
        
        column_mapping = {
            'm√£ ck': 'M√£ CK', 'ma ck': 'M√£ CK', 'm√£': 'M√£ CK', 'code': 'M√£ CK',
            's√†n': 'S√†n', 'san': 'S√†n', 'exchange': 'S√†n',
            't√™n c√¥ng ty': 'T√™n c√¥ng ty', 'ten cong ty': 'T√™n c√¥ng ty', 'name': 'T√™n c√¥ng ty',
        }
        
        for old_col, new_col in column_mapping.items():
            if old_col in df.columns:
                df.rename(columns={old_col: new_col}, inplace=True)
        
        required_cols = ['M√£ CK', 'S√†n']
        missing_cols = [col for col in required_cols if col not in df.columns]
        
        if missing_cols:
            return None, f"Thi·∫øu c√°c c·ªôt: {', '.join(missing_cols)}"
        
        if 'T√™n c√¥ng ty' not in df.columns:
            df['T√™n c√¥ng ty'] = ''
        
        df['M√£ CK'] = df['M√£ CK'].astype(str).str.strip().str.upper()
        df['S√†n'] = df['S√†n'].astype(str).str.strip().str.upper()
        df['T√™n c√¥ng ty'] = df['T√™n c√¥ng ty'].astype(str).str.strip()
        
        df = df[df['S√†n'].isin(['HNX', 'UPCOM'])]
        df['S√†n'] = df['S√†n'].replace('UPCOM', 'UPCoM')
        df = df.drop_duplicates(subset=['M√£ CK'])
        
        return df, None
        
    except Exception as e:
        return None, f"L·ªói ƒë·ªçc file: {str(e)}"

def create_sample_excel():
    """T·∫°o file Excel m·∫´u"""
    sample_data = {
        'M√£ CK': ['SHS', 'PVS', 'NVB', 'LPB', 'EIB', 'CEO'],
        'S√†n': ['HNX', 'HNX', 'HNX', 'UPCoM', 'UPCoM', 'HNX'],
        'T√™n c√¥ng ty': ['Ch·ª©ng kho√°n S√†i G√≤n - H√† N·ªôi', 'Ch·ª©ng kho√°n D·∫ßu kh√≠', 
                        'Ng√¢n h√†ng Qu·ªëc d√¢n', 'Ng√¢n h√†ng L·ªôc Ph√°t', 
                        'Ng√¢n h√†ng Xu·∫•t nh·∫≠p kh·∫©u', 'T·∫≠p ƒëo√†n CEO']
    }
    df = pd.DataFrame(sample_data)
    
    buffer = io.BytesIO()
    with pd.ExcelWriter(buffer, engine='openpyxl') as writer:
        df.to_excel(writer, index=False, sheet_name='Danh s√°ch m√£')
    
    return buffer.getvalue()

# ============================================================
# KEYWORD RISK DETECTOR
# ============================================================

class KeywordRiskDetector:
    def __init__(self):
        self.keywords_db = {
            # A. N·ªôi b·ªô & Qu·∫£n tr·ªã
            "l√£nh ƒë·∫°o b·ªã b·∫Øt": {"category": "A. N·ªôi b·ªô", "severity": "severe", "score": -95, "violation": "I.2, II.A"},
            "l√£nh ƒë·∫°o b·ªè tr·ªën": {"category": "A. N·ªôi b·ªô", "severity": "severe", "score": -95, "violation": "I.2, II.A"},
            "c·ªï ƒë√¥ng l·ªõn b√°n chui": {"category": "A. N·ªôi b·ªô", "severity": "severe", "score": -85, "violation": "I.1, II.A"},
            "ch·ªß t·ªãch b·∫•t ng·ªù tho√°i h·∫øt v·ªën": {"category": "A. N·ªôi b·ªô", "severity": "severe", "score": -85, "violation": "I.1, II.A"},
            
            # B. T√†i ch√≠nh
            "b·∫•t ng·ªù b√°o l·ªó": {"category": "B. T√†i ch√≠nh", "severity": "severe", "score": -80, "violation": "I.4, II.B"},
            "√¢m v·ªën ch·ªß": {"category": "B. T√†i ch√≠nh", "severity": "severe", "score": -90, "violation": "II.B"},
            "m·∫•t kh·∫£ nƒÉng thanh to√°n": {"category": "B. T√†i ch√≠nh", "severity": "severe", "score": -90, "violation": "II.B"},
            "n·ª£ x·∫•u b·∫•t th∆∞·ªùng": {"category": "B. T√†i ch√≠nh", "severity": "severe", "score": -80, "violation": "II.B"},
            
            # C. Thao t√∫ng & Bi·∫øn ƒë·ªông gi√° b·∫•t th∆∞·ªùng
            "ƒë·ªôi l√°i l√†m gi√°": {"category": "C. Thao t√∫ng", "severity": "severe", "score": -95, "violation": "I.3, II.C"},
            "tƒÉng tr·∫ßn li√™n ti·∫øp": {"category": "C. Thao t√∫ng", "severity": "warning", "score": -60, "violation": "I.2, II.C"},
            "gi·∫£m s√†n li√™n t·ª•c": {"category": "C. Thao t√∫ng", "severity": "warning", "score": -70, "violation": "I.2, II.C"},
            "b·ªëc ƒë·∫ßu": {"category": "C. Thao t√∫ng", "severity": "warning", "score": -65, "violation": "I.2, I.3, II.C"},
            "k·ªãch tr·∫ßn": {"category": "C. Thao t√∫ng", "severity": "warning", "score": -65, "violation": "I.2, I.3, II.C"},
            "r·ªõt ƒë√°y": {"category": "C. Thao t√∫ng", "severity": "warning", "score": -70, "violation": "I.2, I.3, II.C"},
            "c·ªï phi·∫øu tƒÉng phi m√£": {"category": "C. Thao t√∫ng", "severity": "warning", "score": -65, "violation": "I.2, I.4, II.C"},
            "tƒÉng d·ª±ng ƒë·ª©ng": {"category": "C. Thao t√∫ng", "severity": "warning", "score": -60, "violation": "I.2, II.C"},
            "kh·ªëi l∆∞·ª£ng tƒÉng b·∫•t th∆∞·ªùng": {"category": "C. Thao t√∫ng", "severity": "warning", "score": -65, "violation": "I.6, II.C"},
            "giao d·ªãch n·ªôi gi√°n": {"category": "C. Thao t√∫ng", "severity": "severe", "score": -90, "violation": "I.1, II.C"},
            
            # D. M&A
            "ni√™m y·∫øt c·ª≠a sau": {"category": "D. M&A", "severity": "severe", "score": -85, "violation": "I.5, II.D"},
            "th√¢u t√≥m": {"category": "D. M&A", "severity": "warning", "score": -50, "violation": "I.5, II.D"},
            
            # E. Ph√°p l√Ω
            "c√¥ng an ƒëi·ªÅu tra": {"category": "E. Ph√°p l√Ω", "severity": "severe", "score": -90, "violation": "II.E"},
            "kh·ªüi t·ªë l√£nh ƒë·∫°o": {"category": "E. Ph√°p l√Ω", "severity": "severe", "score": -95, "violation": "II.E"},
            "gian l·∫≠n t√†i ch√≠nh": {"category": "E. Ph√°p l√Ω", "severity": "severe", "score": -95, "violation": "II.E"},
            
            # F. S·ª± ki·ªán b√™n ngo√†i
            "ch√°y nh√† x∆∞·ªüng": {"category": "F. S·ª± ki·ªán ngo√†i", "severity": "severe", "score": -75, "violation": "II.F"},
            "b·ªã thu h·ªìi gi·∫•y ph√©p": {"category": "F. S·ª± ki·ªán ngo√†i", "severity": "severe", "score": -90, "violation": "II.F"},
            
            # T√≠ch c·ª±c
            "l·ª£i nhu·∫≠n tƒÉng": {"category": "T√≠ch c·ª±c", "severity": "positive", "score": 70, "violation": ""},
            "tƒÉng tr∆∞·ªüng m·∫°nh": {"category": "T√≠ch c·ª±c", "severity": "positive", "score": 65, "violation": ""},
            "doanh thu k·ª∑ l·ª•c": {"category": "T√≠ch c·ª±c", "severity": "positive", "score": 75, "violation": ""},
        }
    
    def analyze(self, text):
        text_lower = text.lower()
        found_keywords = []
        total_score = 0
        categories = set()
        violations = set()
        max_severity = "normal"
        
        for keyword, info in self.keywords_db.items():
            if keyword in text_lower:
                found_keywords.append({
                    "keyword": keyword,
                    "category": info["category"],
                    "severity": info["severity"],
                    "score": info["score"],
                    "violation": info["violation"]
                })
                total_score += info["score"]
                categories.add(info["category"])
                if info["violation"]:
                    violations.add(info["violation"])
                
                if info["severity"] == "severe":
                    max_severity = "severe"
                elif info["severity"] == "warning" and max_severity != "severe":
                    max_severity = "warning"
                elif info["severity"] == "positive" and max_severity == "normal":
                    max_severity = "positive"
        
        return {
            "keywords": found_keywords,
            "total_score": total_score,
            "severity": max_severity,
            "categories": list(categories),
            "violations": ", ".join(sorted(violations))
        }

# ============================================================
# SENTIMENT ANALYZER
# ============================================================

class SimpleSentimentAnalyzer:
    def __init__(self):
        self.keyword_detector = KeywordRiskDetector()
        self.positive_words = ['tƒÉng', 'tƒÉng tr∆∞·ªüng', 'l·ª£i nhu·∫≠n', 'th√†nh c√¥ng', 't·ªët', 'cao', 'm·∫°nh', 'v∆∞·ª£t']
        self.negative_words = ['gi·∫£m', 's·ª•t gi·∫£m', 'l·ªó', 'thua l·ªó', 'kh√≥ khƒÉn', 'ti√™u c·ª±c', 'suy gi·∫£m']
    
    def analyze_sentiment(self, title, content):
        text = (title + " " + content).lower()
        keyword_analysis = self.keyword_detector.analyze(title + " " + content)
        
        pos_count = sum(1 for word in self.positive_words if word in text)
        neg_count = sum(1 for word in self.negative_words if word in text)
        
        base_score = 50 + (pos_count * 5) - (neg_count * 5)
        
        if keyword_analysis["severity"] == "severe":
            final_score = min(20, base_score + keyword_analysis["total_score"])
        elif keyword_analysis["severity"] == "warning":
            final_score = min(40, base_score + keyword_analysis["total_score"] * 0.7)
        elif keyword_analysis["severity"] == "positive":
            final_score = max(60, base_score + keyword_analysis["total_score"])
        else:
            final_score = base_score
        
        final_score = max(0, min(100, final_score))
        
        if final_score >= 60:
            label = "T√≠ch c·ª±c"
        elif final_score >= 40:
            label = "Trung l·∫≠p"
        else:
            label = "Ti√™u c·ª±c"
        
        if keyword_analysis["severity"] == "severe":
            risk_level = "Nghi√™m tr·ªçng"
        elif keyword_analysis["severity"] == "warning":
            risk_level = "C·∫£nh b√°o"
        elif keyword_analysis["severity"] == "positive":
            risk_level = "T√≠ch c·ª±c"
        else:
            risk_level = "B√¨nh th∆∞·ªùng"
        
        return {
            "sentiment_score": round(final_score, 1),
            "sentiment_label": label,
            "risk_level": risk_level,
            "keywords": keyword_analysis["keywords"],
            "categories": ", ".join(keyword_analysis["categories"]) if keyword_analysis["categories"] else "",
            "violations": keyword_analysis["violations"]
        }

# ============================================================
# STOCK SCRAPER
# ============================================================

class StockScraperWeb:
    def __init__(self, stock_df, time_filter_hours=24):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept-Language': 'vi-VN,vi;q=0.9,en;q=0.8',
        }
        self.all_articles = []
        self.session = requests.Session()
        self.time_filter_hours = time_filter_hours
        
        self.vietnam_tz = timezone(timedelta(hours=7))
        self.cutoff_time = datetime.now(self.vietnam_tz) - timedelta(hours=time_filter_hours)
        
        self.sentiment_analyzer = SimpleSentimentAnalyzer()
        
        # Load stock list
        self.stock_df = stock_df
        self.hnx_stocks = set(stock_df[stock_df['S√†n'] == 'HNX']['M√£ CK'].tolist())
        self.upcom_stocks = set(stock_df[stock_df['S√†n'] == 'UPCoM']['M√£ CK'].tolist())
        
        self.code_to_name = dict(zip(stock_df['M√£ CK'], stock_df['T√™n c√¥ng ty']))
        
        self.name_to_code = {}
        for code, name in self.code_to_name.items():
            if name:
                words = name.lower().split()
                for word in words:
                    if len(word) > 3:
                        if word not in self.name_to_code:
                            self.name_to_code[word] = []
                        self.name_to_code[word].append(code)
        
        self.stock_to_exchange = {}
        for code in self.hnx_stocks:
            self.stock_to_exchange[code] = 'HNX'
        for code in self.upcom_stocks:
            self.stock_to_exchange[code] = 'UPCoM'
        
        self.stats = {
            'total_crawled': 0,
            'hnx_found': 0,
            'upcom_found': 0,
            'severe_risk': 0,
            'warning_risk': 0,
            'found_by_code': 0,
            'found_by_name': 0
        }
    
    def clean_text(self, text):
        """L√†m s·∫°ch text - t·ª´ V1.0"""
        if not text:
            return ""
        text = re.sub(r'[^\w\s.,;:!?()%\-\+\/\"\'√†√°·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√®√©·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±·ª≥√Ω·ª∑·ªπ·ªµƒë√Ä√Å·∫¢√É·∫†ƒÇ·∫Æ·∫∞·∫≤·∫¥·∫∂√Ç·∫§·∫¶·∫®·∫™·∫¨√à√â·∫∫·∫º·∫∏√ä·∫æ·ªÄ·ªÇ·ªÑ·ªÜ√å√ç·ªàƒ®·ªä√í√ì·ªé√ï·ªå√î·ªê·ªí·ªî·ªñ·ªò∆†·ªö·ªú·ªû·ª†·ª¢√ô√ö·ª¶≈®·ª§∆Ø·ª®·ª™·ª¨·ªÆ·ª∞·ª≤√ù·ª∂·ª∏·ª¥ƒê]', ' ', text)
        text = re.sub(r'\s+', ' ', text)
        return text.strip()
    
    def advanced_summarize(self, content, title, max_sentences=4):
        """T√≥m t·∫Øt EXTRACTIVE - t·ª´ V1.0"""
        content = self.clean_text(content)
        title = self.clean_text(title)
        
        if not content or len(content) < 100:
            return content
        
        full_text = title + ". " + content
        sentences = re.split(r'[.!?]+', full_text)
        sentences = [s.strip() for s in sentences if len(s.strip()) > 30]
        
        if len(sentences) <= max_sentences:
            return '. '.join(sentences) + '.'
        
        important_keywords = {
            'tƒÉng': 3, 'gi·∫£m': 3, 'tƒÉng tr∆∞·ªüng': 3,
            'l·ª£i nhu·∫≠n': 4, 'doanh thu': 4, 'l·ªó': 3,
            't·ª∑ ƒë·ªìng': 3, 'ngh√¨n t·ª∑': 4,
            'c·ªï phi·∫øu': 3, 'ni√™m y·∫øt': 3,
            'giao d·ªãch': 2, 'thanh kho·∫£n': 3,
            'qu√Ω': 3, 'nƒÉm': 2,
            'ph√°t h√†nh': 3, 'tr√°i phi·∫øu': 3,
            'ƒë·∫ßu t∆∞': 2, 'v·ªën': 3,
        }
        
        scored_sentences = []
        for i, sentence in enumerate(sentences):
            score = 0
            sentence_lower = sentence.lower()
            
            if i == 0:
                score += 5
            elif i == 1:
                score += 3
            elif i < 5:
                score += 1
            
            for keyword, weight in important_keywords.items():
                if keyword in sentence_lower:
                    score += weight
            
            numbers = re.findall(r'\d+(?:[.,]\d+)*', sentence)
            if numbers:
                score += len(numbers)
                if any(num for num in numbers if len(num.replace('.', '').replace(',', '')) >= 4):
                    score += 2
            
            if '%' in sentence:
                score += 3
            
            word_count = len(sentence.split())
            if 12 <= word_count <= 35:
                score += 2
            elif word_count < 8 or word_count > 50:
                score -= 1
            
            for code in list(self.hnx_stocks) + list(self.upcom_stocks):
                if code in sentence.upper():
                    score += 3
                    break
            
            scored_sentences.append((sentence, score, i))
        
        scored_sentences.sort(key=lambda x: x[1], reverse=True)
        top_sentences = scored_sentences[:max_sentences]
        top_sentences.sort(key=lambda x: x[2])
        
        summary = '. '.join([s[0] for s in top_sentences])
        if not summary.endswith('.'):
            summary += '.'
        
        summary = self.clean_text(summary)
        return summary
    
    def is_generic_news(self, title):
        """Ki·ªÉm tra xem c√≥ ph·∫£i tin t·ª©c chung kh√¥ng"""
        title_lower = title.lower()
        
        generic_patterns = [
            r'l·ªãch\s+s·ª±\s+ki·ªán',
            r'tin\s+v·∫Øn',
            r't·ªïng\s+h·ª£p',
            r'ƒëi·ªÉm\s+tin',
            r'nh·ªãp\s+ƒë·∫≠p',
            r'th·ªã\s+tr∆∞·ªùng\s+ng√†y',
            r'ch·ª©ng\s+kho√°n\s+ng√†y',
            r'phi√™n\s+giao\s+d·ªãch',
            r'c√°c\s+tin\s+t·ª©c',
            r'tin\s+nhanh',
            r'c·∫≠p\s+nh·∫≠t',
            r'ƒëi·ªÉm\s+l·∫°i',
        ]
        
        for pattern in generic_patterns:
            if re.search(pattern, title_lower):
                return True
        
        return False
    
    def extract_stock(self, text):
        """Tr√≠ch xu·∫•t m√£ CK - N√ÇNG CAO: Y√äU C·∫¶U T√çN HI·ªÜU NH·∫¨N DI·ªÜN"""
        text_upper = text.upper()
        text_lower = text.lower()
        
        # ============================================================
        # DANH S√ÅCH M√É "NGUY HI·ªÇM" - CH·ªà NH·∫¨N DI·ªÜN KHI C√ì T√çN HI·ªÜU R√ï R√ÄNG
        # ============================================================
        RISKY_CODES = {'THU', 'TIN', 'TOP', 'HAI', 'LAI', 'CEO', 'CCP'}
        
        # ============================================================
        # B∆Ø·ªöC 1: T√åM THEO C√ÅC PATTERN R√ï R√ÄNG (∆ØU TI√äN CAO NH·∫§T)
        # ============================================================
        
        # Pattern nh√≥m 1: Trong ngo·∫∑c v·ªõi s√†n
        patterns_with_exchange = [
            r'\((?:UPCOM|HNX):\s*([A-Z]{3})\)',           # (UPCOM: ABC), (HNX: ABC)
            r'\(([A-Z]{3})\s*[-‚Äì]\s*(?:UPCOM|HNX)\)',     # (ABC - UPCOM), (ABC - HNX)
            r'\(([A-Z]{3})\s*,\s*(?:UPCOM|HNX)\)',        # (ABC, UPCOM), (ABC, HNX)
            r'\((?:UPCOM|HNX)\s*[-‚Äì]\s*([A-Z]{3})\)',     # (UPCOM - ABC), (HNX - ABC)
        ]
        
        for pattern in patterns_with_exchange:
            match = re.search(pattern, text_upper)
            if match:
                code = match.group(1)
                if code in self.hnx_stocks:
                    return code, 'HNX', 'code'
                elif code in self.upcom_stocks:
                    return code, 'UPCoM', 'code'
        
        # Pattern nh√≥m 2: C√≥ t·ª´ kh√≥a "m√£"
        patterns_with_ma = [
            r'M√É\s*(?:CK|CH·ª®NG KHO√ÅN|CP)?:?\s*([A-Z]{3})\b',    # M√£ CK: ABC, M√£: ABC
            r'M√É\s+([A-Z]{3})\b',                                # M√£ ABC
            r'\(M√É:?\s*([A-Z]
