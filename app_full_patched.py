# ============================================================
# üéØ STREAMLIT WEB APP V2.4 - UPLOAD + SUMMARIZE
# ============================================================
# ‚úÖ Upload danh s√°ch m√£ CK
# ‚úÖ T√≥m t·∫Øt extractive (t·ª´ V1.0)
# ‚úÖ Sentiment analysis
# ‚úÖ Risk detection
# ============================================================

import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime, timedelta, timezone
import time
import re
from urllib.parse import urljoin
import io

# ============================================================
# CONFIG
# ============================================================

st.set_page_config(
    page_title="C√†o Tin Ch·ª©ng Kho√°n V2.4",
    page_icon="üìà",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ============================================================
# CSS
# ============================================================

st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 1rem;
    }
    .upload-box {
        background-color: #e8f4f8;
        padding: 1.5rem;
        border-radius: 0.5rem;
        border: 2px dashed #1f77b4;
        margin: 1rem 0;
    }
    .severe-card {
        background-color: #ffe6e6;
        border-left: 5px solid #ff4444;
        padding: 1rem;
        margin: 0.5rem 0;
        border-radius: 0.3rem;
    }
    .warning-card {
        background-color: #fff8e6;
        border-left: 5px solid #ffaa00;
        padding: 1rem;
        margin: 0.5rem 0;
        border-radius: 0.3rem;
    }
    .positive-card {
        background-color: #e6ffe6;
        border-left: 5px solid #44ff44;
        padding: 1rem;
        margin: 0.5rem 0;
        border-radius: 0.3rem;
    }
</style>
""", unsafe_allow_html=True)

# ============================================================
# HELPER FUNCTIONS
# ============================================================

def load_default_stock_list():
    """Danh s√°ch m√£ m·∫∑c ƒë·ªãnh"""
    default_data = {
        'M√£ CK': ['SHS', 'PVS', 'NVB', 'VCS', 'BVS', 'CEO', 'VGC', 'PVC',
                  'LPB', 'EIB', 'BAB', 'OCB', 'HDG', 'PAN'],
        'S√†n': ['HNX']*8 + ['UPCoM']*6,
        'T√™n c√¥ng ty': ['Ch·ª©ng kho√°n SHS', 'Ch·ª©ng kho√°n PVS', 'Ng√¢n h√†ng NVB',
                        'Ch·ª©ng kho√°n VCS', 'Ch·ª©ng kho√°n BVS', 'T·∫≠p ƒëo√†n CEO',
                        'Viglacera', 'PVC', 'Ng√¢n h√†ng LPB', 'Ng√¢n h√†ng EIB',
                        'Ng√¢n h√†ng BAB', 'Ng√¢n h√†ng OCB', 'T·∫≠p ƒëo√†n HDG', 'PAN Group']
    }
    return pd.DataFrame(default_data)

def parse_stock_file(uploaded_file):
    """Parse Excel/CSV file - H·ªñ TR·ª¢ T·∫§T C·∫¢ C√ÅC S√ÄN"""
    try:
        if uploaded_file.name.endswith('.csv'):
            df = pd.read_csv(uploaded_file)
        else:
            df = pd.read_excel(uploaded_file)
        
        df.columns = df.columns.str.strip().str.lower()
        
        column_mapping = {
            'm√£ ck': 'M√£ CK', 'ma ck': 'M√£ CK', 'm√£': 'M√£ CK', 'code': 'M√£ CK',
            's√†n': 'S√†n', 'san': 'S√†n', 'exchange': 'S√†n',
            't√™n c√¥ng ty': 'T√™n c√¥ng ty', 'ten cong ty': 'T√™n c√¥ng ty', 'name': 'T√™n c√¥ng ty',
        }
        
        for old_col, new_col in column_mapping.items():
            if old_col in df.columns:
                df.rename(columns={old_col: new_col}, inplace=True)
        
        required_cols = ['M√£ CK', 'S√†n']
        missing_cols = [col for col in required_cols if col not in df.columns]
        
        if missing_cols:
            return None, f"Thi·∫øu c√°c c·ªôt: {', '.join(missing_cols)}"
        
        if 'T√™n c√¥ng ty' not in df.columns:
            df['T√™n c√¥ng ty'] = ''
        
        df['M√£ CK'] = df['M√£ CK'].astype(str).str.strip().str.upper()
        df['S√†n'] = df['S√†n'].astype(str).str.strip().str.upper()
        df['T√™n c√¥ng ty'] = df['T√™n c√¥ng ty'].astype(str).str.strip()
        
        # Chu·∫©n h√≥a t√™n s√†n
        df['S√†n'] = df['S√†n'].replace({
            'UPCOM': 'UPCoM',
            'HSX': 'HOSE',
            'HOSTC': 'HOSE'
        })
        
        # GI·ªÆ L·∫†I T·∫§T C·∫¢ C√ÅC S√ÄN (HOSE, HNX, UPCoM)
        valid_exchanges = ['HNX', 'UPCoM', 'HOSE']
        df = df[df['S√†n'].isin(valid_exchanges)]
        df = df.drop_duplicates(subset=['M√£ CK'])
        
        if len(df) == 0:
            return None, "Kh√¥ng t√¨m th·∫•y m√£ CK h·ª£p l·ªá trong file"
        
        return df, None
        
    except Exception as e:
        return None, f"L·ªói ƒë·ªçc file: {str(e)}"

def create_sample_excel():
    """T·∫°o file Excel m·∫´u - BAO G·ªíM C·∫¢ 3 S√ÄN"""
    sample_data = {
        'M√£ CK': ['VCB', 'FPT', 'VNM', 'SHS', 'PVS', 'NVB', 'LPB', 'EIB', 'CEO'],
        'S√†n': ['HOSE', 'HOSE', 'HOSE', 'HNX', 'HNX', 'HNX', 'UPCoM', 'UPCoM', 'HNX'],
        'T√™n c√¥ng ty': ['Ng√¢n h√†ng Vietcombank', 'FPT Corporation', 'Vinamilk',
                        'Ch·ª©ng kho√°n S√†i G√≤n - H√† N·ªôi', 'Ch·ª©ng kho√°n D·∫ßu kh√≠', 
                        'Ng√¢n h√†ng Qu·ªëc d√¢n', 'Ng√¢n h√†ng L·ªôc Ph√°t', 
                        'Ng√¢n h√†ng Xu·∫•t nh·∫≠p kh·∫©u', 'T·∫≠p ƒëo√†n CEO']
    }
    df = pd.DataFrame(sample_data)
    
    buffer = io.BytesIO()
    with pd.ExcelWriter(buffer, engine='openpyxl') as writer:
        df.to_excel(writer, index=False, sheet_name='Danh s√°ch m√£')
    
    return buffer.getvalue()

# ============================================================
# KEYWORD RISK DETECTOR
# ============================================================

class KeywordRiskDetector:
    def __init__(self):
        self.keywords_db = {
            # A. N·ªôi b·ªô & Qu·∫£n tr·ªã
            "l√£nh ƒë·∫°o b·ªã b·∫Øt": {"category": "A. N·ªôi b·ªô", "severity": "severe", "score": -95, "violation": "I.2, II.A"},
            "l√£nh ƒë·∫°o b·ªè tr·ªën": {"category": "A. N·ªôi b·ªô", "severity": "severe", "score": -95, "violation": "I.2, II.A"},
            "c·ªï ƒë√¥ng l·ªõn b√°n chui": {"category": "A. N·ªôi b·ªô", "severity": "severe", "score": -85, "violation": "I.1, II.A"},
            "ch·ªß t·ªãch b·∫•t ng·ªù tho√°i h·∫øt v·ªën": {"category": "A. N·ªôi b·ªô", "severity": "severe", "score": -85, "violation": "I.1, II.A"},
            
            # B. T√†i ch√≠nh
            "b·∫•t ng·ªù b√°o l·ªó": {"category": "B. T√†i ch√≠nh", "severity": "severe", "score": -80, "violation": "I.4, II.B"},
            "√¢m v·ªën ch·ªß": {"category": "B. T√†i ch√≠nh", "severity": "severe", "score": -90, "violation": "II.B"},
            "m·∫•t kh·∫£ nƒÉng thanh to√°n": {"category": "B. T√†i ch√≠nh", "severity": "severe", "score": -90, "violation": "II.B"},
            "n·ª£ x·∫•u b·∫•t th∆∞·ªùng": {"category": "B. T√†i ch√≠nh", "severity": "severe", "score": -80, "violation": "II.B"},
            
            # C. Thao t√∫ng & Bi·∫øn ƒë·ªông gi√° b·∫•t th∆∞·ªùng
            "ƒë·ªôi l√°i l√†m gi√°": {"category": "C. Thao t√∫ng", "severity": "severe", "score": -95, "violation": "I.3, II.C"},
            "tƒÉng tr·∫ßn li√™n ti·∫øp": {"category": "C. Thao t√∫ng", "severity": "warning", "score": -60, "violation": "I.2, II.C"},
            "gi·∫£m s√†n li√™n t·ª•c": {"category": "C. Thao t√∫ng", "severity": "warning", "score": -70, "violation": "I.2, II.C"},
            "b·ªëc ƒë·∫ßu": {"category": "C. Thao t√∫ng", "severity": "warning", "score": -65, "violation": "I.2, I.3, II.C"},
            "k·ªãch tr·∫ßn": {"category": "C. Thao t√∫ng", "severity": "warning", "score": -65, "violation": "I.2, I.3, II.C"},
            "r·ªõt ƒë√°y": {"category": "C. Thao t√∫ng", "severity": "warning", "score": -70, "violation": "I.2, I.3, II.C"},
            "c·ªï phi·∫øu tƒÉng phi m√£": {"category": "C. Thao t√∫ng", "severity": "warning", "score": -65, "violation": "I.2, I.4, II.C"},
            "tƒÉng d·ª±ng ƒë·ª©ng": {"category": "C. Thao t√∫ng", "severity": "warning", "score": -60, "violation": "I.2, II.C"},
            "kh·ªëi l∆∞·ª£ng tƒÉng b·∫•t th∆∞·ªùng": {"category": "C. Thao t√∫ng", "severity": "warning", "score": -65, "violation": "I.6, II.C"},
            "giao d·ªãch n·ªôi gi√°n": {"category": "C. Thao t√∫ng", "severity": "severe", "score": -90, "violation": "I.1, II.C"},
            
            # D. M&A
            "ni√™m y·∫øt c·ª≠a sau": {"category": "D. M&A", "severity": "severe", "score": -85, "violation": "I.5, II.D"},
            "th√¢u t√≥m": {"category": "D. M&A", "severity": "warning", "score": -50, "violation": "I.5, II.D"},
            
            # E. Ph√°p l√Ω
            "c√¥ng an ƒëi·ªÅu tra": {"category": "E. Ph√°p l√Ω", "severity": "severe", "score": -90, "violation": "II.E"},
            "kh·ªüi t·ªë l√£nh ƒë·∫°o": {"category": "E. Ph√°p l√Ω", "severity": "severe", "score": -95, "violation": "II.E"},
            "gian l·∫≠n t√†i ch√≠nh": {"category": "E. Ph√°p l√Ω", "severity": "severe", "score": -95, "violation": "II.E"},
            
            # F. S·ª± ki·ªán b√™n ngo√†i
            "ch√°y nh√† x∆∞·ªüng": {"category": "F. S·ª± ki·ªán ngo√†i", "severity": "severe", "score": -75, "violation": "II.F"},
            "b·ªã thu h·ªìi gi·∫•y ph√©p": {"category": "F. S·ª± ki·ªán ngo√†i", "severity": "severe", "score": -90, "violation": "II.F"},
            
            # T√≠ch c·ª±c
            "l·ª£i nhu·∫≠n tƒÉng": {"category": "T√≠ch c·ª±c", "severity": "positive", "score": 70, "violation": ""},
            "tƒÉng tr∆∞·ªüng m·∫°nh": {"category": "T√≠ch c·ª±c", "severity": "positive", "score": 65, "violation": ""},
            "doanh thu k·ª∑ l·ª•c": {"category": "T√≠ch c·ª±c", "severity": "positive", "score": 75, "violation": ""},
        }
    
    def analyze(self, text):
        text_lower = text.lower()
        found_keywords = []
        total_score = 0
        categories = set()
        violations = set()
        max_severity = "normal"
        
        for keyword, info in self.keywords_db.items():
            if keyword in text_lower:
                found_keywords.append({
                    "keyword": keyword,
                    "category": info["category"],
                    "severity": info["severity"],
                    "score": info["score"],
                    "violation": info["violation"]
                })
                total_score += info["score"]
                categories.add(info["category"])
                if info["violation"]:
                    violations.add(info["violation"])
                
                if info["severity"] == "severe":
                    max_severity = "severe"
                elif info["severity"] == "warning" and max_severity != "severe":
                    max_severity = "warning"
                elif info["severity"] == "positive" and max_severity == "normal":
                    max_severity = "positive"
        
        return {
            "keywords": found_keywords,
            "total_score": total_score,
            "severity": max_severity,
            "categories": list(categories),
            "violations": ", ".join(sorted(violations))
        }

# ============================================================
# SENTIMENT ANALYZER
# ============================================================

class SimpleSentimentAnalyzer:
    def __init__(self):
        self.keyword_detector = KeywordRiskDetector()
        self.positive_words = ['tƒÉng', 'tƒÉng tr∆∞·ªüng', 'l·ª£i nhu·∫≠n', 'th√†nh c√¥ng', 't·ªët', 'cao', 'm·∫°nh', 'v∆∞·ª£t']
        self.negative_words = ['gi·∫£m', 's·ª•t gi·∫£m', 'l·ªó', 'thua l·ªó', 'kh√≥ khƒÉn', 'ti√™u c·ª±c', 'suy gi·∫£m']
    
    def analyze_sentiment(self, title, content):
        text = (title + " " + content).lower()
        keyword_analysis = self.keyword_detector.analyze(title + " " + content)
        
        pos_count = sum(1 for word in self.positive_words if word in text)
        neg_count = sum(1 for word in self.negative_words if word in text)
        
        base_score = 50 + (pos_count * 5) - (neg_count * 5)
        
        if keyword_analysis["severity"] == "severe":
            final_score = min(20, base_score + keyword_analysis["total_score"])
        elif keyword_analysis["severity"] == "warning":
            final_score = min(40, base_score + keyword_analysis["total_score"] * 0.7)
        elif keyword_analysis["severity"] == "positive":
            final_score = max(60, base_score + keyword_analysis["total_score"])
        else:
            final_score = base_score
        
        final_score = max(0, min(100, final_score))
        
        if final_score >= 60:
            label = "T√≠ch c·ª±c"
        elif final_score >= 40:
            label = "Trung l·∫≠p"
        else:
            label = "Ti√™u c·ª±c"
        
        if keyword_analysis["severity"] == "severe":
            risk_level = "Nghi√™m tr·ªçng"
        elif keyword_analysis["severity"] == "warning":
            risk_level = "C·∫£nh b√°o"
        elif keyword_analysis["severity"] == "positive":
            risk_level = "T√≠ch c·ª±c"
        else:
            risk_level = "B√¨nh th∆∞·ªùng"
        
        return {
            "sentiment_score": round(final_score, 1),
            "sentiment_label": label,
            "risk_level": risk_level,
            "keywords": keyword_analysis["keywords"],
            "categories": ", ".join(keyword_analysis["categories"]) if keyword_analysis["categories"] else "",
            "violations": keyword_analysis["violations"]
        }

# ============================================================
# STOCK SCRAPER
# ============================================================

class StockScraperWeb:
    def __init__(self, stock_df, time_filter_hours=24):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept-Language': 'vi-VN,vi;q=0.9,en;q=0.8',
        }
        self.all_articles = []
        self.session = requests.Session()
        self.time_filter_hours = time_filter_hours
        
        self.vietnam_tz = timezone(timedelta(hours=7))
        self.cutoff_time = datetime.now(self.vietnam_tz) - timedelta(hours=time_filter_hours)
        
        self.sentiment_analyzer = SimpleSentimentAnalyzer()
        
        # Load stock list - TO√ÄN B·ªò TH·ªä TR∆Ø·ªúNG
        self.stock_df = stock_df
        
        # Chu·∫©n h√≥a c·ªôt S√†n
        self.stock_df['S√†n'] = self.stock_df['S√†n'].str.strip().str.upper()
        self.stock_df['S√†n'] = self.stock_df['S√†n'].replace({
            'UPCOM': 'UPCoM',
            'HSX': 'HOSE',
            'HOSTC': 'HOSE'
        })
        
        # T√°ch theo s√†n
        self.hnx_stocks = set(stock_df[stock_df['S√†n'] == 'HNX']['M√£ CK'].tolist())
        self.upcom_stocks = set(stock_df[stock_df['S√†n'] == 'UPCoM']['M√£ CK'].tolist())
        self.hose_stocks = set(stock_df[stock_df['S√†n'] == 'HOSE']['M√£ CK'].tolist())
        
        # TO√ÄN B·ªò M√É (d√πng ƒë·ªÉ nh·∫≠n di·ªán)
        self.all_stocks = self.hnx_stocks | self.upcom_stocks | self.hose_stocks
        
        self.code_to_name = dict(zip(stock_df['M√£ CK'], stock_df['T√™n c√¥ng ty']))
        
        self.name_to_code = {}
        for code, name in self.code_to_name.items():
            if name:
                words = name.lower().split()
                for word in words:
                    if len(word) > 3:
                        if word not in self.name_to_code:
                            self.name_to_code[word] = []
                        self.name_to_code[word].append(code)
        
        self.stock_to_exchange = {}
        for code in self.hnx_stocks:
            self.stock_to_exchange[code] = 'HNX'
        for code in self.upcom_stocks:
            self.stock_to_exchange[code] = 'UPCoM'
        for code in self.hose_stocks:
            self.stock_to_exchange[code] = 'HOSE'
        
        self.stats = {
            'total_crawled': 0,
            'hnx_found': 0,
            'upcom_found': 0,
            'hose_filtered': 0,
            'severe_risk': 0,
            'warning_risk': 0,
            'found_by_code': 0,
            'found_by_name': 0
        }
    
    def clean_text(self, text):
        """L√†m s·∫°ch text - t·ª´ V1.0"""
        if not text:
            return ""
        text = re.sub(r'[^\w\s.,;:!?()%\-\+\/\"\'√†√°·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√®√©·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±·ª≥√Ω·ª∑·ªπ·ªµƒë√Ä√Å·∫¢√É·∫†ƒÇ·∫Æ·∫∞·∫≤·∫¥·∫∂√Ç·∫§·∫¶·∫®·∫™·∫¨√à√â·∫∫·∫º·∫∏√ä·∫æ·ªÄ·ªÇ·ªÑ·ªÜ√å√ç·ªàƒ®·ªä√í√ì·ªé√ï·ªå√î·ªê·ªí·ªî·ªñ·ªò∆†·ªö·ªú·ªû·ª†·ª¢√ô√ö·ª¶≈®·ª§∆Ø·ª®·ª™·ª¨·ªÆ·ª∞·ª≤√ù·ª∂·ª∏·ª¥ƒê]', ' ', text)
        text = re.sub(r'\s+', ' ', text)
        return text.strip()
    
    def advanced_summarize(self, content, title, max_sentences=4):
        """T√≥m t·∫Øt EXTRACTIVE - t·ª´ V1.0"""
        content = self.clean_text(content)
        title = self.clean_text(title)
        
        if not content or len(content) < 100:
            return content
        
        full_text = title + ". " + content
        sentences = re.split(r'[.!?]+', full_text)
        sentences = [s.strip() for s in sentences if len(s.strip()) > 30]
        
        if len(sentences) <= max_sentences:
            return '. '.join(sentences) + '.'
        
        important_keywords = {
            'tƒÉng': 3, 'gi·∫£m': 3, 'tƒÉng tr∆∞·ªüng': 3,
            'l·ª£i nhu·∫≠n': 4, 'doanh thu': 4, 'l·ªó': 3,
            't·ª∑ ƒë·ªìng': 3, 'ngh√¨n t·ª∑': 4,
            'c·ªï phi·∫øu': 3, 'ni√™m y·∫øt': 3,
            'giao d·ªãch': 2, 'thanh kho·∫£n': 3,
            'qu√Ω': 3, 'nƒÉm': 2,
            'ph√°t h√†nh': 3, 'tr√°i phi·∫øu': 3,
            'ƒë·∫ßu t∆∞': 2, 'v·ªën': 3,
        }
        
        scored_sentences = []
        for i, sentence in enumerate(sentences):
            score = 0
            sentence_lower = sentence.lower()
            
            if i == 0:
                score += 5
            elif i == 1:
                score += 3
            elif i < 5:
                score += 1
            
            for keyword, weight in important_keywords.items():
                if keyword in sentence_lower:
                    score += weight
            
            numbers = re.findall(r'\d+(?:[.,]\d+)*', sentence)
            if numbers:
                score += len(numbers)
                if any(num for num in numbers if len(num.replace('.', '').replace(',', '')) >= 4):
                    score += 2
            
            if '%' in sentence:
                score += 3
            
            word_count = len(sentence.split())
            if 12 <= word_count <= 35:
                score += 2
            elif word_count < 8 or word_count > 50:
                score -= 1
            
            for code in list(self.hnx_stocks) + list(self.upcom_stocks):
                if code in sentence.upper():
                    score += 3
                    break
            
            scored_sentences.append((sentence, score, i))
        
        scored_sentences.sort(key=lambda x: x[1], reverse=True)
        top_sentences = scored_sentences[:max_sentences]
        top_sentences.sort(key=lambda x: x[2])
        
        summary = '. '.join([s[0] for s in top_sentences])
        if not summary.endswith('.'):
            summary += '.'
        
        summary = self.clean_text(summary)
        return summary
    
    def is_generic_news(self, title):
        """Ki·ªÉm tra xem c√≥ ph·∫£i tin t·ª©c chung kh√¥ng"""
        title_lower = title.lower()
        
        generic_patterns = [
            r'l·ªãch\s+s·ª±\s+ki·ªán',
            r'tin\s+v·∫Øn',
            r't·ªïng\s+h·ª£p',
            r'ƒëi·ªÉm\s+tin',
            r'nh·ªãp\s+ƒë·∫≠p',
            r'th·ªã\s+tr∆∞·ªùng\s+ng√†y',
            r'ch·ª©ng\s+kho√°n\s+ng√†y',
            r'phi√™n\s+giao\s+d·ªãch',
            r'c√°c\s+tin\s+t·ª©c',
            r'tin\s+nhanh',
            r'c·∫≠p\s+nh·∫≠t',
            r'ƒëi·ªÉm\s+l·∫°i',
        ]
        
        for pattern in generic_patterns:
            if re.search(pattern, title_lower):
                return True
        
        return False
    
    def extract_stock(self, text):
        """Tr√≠ch xu·∫•t m√£ CK - T√åM T·∫§T C·∫¢ M√É C√ì T√çN HI·ªÜU, SAU ƒê√ì L·ªåC HOSE"""
        text_upper = text.upper()
        text_lower = text.lower()
        
        # ============================================================
        # DANH S√ÅCH M√É D·ªÑ NH·∫¶M L·∫™N - Y√äU C·∫¶U T√çN HI·ªÜU M·∫†NH
        # ============================================================
        HIGH_RISK_CODES = {
            'THU': ['doanh thu', 'thu nh·∫≠p', 'thu ƒë∆∞·ª£c', 'thu v·ªÅ'],
            'TIN': ['tin v·∫Øn', 'tin t·ª©c', 'nh·∫≠n tin', 'tin nhanh', 'tin m·ªõi'],
            'USD': ['usd', 'ƒë√¥ la', 'dollar'],
            'CEO': ['ceo c·ªßa', 'v·ªã tr√≠ ceo', 'l√†m ceo'],
            'CAR': ['car', 'xe h∆°i', '√¥ t√¥'],
            'HAI': ['hai nƒÉm', 'hai qu√Ω', 'hai th√°ng', 'c·∫£ hai'],
            'TOP': ['top', 'ƒë·ª©ng top', 'n·∫±m trong top'],
            'VAN': ['vƒÉn b·∫£n', 'vƒÉn ph√≤ng'],
            'BAO': ['bao g·ªìm', 'bao nhi√™u'],
            'GIA': ['gi√°', 'gia ƒë√¨nh', 'gia tƒÉng'],
            'NAM': ['nam', 'nƒÉm', 'mi·ªÅn nam'],
            'MAI': ['mai', 'ng√†y mai'],
            'HOI': ['h·ªôi', 'h·ªôi ƒë·ªìng', 'h·ªôi ngh·ªã'],
            'CAN': ['c·∫ßn', 'c·∫ßn thi·∫øt'],
            'DAT': ['ƒë·∫°t', 'ƒë·∫°t ƒë∆∞·ª£c'],
            'SAO': ['sao', 'ng√¥i sao', 't·∫°i sao'],
        }
        
        # ============================================================
        # B∆Ø·ªöC 1: T√åM THEO C√ÅC PATTERN R√ï R√ÄNG (∆ØU TI√äN CAO NH·∫§T)
        # ============================================================
        
        # Pattern nh√≥m 1: Trong ngo·∫∑c v·ªõi s√†n
        patterns_with_exchange = [
            r'\((?:UPCOM|HNX|HOSE|HSX):\s*([A-Z]{3})\)',           # (UPCOM: ABC)
            r'\(([A-Z]{3})\s*[-‚Äì]\s*(?:UPCOM|HNX|HOSE|HSX)\)',     # (ABC - UPCOM)
            r'\(([A-Z]{3})\s*,\s*(?:UPCOM|HNX|HOSE|HSX)\)',        # (ABC, UPCOM)
            r'\((?:UPCOM|HNX|HOSE|HSX)\s*[-‚Äì]\s*([A-Z]{3})\)',     # (UPCOM - ABC)
        ]
        
        for pattern in patterns_with_exchange:
            match = re.search(pattern, text_upper)
            if match:
                code = match.group(1)
                if code in self.all_stocks:
                    exchange = self.stock_to_exchange.get(code)
                    # L·ªåC HOSE NGAY T·∫†I ƒê√ÇY
                    if exchange == 'HOSE':
                        continue
                    if exchange in ['HNX', 'UPCoM']:
                        return code, exchange, 'code'
        
        # Pattern nh√≥m 2: C√≥ t·ª´ kh√≥a "m√£"
        patterns_with_ma = [
            r'M√É\s*(?:CK|CH·ª®NG KHO√ÅN|CP)?:?\s*([A-Z]{3})\b',
            r'M√É\s+([A-Z]{3})\b',
            r'\(M√É:?\s*([A-Z]{3})\)',
            r'\(M√É\s*CK:?\s*([A-Z]{3})\)',
        ]
        
        for pattern in patterns_with_ma:
            match = re.search(pattern, text_upper)
            if match:
                code = match.group(1)
                if code in self.all_stocks:
                    exchange = self.stock_to_exchange.get(code)
                    if exchange == 'HOSE':
                        continue
                    if exchange in ['HNX', 'UPCoM']:
                        return code, exchange, 'code'
        
        # Pattern nh√≥m 3: C√≥ t·ª´ "c·ªï phi·∫øu"
        patterns_with_cp = [
            r'C·ªî\s+PHI·∫æU\s+([A-Z]{3})\b',
            r'\(C·ªî\s+PHI·∫æU:?\s*([A-Z]{3})\)',
        ]
        
        for pattern in patterns_with_cp:
            match = re.search(pattern, text_upper)
            if match:
                code = match.group(1)
                if code in self.all_stocks:
                    exchange = self.stock_to_exchange.get(code)
                    if exchange == 'HOSE':
                        continue
                    if exchange in ['HNX', 'UPCoM']:
                        return code, exchange, 'code'
        
        # Pattern nh√≥m 4: ƒê∆°n gi·∫£n trong ngo·∫∑c
        match = re.search(r'\(([A-Z]{3})\)', text_upper)
        if match:
            code = match.group(1)
            if code in self.all_stocks:
                exchange = self.stock_to_exchange.get(code)
                if exchange == 'HOSE':
                    continue
                if exchange in ['HNX', 'UPCoM']:
                    return code, exchange, 'code'
        
        # ============================================================
        # B∆Ø·ªöC 2: T√åM TO√ÄN B·ªò M√É C√ì T√çN HI·ªÜU, SAU ƒê√ì L·ªåC HOSE
        # ============================================================
        
        # ƒê·ªãnh nghƒ©a c√°c t√≠n hi·ªáu nh·∫≠n di·ªán
        context_indicators = [
            r'C√îNG\s+TY\s+',
            r'M√É\s+',
            r'C·ªî\s+PHI·∫æU\s+',
            r'CP\s+',
            r'CK\s+',
            r'CTCP\s+',
            r'TNHH\s+',
            r'T·∫¨P\s+ƒêO√ÄN\s+',
            r'NG√ÇN\s+H√ÄNG\s+',
            r'NH\s+',
        ]
        
        # T√≠n hi·ªáu M·∫†NH cho m√£ d·ªÖ nh·∫ßm
        strong_indicators = [
            r'C√îNG\s+TY\s+',
            r'CTCP\s+',
            r'T·∫¨P\s+ƒêO√ÄN\s+',
            r'NG√ÇN\s+H√ÄNG\s+',
            r'M√É\s+(?:CK|CP|CH·ª®NG KHO√ÅN)?:?\s*',
            r'C·ªî\s+PHI·∫æU\s+',
        ]
        
        # T√¨m t·∫•t c·∫£ c√°c c·ª•m 3 k√Ω t·ª± hoa t√°ch bi·ªát
        all_codes_in_text = re.finditer(r'\b([A-Z]{3})\b', text_upper)
        
        # L∆∞u t·∫•t c·∫£ c√°c m√£ t√¨m ƒë∆∞·ª£c (k·ªÉ c·∫£ HOSE)
        found_codes = []
        
        for match in all_codes_in_text:
            code = match.group(1)
            
            # Ki·ªÉm tra xem m√£ c√≥ trong danh s√°ch TO√ÄN TH·ªä TR∆Ø·ªúNG kh√¥ng
            if code not in self.all_stocks:
                continue
            
            # L·∫•y context xung quanh
            start = max(0, match.start() - 50)
            end = min(len(text_upper), match.end() + 50)
            context = text_upper[start:end]
            
            wider_context_start = max(0, match.start() - 100)
            wider_context_end = min(len(text_upper), match.end() + 100)
            wider_context = text_upper[wider_context_start:wider_context_end]
            
            # ========================================================
            # KI·ªÇM TRA M√É D·ªÑ NH·∫¶M L·∫™N
            # ========================================================
            if code in HIGH_RISK_CODES:
                is_common_word = False
                for false_pattern in HIGH_RISK_CODES[code]:
                    wider_context_lower = text_lower[wider_context_start:wider_context_end]
                    if false_pattern in wider_context_lower:
                        fp_pos = wider_context_lower.find(false_pattern)
                        code_pos_in_wider = match.start() - wider_context_start
                        if fp_pos <= code_pos_in_wider <= fp_pos + len(false_pattern):
                            is_common_word = True
                            break
                
                if is_common_word:
                    has_strong_indicator = False
                    before_context = text_upper[max(0, match.start() - 30):match.start()]
                    
                    for strong_ind in strong_indicators:
                        if re.search(strong_ind, before_context):
                            has_strong_indicator = True
                            break
                    
                    if not has_strong_indicator:
                        continue
            
            # ========================================================
            # KI·ªÇM TRA BLACKLIST
            # ========================================================
            blacklist_in_context = [
                r'CH·ª®NG\s+KHO√ÅN\s+' + code,
                r'CTCK\s+' + code,
                r'VN-?INDEX',
                r'NH·∫¨N\s+ƒê·ªäNH',
            ]
            
            is_blacklisted = False
            for bl_pattern in blacklist_in_context:
                if re.search(bl_pattern, context):
                    is_blacklisted = True
                    break
            
            if is_blacklisted:
                continue
            
            # ========================================================
            # KI·ªÇM TRA T√çN HI·ªÜU NH·∫¨N DI·ªÜN
            # ========================================================
            has_indicator = False
            for indicator in context_indicators:
                before_context = text_upper[max(0, match.start() - 30):match.start()]
                if re.search(indicator, before_context):
                    has_indicator = True
                    break
            
            # N·∫øu c√≥ t√≠n hi·ªáu, l∆∞u l·∫°i m√£ n√†y
            if has_indicator:
                exchange = self.stock_to_exchange.get(code)
                found_codes.append({
                    'code': code,
                    'exchange': exchange,
                    'position': match.start()
                })
        
        # ========================================================
        # L·ªåC HOSE V√Ä TR·∫¢ V·ªÄ M√É ƒê·∫¶U TI√äN (HNX/UPCoM)
        # ========================================================
        for item in found_codes:
            if item['exchange'] == 'HOSE':
                self.stats['hose_filtered'] += 1
                continue
            if item['exchange'] in ['HNX', 'UPCoM']:
                return item['code'], item['exchange'], 'code'
        
        # ============================================================
        # B∆Ø·ªöC 3: T√åM THEO T√äN C√îNG TY (∆ØU TI√äN TH·∫§P NH·∫§T)
        # ============================================================
        words = text_lower.split()
        matched_codes = []
        for word in words:
            if len(word) > 3 and word in self.name_to_code:
                matched_codes.extend(self.name_to_code[word])
        
        if matched_codes:
            from collections import Counter
            most_common = Counter(matched_codes).most_common(1)[0][0]
            exchange = self.stock_to_exchange.get(most_common)
            # L·ªåC HOSE
            if exchange == 'HOSE':
                return None, None, None
            if exchange in ['HNX', 'UPCoM']:
                return most_common, exchange, 'name'
        
        return None, None, None 
            # ========================================================
            has_indicator = False
            for indicator in context_indicators:
                # T√¨m indicator TR∆Ø·ªöC m√£ (trong v√≤ng 30 k√Ω t·ª±)
                before_context = text_upper[max(0, match.start() - 30):match.start()]
                if re.search(indicator, before_context):
                    has_indicator = True
                    break
            
            # N·∫øu c√≥ t√≠n hi·ªáu nh·∫≠n di·ªán, return m√£ n√†y
            if has_indicator:
                if code in self.hnx_stocks:
                    return code, 'HNX', 'code'
                elif code in self.upcom_stocks:
                    return code, 'UPCoM', 'code'
        
        # ============================================================
        # B∆Ø·ªöC 3: T√åM THEO T√äN C√îNG TY (∆ØU TI√äN TH·∫§P NH·∫§T)
        # ============================================================
        
        words = text_lower.split()
        matched_codes = []
        for word in words:
            if len(word) > 3 and word in self.name_to_code:
                matched_codes.extend(self.name_to_code[word])
        
        if matched_codes:
            from collections import Counter
            most_common = Counter(matched_codes).most_common(1)[0][0]
            exchange = self.stock_to_exchange.get(most_common)
            return most_common, exchange, 'name'
        
        return None, None, None
    
    def fetch_url(self, url, max_retries=2):
        for attempt in range(max_retries):
            try:
                response = self.session.get(url, headers=self.headers, timeout=15)
                response.raise_for_status()
                return response
            except:
                if attempt < max_retries - 1:
                    time.sleep(1)
                return None
    
    def parse_date(self, date_text):
        """Parse ng√†y th√°ng t·ª´ nhi·ªÅu ƒë·ªãnh d·∫°ng kh√°c nhau"""
        if not date_text:
            return None
        
        try:
            # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a
            date_text = date_text.strip()
            
            # ƒê·ªãnh d·∫°ng ISO: 2025-10-21T14:30:00+07:00
            if 'T' in date_text or '+' in date_text:
                match = re.search(r'(\d{4})-(\d{2})-(\d{2})', date_text)
                if match:
                    year, month, day = match.groups()
                    return datetime(int(year), int(month), int(day), tzinfo=self.vietnam_tz)
            
            # ƒê·ªãnh d·∫°ng: 21/10/2025 14:30
            match = re.search(r'(\d{1,2})[/-](\d{1,2})[/-](\d{4})', date_text)
            if match:
                day, month, year = match.groups()
                return datetime(int(year), int(month), int(day), tzinfo=self.vietnam_tz)
            
            # ƒê·ªãnh d·∫°ng: 21-10-2025
            match = re.search(r'(\d{1,2})[/-](\d{1,2})[/-](\d{4})', date_text)
            if match:
                day, month, year = match.groups()
                return datetime(int(year), int(month), int(day), tzinfo=self.vietnam_tz)
            
            # ƒê·ªãnh d·∫°ng ti·∫øng Vi·ªát: "21 Th√°ng 10 2025" ho·∫∑c "21/10/2025"
            match = re.search(r'(\d{1,2})\s*[/-]\s*(\d{1,2})\s*[/-]\s*(\d{4})', date_text)
            if match:
                day, month, year = match.groups()
                return datetime(int(year), int(month), int(day), tzinfo=self.vietnam_tz)
            
            # T·ª´ kh√≥a th·ªùi gian t∆∞∆°ng ƒë·ªëi
            date_text_lower = date_text.lower()
            now = datetime.now(self.vietnam_tz)
            
            if 'h√¥m nay' in date_text_lower or 'today' in date_text_lower:
                return now
            elif 'h√¥m qua' in date_text_lower or 'yesterday' in date_text_lower:
                return now - timedelta(days=1)
            elif 'gi·ªù tr∆∞·ªõc' in date_text_lower or 'hours ago' in date_text_lower:
                hours_match = re.search(r'(\d+)', date_text)
                if hours_match:
                    hours = int(hours_match.group(1))
                    return now - timedelta(hours=hours)
            elif 'ph√∫t tr∆∞·ªõc' in date_text_lower or 'minutes ago' in date_text_lower:
                return now
            
        except:
            pass
        
        return None
    
    def fetch_article_content(self, url):
        """L·∫•y n·ªôi dung b√†i vi·∫øt - t·ª´ V1.0"""
        try:
            response = self.fetch_url(url)
            if not response:
                return None, None, None
            
            response.encoding = 'utf-8'
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # T√¨m ng√†y - M·ªû R·ªòNG C√ÅC SELECTOR
            date_text = None
            article_date_obj = None
            
            # Th·ª≠ nhi·ªÅu pattern kh√°c nhau
            for pattern in [
                {'class': re.compile(r'date|time|publish|post.*date', re.I)},
                {'itemprop': 'datePublished'},
                {'property': 'article:published_time'},
                {'name': 'pubdate'},
                {'class': re.compile(r'meta.*time', re.I)}
            ]:
                date_elem = soup.find(['time', 'span', 'div', 'meta'], pattern)
                if date_elem:
                    date_text = date_elem.get('datetime') or date_elem.get('content') or date_elem.get_text(strip=True)
                    if date_text:
                        article_date_obj = self.parse_date(date_text)
                        if article_date_obj:
                            break
            
            # N·∫øu kh√¥ng t√¨m th·∫•y, d√πng ng√†y hi·ªán t·∫°i
            if not article_date_obj:
                article_date_obj = datetime.now(self.vietnam_tz)
            
            article_date_str = article_date_obj.strftime('%d/%m/%Y %H:%M')
            
            # T√¨m n·ªôi dung
            content = ""
            for selector in [
                ('article', {}),
                ('div', {'class': re.compile(r'content|article|detail|body', re.I)}),
            ]:
                content_div = soup.find(selector[0], selector[1])
                if content_div:
                    paragraphs = content_div.find_all('p')
                    content = ' '.join([p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 50])
                    if content:
                        break
            
            if not content:
                paragraphs = soup.find_all('p')
                valid_p = [p.get_text(strip=True) for p in paragraphs if 50 < len(p.get_text(strip=True)) < 1000]
                content = ' '.join(valid_p[:8])
            
            content = self.clean_text(content)
            return content, article_date_str, article_date_obj
        
        except:
            return None, None, None
    
    def scrape_source(self, url, source_name, pattern, max_articles=20, progress_callback=None):
        try:
            response = self.fetch_url(url)
            if not response:
                return 0
            
            response.encoding = 'utf-8'
            soup = BeautifulSoup(response.text, 'html.parser')
            
            count = 0
            seen = set()
            links = soup.find_all('a', href=True)
            total_links = len(links)
            
            # B∆Ø·ªöC 1: C√ÄO TO√ÄN B·ªò B√ÄI VI·∫æT TR∆Ø·ªöC
            all_crawled_articles = []
            
            for idx, link_tag in enumerate(links):
                if progress_callback:
                    progress = (idx + 1) / total_links * 0.5  # 50% cho vi·ªác c√†o
                    progress_callback(f"{source_name} - ƒêang c√†o: {idx+1}/{total_links}", progress)
                
                href = link_tag.get('href', '')
                
                if pattern(href) and href not in seen:
                    title = link_tag.get_text(strip=True)
                    
                    # ‚úÖ L·ªåC TIN CHUNG NGAY T·∫†I TI√äU ƒê·ªÄ
                    if title and len(title) > 30 and not self.is_generic_news(title):
                        seen.add(href)
                        full_link = urljoin(url, href)
                        
                        # FETCH N·ªòI DUNG ƒê·∫¶Y ƒê·ª¶
                        content, article_date_str, article_date_obj = self.fetch_article_content(full_link)
                        
                        # ‚úÖ L·ªåC TH·ªúI GIAN NGAY T·∫†I ƒê√ÇY
                        if content and article_date_obj:
                            # Ki·ªÉm tra xem b√†i vi·∫øt c√≥ n·∫±m trong kho·∫£ng th·ªùi gian kh√¥ng
                            if article_date_obj >= self.cutoff_time:
                                all_crawled_articles.append({
                                    'title': title,
                                    'link': full_link,
                                    'date': article_date_str,
                                    'date_obj': article_date_obj,
                                    'content': content
                                })
                            # else: b·ªè qua b√†i vi·∫øt qu√° c≈©
                            
                            time.sleep(0.3)
                            
                            if len(all_crawled_articles) >= max_articles * 3:  # C√†o nhi·ªÅu h∆°n ƒë·ªÉ l·ªçc sau
                                break
            
            self.stats['total_crawled'] = len(all_crawled_articles)
            
            # B∆Ø·ªöC 2: L·ªåC M√É CK T·ª™ N·ªòI DUNG
            for idx, article in enumerate(all_crawled_articles):
                if progress_callback:
                    progress = 0.5 + (idx + 1) / len(all_crawled_articles) * 0.5  # 50% c√≤n l·∫°i cho vi·ªác l·ªçc
                    progress_callback(f"{source_name} - ƒêang l·ªçc m√£: {idx+1}/{len(all_crawled_articles)}", progress)
                
                # TR√çCH XU·∫§T M√É CK T·ª™ N·ªòI DUNG (kh√¥ng ph·∫£i ti√™u ƒë·ªÅ)
                full_text = article['title'] + " " + article['content']
                stock_code, exchange, match_method = self.extract_stock(full_text)
                
                if stock_code and exchange in ['HNX', 'UPCoM']:
                    if match_method == 'code':
                        self.stats['found_by_code'] += 1
                    else:
                        self.stats['found_by_name'] += 1
                    
                    company_name = self.code_to_name.get(stock_code, '')
                    
                    # T√ìM T·∫ÆT
                    summary = self.advanced_summarize(article['content'], article['title'], max_sentences=4)
                    
                    # SENTIMENT
                    sentiment_result = self.sentiment_analyzer.analyze_sentiment(article['title'], article['content'])
                    
                    if exchange == 'HNX':
                        self.stats['hnx_found'] += 1
                    else:
                        self.stats['upcom_found'] += 1
                    
                    if sentiment_result['risk_level'] == 'Nghi√™m tr·ªçng':
                        self.stats['severe_risk'] += 1
                    elif sentiment_result['risk_level'] == 'C·∫£nh b√°o':
                        self.stats['warning_risk'] += 1
                    
                    self.all_articles.append({
                        'Ti√™u ƒë·ªÅ': article['title'],
                        'Link': article['link'],
                        'Ng√†y': article['date'],
                        'M√£ CK': stock_code,
                        'T√™n c√¥ng ty': company_name,
                        'S√†n': exchange,
                        'Sentiment': sentiment_result['sentiment_label'],
                        'ƒêi·ªÉm': sentiment_result['sentiment_score'],
                        'Risk': sentiment_result['risk_level'],
                        'Vi ph·∫°m': sentiment_result['violations'],
                        'Keywords': "; ".join([k['keyword'] for k in sentiment_result['keywords'][:3]]),
                        'N·ªôi dung t√≥m t·∫Øt': summary,
                        'T√¨m theo': 'M√£ CK' if match_method == 'code' else 'T√™n c√¥ng ty'
                    })
                    
                    count += 1
                    
                    if count >= max_articles:
                        break
            
            return count
        
        except Exception as e:
            st.error(f"L·ªói {source_name}: {str(e)}")
            return 0
    
    def run(self, max_articles_per_source=20, progress_callback=None):
        sources = [
            ("https://cafef.vn/thi-truong-chung-khoan.chn", "CafeF", lambda h: '.chn' in h),
            ("https://vietstock.vn/chung-khoan.htm", "VietStock", lambda h: re.search(r'/\d{4}/\d{2}/.+\.htm', h)),
            ("https://nguoiquansat.vn/chung-khoan", "Ng∆∞·ªùi Quan S√°t", lambda h: '/chung-khoan/' in h and h.startswith('/')),
            ("https://baomoi.com/chung-khoan.epi", "B√°o M·ªõi", lambda h: h.startswith('/') and any(x in h for x in ['.epi', '-c111'])),
            ("https://www.tinnhanhchungkhoan.vn/chung-khoan/", "Tin Nhanh CK (CK)", lambda h: '/chung-khoan/' in h or '/doanh-nghiep/' in h),
            ("https://www.tinnhanhchungkhoan.vn/doanh-nghiep/", "Tin Nhanh CK (DN)", lambda h: '/doanh-nghiep/' in h or '/chung-khoan/' in h),
        ]
        
        for url, name, pattern in sources:
            self.scrape_source(url, name, pattern, max_articles_per_source, progress_callback)
            time.sleep(1)
        
        if len(self.all_articles) == 0:
            return None
        
        df = pd.DataFrame(self.all_articles)
        df = df.drop_duplicates(subset=['Ti√™u ƒë·ªÅ'], keep='first')
        df.insert(0, 'STT', range(1, len(df) + 1))
        
        return df

# ============================================================
# STREAMLIT APP
# ============================================================

def main():
    st.markdown('<div class="main-header">üìà TOOL C√ÄO TIN V2.4</div>', unsafe_allow_html=True)
    st.markdown('<div style="text-align:center;color:#666;margin-bottom:2rem;">HNX & UPCoM - Upload + Summarize + Sentiment</div>', unsafe_allow_html=True)
    
    # Sidebar
    with st.sidebar:
        st.header("‚öôÔ∏è C√ÄI ƒê·∫∂T")
        
        st.subheader("üìÇ DANH S√ÅCH M√É CK")
        st.markdown('<div class="upload-box">', unsafe_allow_html=True)
        st.write("**Upload file Excel/CSV**")
        st.caption("G·ªìm 3 c·ªôt: M√£ CK | S√†n (HOSE/HNX/UPCoM) | T√™n c√¥ng ty")
        st.info("‚ÑπÔ∏è File c√≥ th·ªÉ ch·ª©a t·∫•t c·∫£ c√°c s√†n. Tool s·∫Ω t·ª± ƒë·ªông l·ªçc ch·ªâ gi·ªØ HNX & UPCoM")
        
        uploaded_file = st.file_uploader(
            "Ch·ªçn file",
            type=['xlsx', 'xls', 'csv'],
            help="File ph·∫£i c√≥ c√°c c·ªôt: M√£ CK, S√†n (HNX/UPCoM), T√™n c√¥ng ty"
        )
        
        sample_excel = create_sample_excel()
        st.download_button(
            label="üì• T·∫£i file m·∫´u",
            data=sample_excel,
            file_name="mau_danh_sach_ma_ck.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )
        st.markdown('</div>', unsafe_allow_html=True)
        
        if uploaded_file is not None:
            stock_df, error = parse_stock_file(uploaded_file)
            
            if error:
                st.error(f"‚ùå {error}")
                st.session_state['stock_df'] = load_default_stock_list()
            else:
                total_count = len(stock_df)
                hnx_count = len(stock_df[stock_df['S√†n'] == 'HNX'])
                upcom_count = len(stock_df[stock_df['S√†n'] == 'UPCoM'])
                hose_count = len(stock_df[stock_df['S√†n'] == 'HOSE'])
                
                st.success(f"‚úÖ ƒê√£ load {total_count} m√£ CK")
                st.session_state['stock_df'] = stock_df
                
                st.info(f"üìä **Ph√¢n b·ªë:**\n- HOSE: {hose_count} (d√πng ƒë·ªÉ nh·∫≠n di·ªán, s·∫Ω b·ªã l·ªçc)\n- HNX: {hnx_count} ‚úÖ\n- UPCoM: {upcom_count} ‚úÖ")
        else:
            if 'stock_df' not in st.session_state:
                st.session_state['stock_df'] = load_default_stock_list()
                st.warning("‚ö†Ô∏è ƒêang d√πng danh s√°ch m·∫∑c ƒë·ªãnh")
        
        st.markdown("---")
        st.subheader("üîß T√ôY CH·ªàNH")
        
        time_filter = st.selectbox(
            "‚è∞ Kho·∫£ng th·ªùi gian",
            options=[6, 12, 24, 48, 72, 168],
            format_func=lambda x: f"{x} gi·ªù" if x < 168 else "1 tu·∫ßn",
            index=2
        )
        
        max_articles = st.slider(
            "üìä S·ªë b√†i t·ªëi ƒëa/ngu·ªìn",
            min_value=5,
            max_value=50,
            value=20,
            step=5
        )
        
        st.markdown("---")
        st.info("üí° **H∆∞·ªõng d·∫´n:**\n1. Upload danh s√°ch m√£\n2. Ch·ªçn th·ªùi gian\n3. B·∫•m 'B·∫Øt ƒë·∫ßu'\n4. Download Excel")
    
    # Main content
    if st.button("üöÄ B·∫ÆT ƒê·∫¶U C√ÄO TIN", type="primary"):
        stock_df = st.session_state.get('stock_df')
        
        if stock_df is None or len(stock_df) == 0:
            st.error("‚ùå Ch∆∞a c√≥ danh s√°ch m√£ CK! Vui l√≤ng upload file.")
            return
        
        with st.spinner("ƒêang c√†o tin..."):
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            def update_progress(message, progress):
                status_text.text(message)
                progress_bar.progress(progress)
            
            scraper = StockScraperWeb(stock_df, time_filter_hours=time_filter)
            df = scraper.run(max_articles_per_source=max_articles, progress_callback=update_progress)
            
            progress_bar.empty()
            status_text.empty()
            
            if df is not None:
                st.success(f"‚úÖ Ho√†n t·∫•t! T√¨m th·∫•y {len(df)} b√†i vi·∫øt")
                st.info(f"üîç T√¨m theo m√£ CK: {scraper.stats['found_by_code']} | T√¨m theo t√™n: {scraper.stats['found_by_name']}")
                
                st.session_state['df'] = df
                st.session_state['stats'] = scraper.stats
            else:
                st.error("Kh√¥ng t√¨m th·∫•y b√†i vi·∫øt n√†o!")
    
    # Display results
    if 'df' in st.session_state:
        df = st.session_state['df']
        stats = st.session_state['stats']
        
        # Metrics
        col1, col2, col3, col4, col5, col6 = st.columns(6)
        with col1:
            st.metric("üìä T·ªïng b√†i", len(df))
        with col2:
            st.metric("‚ö†Ô∏è Nghi√™m tr·ªçng", stats['severe_risk'])
        with col3:
            st.metric("‚ö†Ô∏è C·∫£nh b√°o", stats['warning_risk'])
        with col4:
            st.metric("üî§ T√¨m theo m√£", stats['found_by_code'])
        with col5:
            st.metric("üìù T√¨m theo t√™n", stats['found_by_name'])
        with col6:
            st.metric("üö´ L·ªçc HOSE", stats['hose_filtered'])
        
        # Download button
        buffer = io.BytesIO()
        with pd.ExcelWriter(buffer, engine='openpyxl') as writer:
            df.to_excel(writer, index=False, sheet_name='T·∫•t c·∫£')
            
            df_severe = df[df['Risk'] == 'Nghi√™m tr·ªçng']
            if len(df_severe) > 0:
                df_severe.to_excel(writer, index=False, sheet_name='Nghi√™m tr·ªçng')
            
            df_warning = df[df['Risk'] == 'C·∫£nh b√°o']
            if len(df_warning) > 0:
                df_warning.to_excel(writer, index=False, sheet_name='C·∫£nh b√°o')
        
        st.download_button(
            label="‚¨áÔ∏è Download Excel",
            data=buffer.getvalue(),
            file_name=f"Tin_CK_{datetime.now().strftime('%d%m%Y_%H%M')}.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )
        
        st.markdown("---")
        
        # Filters
        st.subheader("üîç L·ªåC & T√åM KI·∫æM")
        
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            search_code = st.text_input("M√£ CK", placeholder="VD: SHS")
        with col2:
            filter_san = st.selectbox("S√†n", ["T·∫•t c·∫£", "HNX", "UPCoM"])
        with col3:
            filter_risk = st.selectbox("Risk Level", ["T·∫•t c·∫£", "Nghi√™m tr·ªçng", "C·∫£nh b√°o", "B√¨nh th∆∞·ªùng", "T√≠ch c·ª±c"])
        with col4:
            filter_method = st.selectbox("T√¨m theo", ["T·∫•t c·∫£", "M√£ CK", "T√™n c√¥ng ty"])
        
        # Apply filters
        df_filtered = df.copy()
        
        if search_code:
            df_filtered = df_filtered[
                df_filtered['M√£ CK'].str.contains(search_code.upper(), case=False, na=False) |
                df_filtered['T√™n c√¥ng ty'].str.contains(search_code, case=False, na=False)
            ]
        
        if filter_san != "T·∫•t c·∫£":
            df_filtered = df_filtered[df_filtered['S√†n'] == filter_san]
        
        if filter_risk != "T·∫•t c·∫£":
            df_filtered = df_filtered[df_filtered['Risk'] == filter_risk]
        
        if filter_method != "T·∫•t c·∫£":
            df_filtered = df_filtered[df_filtered['T√¨m theo'] == filter_method]
        
        st.info(f"Hi·ªÉn th·ªã {len(df_filtered)} / {len(df)} b√†i")
        
        # Display articles
        st.subheader("üì∞ DANH S√ÅCH B√ÄI VI·∫æT")
        
        for idx, row in df_filtered.iterrows():
            if row['Risk'] == 'Nghi√™m tr·ªçng':
                card_class = "severe-card"
                icon = "‚ö†Ô∏è"
            elif row['Risk'] == 'C·∫£nh b√°o':
                card_class = "warning-card"
                icon = "‚ö†Ô∏è"
            elif row['Risk'] == 'T√≠ch c·ª±c':
                card_class = "positive-card"
                icon = "‚úÖ"
            else:
                card_class = "metric-card"
                icon = "üìÑ"
            
            with st.container():
                st.markdown(f'<div class="{card_class}">', unsafe_allow_html=True)
                
                col1, col2 = st.columns([4, 1])
                
                with col1:
                    if row['T√™n c√¥ng ty']:
                        st.markdown(f"**{icon} {row['M√£ CK']} - {row['T√™n c√¥ng ty']} ({row['S√†n']})**")
                    else:
                        st.markdown(f"**{icon} {row['M√£ CK']} ({row['S√†n']})**")
                    
                    st.markdown(f"{row['Ti√™u ƒë·ªÅ']}")
                    
                    caption_text = f"üìÖ {row['Ng√†y']} | "
                    caption_text += f"Sentiment: {row['Sentiment']} ({row['ƒêi·ªÉm']}) | "
                    caption_text += f"Risk: {row['Risk']} | "
                    caption_text += f"üîç {row['T√¨m theo']}"
                    
                    if row['Vi ph·∫°m']:
                        caption_text += f" | ‚öñÔ∏è {row['Vi ph·∫°m']}"
                    
                    st.caption(caption_text)
                
                with col2:
                    if st.button("üîó Xem", key=f"view_{idx}"):
                        st.markdown(f"[M·ªü b√†i vi·∫øt]({row['Link']})")
                
                # HI·ªÇN TH·ªä T√ìM T·∫ÆT
                if pd.notna(row['N·ªôi dung t√≥m t·∫Øt']) and row['N·ªôi dung t√≥m t·∫Øt']:
                    with st.expander("üìù Xem t√≥m t·∫Øt"):
                        st.write(row['N·ªôi dung t√≥m t·∫Øt'])
                
                if row['Keywords']:
                    st.info(f"üîë Keywords: {row['Keywords']}")
                
                st.markdown('</div>', unsafe_allow_html=True)
                st.markdown("<br>", unsafe_allow_html=True)
        
        # Dashboard
        st.markdown("---")
        st.subheader("üìä DASHBOARD")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.write("**Ph√¢n b·ªë Sentiment**")
            sentiment_counts = df['Sentiment'].value_counts()
            st.bar_chart(sentiment_counts)
        
        with col2:
            st.write("**Ph√¢n b·ªë Risk Level**")
            risk_counts = df['Risk'].value_counts()
            st.bar_chart(risk_counts)
        
        col3, col4 = st.columns(2)
        
        with col3:
            st.write("**Top 10 M√£ CK**")
            top_ma = df['M√£ CK'].value_counts().head(10)
            st.bar_chart(top_ma)
        
        with col4:
            st.write("**Ph√¢n b·ªë theo S√†n**")
            san_counts = df['S√†n'].value_counts()
            st.bar_chart(san_counts)
        
        # Chi ti·∫øt theo m√£
        st.markdown("---")
        st.subheader("üìà CHI TI·∫æT THEO M√É CK")
        
        with st.expander("Xem chi ti·∫øt"):
            summary = df.groupby('M√£ CK').agg({
                'Ti√™u ƒë·ªÅ': 'count',
                'ƒêi·ªÉm': 'mean',
                'Risk': lambda x: x.mode()[0] if len(x) > 0 else 'N/A'
            }).rename(columns={
                'Ti√™u ƒë·ªÅ': 'S·ªë b√†i',
                'ƒêi·ªÉm': 'Sentiment TB',
                'Risk': 'Risk ch√≠nh'
            }).reset_index()
            
            summary = summary.merge(
                df[['M√£ CK', 'T√™n c√¥ng ty', 'S√†n']].drop_duplicates(),
                on='M√£ CK',
                how='left'
            )
            
            summary['Sentiment TB'] = summary['Sentiment TB'].round(1)
            summary = summary.sort_values('S·ªë b√†i', ascending=False)
            
            st.dataframe(
                summary,
                use_container_width=True,
                hide_index=True
            )

if __name__ == "__main__":
    main()
