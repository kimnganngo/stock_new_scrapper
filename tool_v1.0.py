# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  ğŸ¯ STOCK NEWS SCRAPER - STREAMLIT WEB APP (REAL DATA)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  File: app.py
#  Version: 2.2 - Real Data from CafeF & VietStock
#  Deploy: Streamlit Cloud
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import streamlit as st
import pandas as pd
from datetime import datetime, timedelta
import re
import time
from io import BytesIO
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PAGE CONFIG
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

st.set_page_config(
    page_title="Stock News Scraper",
    page_icon="ğŸ“°",
    layout="wide",
    initial_sidebar_state="expanded"
)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CUSTOM CSS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        text-align: center;
        color: #1f77b4;
        padding: 1rem 0;
    }
    .stats-card {
        background-color: #f0f2f6;
        padding: 1rem;
        border-radius: 0.5rem;
        text-align: center;
    }
    .stProgress > div > div > div > div {
        background-color: #1f77b4;
    }
</style>
""", unsafe_allow_html=True)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CLASS: STOCK SCRAPER WITH REAL DATA
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class StockScraperV2:
    """Stock News Scraper vá»›i Real Data tá»« CafeF & VietStock"""
    
    def __init__(self, time_filter_hours=24):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'vi-VN,vi;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
        
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        
        self.time_filter_hours = time_filter_hours
        self.cutoff_time = datetime.now() - timedelta(hours=time_filter_hours)
        
        self._setup_blacklist()
        self._setup_risk_keywords()
        
        self.stats = {
            'total_crawled': 0,
            'specific_stocks': 0,
            'market_general_filtered': 0,
            'time_filtered': 0,
            'high_risk': 0,
            'medium_risk': 0,
            'normal': 0,
            'cafef_articles': 0,
            'vietstock_articles': 0,
            'errors': 0
        }
        
        self.all_articles = []
    
    def _setup_blacklist(self):
        """Blacklist - Loáº¡i bá» tin tá»•ng quan"""
        self.title_blacklist = {
            'vn-index', 'vnindex', 'vn index', 'vn30', 'hnx-index', 'hnxindex',
            'upcom-index', 'upcomindex', 'chá»‰ sá»‘', 'chi so',
            'tá»•ng quan thá»‹ trÆ°á»ng', 'tong quan thi truong', 'thá»‹ trÆ°á»ng chá»©ng khoÃ¡n',
            'thi truong chung khoan', 'thá»‹ trÆ°á»ng chung', 'phiÃªn giao dá»‹ch',
            'phien giao dich', 'káº¿t thÃºc phiÃªn', 'ket thuc phien',
            'má»Ÿ cá»­a', 'mo cua', 'Ä‘Ã³ng cá»­a', 'dong cua',
            'top cá»• phiáº¿u', 'top co phieu', 'top 10', 'top 5', 'top stock',
            'cá»• phiáº¿u nÃ³ng nháº¥t', 'co phieu nong nhat', 'cá»• phiáº¿u hot',
            'danh sÃ¡ch cá»• phiáº¿u', 'danh sach co phieu',
            'tin váº¯n', 'tin van', 'Ä‘iá»ƒm tin', 'diem tin', 'tá»•ng há»£p tin',
            'tong hop tin', 'báº£n tin', 'ban tin', 'sao chá»©ng khoÃ¡n',
            'trong tuáº§n', 'trong tuan', 'tuáº§n qua', 'tuan qua',
            'dÃ²ng tiá»n', 'dong tien', 'thanh khoáº£n thá»‹ trÆ°á»ng',
            'thanh khoan thi truong', 'xu hÆ°á»›ng thá»‹ trÆ°á»ng', 'xu huong',
            'nháº­n Ä‘á»‹nh', 'nhan dinh', 'triá»ƒn vá»ng thá»‹ trÆ°á»ng', 'trien vong',
            'khá»‘i ngoáº¡i mua rÃ²ng', 'khoi ngoai mua rong', 'khá»‘i ngoáº¡i bÃ¡n',
            'giao dá»‹ch khá»‘i ngoáº¡i', 'giao dich khoi ngoai',
            'tuáº§n nÃ y', 'tuan nay', 'ngÃ y hÃ´m nay', 'ngay hom nay',
            'hÃ´m nay', 'hom nay', 'sÃ¡ng nay', 'sang nay'
        }
        
        self.title_blacklist_patterns = [
            r'phiÃªn \d+/\d+',
            r'\d+ cá»• phiáº¿u',
            r'top \d+',
            r'tuáº§n \d+',
            r'thÃ¡ng \d+',
            r'quÃ½ \d+',
        ]
        
        self.required_indicators = [
            r'\b[A-Z]{3,4}\b',
            r'(?:cá»• phiáº¿u|cp|mÃ£)\s+[A-Z]{3,4}',
            r'[A-Z]{3,4}\s+(?:tÄƒng|giáº£m|lÃ£i|lá»—)',
        ]
    
    def _setup_risk_keywords(self):
        """Setup 6 nhÃ³m keyword nguy cÆ¡"""
        self.risk_categories = {
            'A. Ná»™i bá»™ & Quáº£n trá»‹': {
                'keywords': [
                    ('lÃ£nh Ä‘áº¡o bá»‹ báº¯t', 10), ('lÃ£nh Ä‘áº¡o bá» trá»‘n', 10),
                    ('lÃ£nh Ä‘áº¡o máº¥t liÃªn láº¡c', 9), ('cá»• Ä‘Ã´ng lá»›n bÃ¡n chui', 8),
                    ('cá»• Ä‘Ã´ng ná»™i bá»™ bÃ¡n chui', 8), ('chá»§ tá»‹ch bÃ¡n sáº¡ch cá»• phiáº¿u', 9),
                    ('chá»§ tá»‹ch báº¥t ngá» thoÃ¡i háº¿t vá»‘n', 9), ('thÃ¢u tÃ³m quyá»n lá»±c', 7),
                    ('tranh cháº¥p hÄ‘qt', 7), ('tranh cháº¥p há»™i Ä‘á»“ng', 7),
                    ('Ä‘á»•i chá»§', 6), ('lÃ£nh Ä‘áº¡o mua chui', 7),
                ],
                'icon': 'ğŸ‘¥'
            },
            'B. Káº¿t quáº£ kinh doanh & TÃ i chÃ­nh': {
                'keywords': [
                    ('báº¥t ngá» bÃ¡o lá»—', 8), ('thua lá»— báº¥t thÆ°á»ng', 8),
                    ('lá»£i nhuáº­n Ä‘á»™t biáº¿n', 6), ('cháº­m cÃ´ng bá»‘ bctc', 7),
                    ('kiá»ƒm toÃ¡n tá»« chá»‘i', 9), ('kiá»ƒm toÃ¡n ngoáº¡i trá»«', 8),
                    ('ná»£ xáº¥u báº¥t thÆ°á»ng', 8), ('máº¥t kháº£ nÄƒng thanh toÃ¡n', 9),
                    ('chuyá»ƒn lá»— thÃ nh lÃ£i', 7), ('chuyá»ƒn lÃ£i thÃ nh lá»—', 8),
                    ('lá»— sau soÃ¡t xÃ©t', 8), ('lá»— Ã¢m vá»‘n chá»§', 9),
                    ('Ã¢m vá»‘n chá»§', 9), ('doanh thu tÄƒng nhÆ°ng váº«n lá»—', 7),
                ],
                'icon': 'ğŸ’°'
            },
            'C. Thao tÃºng & Giao dá»‹ch báº¥t thÆ°á»ng': {
                'keywords': [
                    ('Ä‘á»™i lÃ¡i lÃ m giÃ¡', 10), ('Ä‘á»™i lÃ¡i', 9),
                    ('tÄƒng tráº§n liÃªn tiáº¿p', 7), ('giáº£m sÃ n liÃªn tá»¥c', 8),
                    ('khá»‘i lÆ°á»£ng tÄƒng báº¥t thÆ°á»ng', 6), ('giao dá»‹ch ná»™i giÃ¡n', 9),
                    ('rÃ² rá»‰ thÃ´ng tin ná»™i bá»™', 8), ('thá»‹ giÃ¡ tÄƒng nhiá»u láº§n', 6),
                    ('tÄƒng dá»±ng Ä‘á»©ng', 7), ('cá»• phiáº¿u tÄƒng phi mÃ£', 7),
                    ('cá»• phiáº¿u tÄƒng ngÆ°á»£c dÃ²ng', 6), ('giÃ¡ cá»• phiáº¿u tÄƒng trong khi lá»—', 8),
                    ('thao tÃºng', 9),
                ],
                'icon': 'âš ï¸'
            },
            'D. M&A & Sá»± kiá»‡n Ä‘áº·c biá»‡t': {
                'keywords': [
                    ('thÃ¢u tÃ³m', 7), ('m&a', 6), ('chÃ o mua cÃ´ng khai', 6),
                    ('niÃªm yáº¿t cá»­a sau', 8), ('sÃ¡p nháº­p ngÆ°á»£c', 7),
                    ('nhÃ¢n sá»± cáº¥p cao báº¥t ngá» tá»« nhiá»‡m', 7),
                    ('báº¥t ngá» giáº£i thá»ƒ', 9), ('giáº£i thá»ƒ', 8),
                ],
                'icon': 'ğŸ”„'
            },
            'E. PhÃ¡p lÃ½ & Xá»­ pháº¡t': {
                'keywords': [
                    ('cÃ´ng an Ä‘iá»u tra', 10), ('khá»Ÿi tá»‘ lÃ£nh Ä‘áº¡o', 10),
                    ('khá»Ÿi tá»‘', 9), ('gian láº­n tÃ i chÃ­nh', 10),
                    ('Ä‘iá»u tra', 8), ('vi pháº¡m', 7), ('xá»­ pháº¡t', 7),
                ],
                'icon': 'âš–ï¸'
            },
            'F. Sá»± kiá»‡n bÃªn ngoÃ i tÃ¡c Ä‘á»™ng': {
                'keywords': [
                    ('máº¥t há»£p Ä‘á»“ng lá»›n', 7), ('sá»± cá»‘ mÃ´i trÆ°á»ng', 8),
                    ('bá»‹ ngá»«ng hoáº¡t Ä‘á»™ng sáº£n xuáº¥t', 8), ('tai náº¡n lao Ä‘á»™ng nghiÃªm trá»ng', 8),
                    ('chÃ¡y kho', 7), ('chÃ¡y nhÃ  xÆ°á»Ÿng', 8),
                    ('bá»‹ thu há»“i giáº¥y phÃ©p', 9), ('Ä‘á»‘i tÃ¡c phÃ¡ sáº£n', 7), ('phÃ¡ sáº£n', 8),
                ],
                'icon': 'ğŸ”¥'
            }
        }
        
        self.all_risk_keywords = {}
        for category, data in self.risk_categories.items():
            for keyword, weight in data['keywords']:
                self.all_risk_keywords[keyword.lower()] = {
                    'category': category,
                    'weight': weight,
                    'icon': data['icon']
                }
    
    def fetch_url(self, url, timeout=10):
        """Fetch URL vá»›i retry logic"""
        max_retries = 3
        for attempt in range(max_retries):
            try:
                response = self.session.get(url, timeout=timeout)
                response.raise_for_status()
                response.encoding = 'utf-8'
                return response
            except Exception as e:
                if attempt == max_retries - 1:
                    st.warning(f"âš ï¸ KhÃ´ng thá»ƒ truy cáº­p {url}: {str(e)}")
                    return None
                time.sleep(2)
        return None
    
    def parse_date_string(self, date_str):
        """Parse date tá»« string"""
        if not date_str:
            return datetime.now()
        
        try:
            date_str = date_str.strip().lower()
            
            # "X phÃºt trÆ°á»›c"
            if 'phÃºt' in date_str:
                minutes = int(re.search(r'(\d+)', date_str).group(1))
                return datetime.now() - timedelta(minutes=minutes)
            
            # "X giá» trÆ°á»›c"
            if 'giá»' in date_str or 'gio' in date_str:
                hours = int(re.search(r'(\d+)', date_str).group(1))
                return datetime.now() - timedelta(hours=hours)
            
            # "hÃ´m qua" hoáº·c "yesterday"
            if 'hÃ´m qua' in date_str or 'hom qua' in date_str:
                return datetime.now() - timedelta(days=1)
            
            # "hÃ´m nay" hoáº·c "today"
            if 'hÃ´m nay' in date_str or 'hom nay' in date_str or 'today' in date_str:
                return datetime.now()
            
            # Format: dd/mm/yyyy hoáº·c dd-mm-yyyy
            date_match = re.search(r'(\d{1,2})[/-](\d{1,2})[/-](\d{4})', date_str)
            if date_match:
                day, month, year = map(int, date_match.groups())
                return datetime(year, month, day)
            
            # Format: yyyy-mm-dd
            date_match = re.search(r'(\d{4})[/-](\d{1,2})[/-](\d{1,2})', date_str)
            if date_match:
                year, month, day = map(int, date_match.groups())
                return datetime(year, month, day)
                
        except Exception as e:
            pass
        
        return datetime.now()
    
    def scrape_cafef(self, max_articles=30):
        """CÃ o tin tá»« CafeF"""
        st.info("ğŸ”„ Äang cÃ o tá»« CafeF...")
        url = "https://cafef.vn/thi-truong-chung-khoan.chn"
        
        response = self.fetch_url(url)
        if not response:
            return
        
        try:
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # TÃ¬m cÃ¡c bÃ i viáº¿t
            articles = soup.find_all('div', class_=['item', 'tlitem'], limit=max_articles)
            
            for article in articles:
                try:
                    # TÃ¬m tiÃªu Ä‘á» vÃ  link
                    title_tag = article.find(['h3', 'h4', 'a'])
                    if not title_tag:
                        continue
                    
                    link_tag = title_tag.find('a') if title_tag.name != 'a' else title_tag
                    if not link_tag:
                        continue
                    
                    title = link_tag.get_text(strip=True)
                    article_url = urljoin(url, link_tag.get('href', ''))
                    
                    # TÃ¬m ngÃ y
                    date_tag = article.find('span', class_=['time', 'date', 'timeago'])
                    date_str = date_tag.get_text(strip=True) if date_tag else ''
                    
                    # TÃ¬m mÃ´ táº£/tÃ³m táº¯t
                    desc_tag = article.find(['p', 'div'], class_=['sapo', 'description', 'summary'])
                    description = desc_tag.get_text(strip=True) if desc_tag else ''
                    
                    if title and article_url:
                        self.all_articles.append({
                            'title': title,
                            'url': article_url,
                            'date_str': date_str,
                            'content': description,
                            'source': 'CafeF'
                        })
                        self.stats['cafef_articles'] += 1
                        
                except Exception as e:
                    continue
            
            st.success(f"âœ… CafeF: {self.stats['cafef_articles']} bÃ i")
            
        except Exception as e:
            st.error(f"âŒ Lá»—i khi cÃ o CafeF: {str(e)}")
            self.stats['errors'] += 1
    
    def scrape_vietstock(self, max_articles=30):
        """CÃ o tin tá»« VietStock"""
        st.info("ğŸ”„ Äang cÃ o tá»« VietStock...")
        url = "https://vietstock.vn/chung-khoan.htm"
        
        response = self.fetch_url(url)
        if not response:
            return
        
        try:
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # TÃ¬m cÃ¡c bÃ i viáº¿t
            articles = soup.find_all(['div', 'li'], class_=['news-item', 'list-news-item', 'item'], limit=max_articles)
            
            for article in articles:
                try:
                    # TÃ¬m tiÃªu Ä‘á» vÃ  link
                    title_tag = article.find('a', class_=['title', 'news-title'])
                    if not title_tag:
                        title_tag = article.find('a')
                    
                    if not title_tag:
                        continue
                    
                    title = title_tag.get_text(strip=True)
                    article_url = urljoin(url, title_tag.get('href', ''))
                    
                    # TÃ¬m ngÃ y
                    date_tag = article.find(['span', 'time'], class_=['time', 'date', 'news-time'])
                    date_str = date_tag.get_text(strip=True) if date_tag else ''
                    
                    # TÃ¬m mÃ´ táº£
                    desc_tag = article.find(['p', 'div'], class_=['summary', 'description', 'news-summary'])
                    description = desc_tag.get_text(strip=True) if desc_tag else ''
                    
                    if title and article_url and 'vietstock.vn' in article_url:
                        self.all_articles.append({
                            'title': title,
                            'url': article_url,
                            'date_str': date_str,
                            'content': description,
                            'source': 'VietStock'
                        })
                        self.stats['vietstock_articles'] += 1
                        
                except Exception as e:
                    continue
            
            st.success(f"âœ… VietStock: {self.stats['vietstock_articles']} bÃ i")
            
        except Exception as e:
            st.error(f"âŒ Lá»—i khi cÃ o VietStock: {str(e)}")
            self.stats['errors'] += 1
    
    def analyze_risk(self, title, content):
        """PhÃ¢n tÃ­ch rá»§i ro"""
        text = (title + " " + content).lower()
        
        matched = []
        total_score = 0
        categories_found = set()
        
        for keyword, info in self.all_risk_keywords.items():
            if keyword in text:
                matched.append({
                    'keyword': keyword,
                    'weight': info['weight'],
                    'category': info['category'],
                    'icon': info['icon']
                })
                total_score += info['weight']
                categories_found.add(info['category'])
        
        risk_score = min(total_score * 5, 100)
        
        if risk_score >= 50 or total_score >= 10:
            risk_level = 'ğŸ”´ NghiÃªm trá»ng'
            alert = 'HIGH_RISK'
        elif risk_score >= 25 or total_score >= 6:
            risk_level = 'ğŸŸ¡ Cáº£nh bÃ¡o'
            alert = 'MEDIUM_RISK'
        else:
            risk_level = 'âšª BÃ¬nh thÆ°á»ng'
            alert = 'NORMAL'
        
        return {
            'risk_score': risk_score,
            'risk_level': risk_level,
            'matched_keywords': matched,
            'categories': list(categories_found),
            'alert': alert,
            'keyword_count': len(matched)
        }
    
    def is_market_general_article(self, title, content=""):
        """Kiá»ƒm tra tin tá»•ng quan"""
        title_lower = title.lower()
        
        for keyword in self.title_blacklist:
            if keyword in title_lower:
                return True
        
        for pattern in self.title_blacklist_patterns:
            if re.search(pattern, title_lower):
                return True
        
        has_stock_code = False
        for pattern in self.required_indicators:
            if re.search(pattern, title.upper()):
                has_stock_code = True
                break
        
        if not has_stock_code and content:
            for pattern in self.required_indicators:
                if re.search(pattern, content.upper()):
                    has_stock_code = True
                    break
        
        return not has_stock_code
    
    def extract_stock_codes(self, text):
        """TrÃ­ch xuáº¥t mÃ£ CK"""
        if not text:
            return []
        
        text_upper = text.upper()
        pattern = r'\b([A-Z]{3,4})\b'
        matches = re.findall(pattern, text_upper)
        
        english_words = {'THE', 'AND', 'FOR', 'ARE', 'BUT', 'NOT', 'YOU', 'ALL', 
                       'CAN', 'HER', 'WAS', 'ONE', 'OUR', 'OUT', 'DAY', 'GET',
                       'HAS', 'HIM', 'HIS', 'HOW', 'MAN', 'NEW', 'NOW', 'OLD',
                       'SEE', 'TWO', 'WAY', 'WHO', 'BOY', 'DID', 'ITS', 'LET',
                       'VIA', 'PER', 'YET', 'NOR', 'OFF', 'PUT', 'RUN', 'TOP'}
        
        return list(set([code for code in matches if code not in english_words]))
    
    def process_articles(self):
        """Xá»­ lÃ½ táº¥t cáº£ bÃ i viáº¿t Ä‘Ã£ cÃ o"""
        results = []
        
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        total = len(self.all_articles)
        
        for idx, article in enumerate(self.all_articles):
            # Update progress
            progress = (idx + 1) / total
            progress_bar.progress(progress)
            status_text.text(f"Äang xá»­ lÃ½: {idx + 1}/{total} bÃ i...")
            
            self.stats['total_crawled'] += 1
            
            title = article['title']
            content = article['content']
            url = article['url']
            source = article['source']
            date_str = article['date_str']
            
            # Parse date
            date_obj = self.parse_date_string(date_str)
            formatted_date = date_obj.strftime('%d/%m/%Y %H:%M')
            
            # Lá»c theo thá»i gian
            if date_obj < self.cutoff_time:
                self.stats['time_filtered'] += 1
                continue
            
            # Lá»c tin tá»•ng quan
            if self.is_market_general_article(title, content):
                self.stats['market_general_filtered'] += 1
                continue
            
            # TrÃ­ch xuáº¥t mÃ£ CK
            stock_codes = self.extract_stock_codes(title + " " + content)
            if not stock_codes:
                continue
            
            # PhÃ¢n tÃ­ch rá»§i ro
            risk = self.analyze_risk(title, content)
            
            # Update stats
            if risk['alert'] == 'HIGH_RISK':
                self.stats['high_risk'] += 1
            elif risk['alert'] == 'MEDIUM_RISK':
                self.stats['medium_risk'] += 1
            else:
                self.stats['normal'] += 1
            
            self.stats['specific_stocks'] += 1
            
            # Format keywords
            keyword_str = ""
            if risk['matched_keywords']:
                kw_list = [f"{kw['icon']} {kw['keyword']}" for kw in risk['matched_keywords'][:3]]
                keyword_str = "; ".join(kw_list)
                if len(risk['matched_keywords']) > 3:
                    keyword_str += f" (+{len(risk['matched_keywords'])-3})"
            
            results.append({
                'Má»©c Ä‘á»™': risk['risk_level'],
                'Äiá»ƒm rá»§i ro': risk['risk_score'],
                'TiÃªu Ä‘á»': title,
                'MÃ£ CK': ', '.join(stock_codes),
                'Sá»‘ mÃ£': len(stock_codes),
                'NgÃ y': formatted_date,
                'Keyword nguy cÆ¡': keyword_str,
                'Sá»‘ keyword': risk['keyword_count'],
                'NhÃ³m': '; '.join(risk['categories']) if risk['categories'] else '',
                'TÃ³m táº¯t': content[:200] + '...' if len(content) > 200 else content,
                'URL': url,
                'Nguá»“n': source
            })
        
        progress_bar.empty()
        status_text.empty()
        
        return pd.DataFrame(results)
    
    def run(self, max_articles_per_source=30):
        """Cháº¡y toÃ n bá»™ scraper"""
        # CÃ o tá»« 2 nguá»“n
        self.scrape_cafef(max_articles=max_articles_per_source)
        time.sleep(1)  # Delay giá»¯a cÃ¡c requests
        self.scrape_vietstock(max_articles=max_articles_per_source)
        
        if not self.all_articles:
            st.error("âŒ KhÃ´ng cÃ o Ä‘Æ°á»£c tin nÃ o. Vui lÃ²ng thá»­ láº¡i sau.")
            return pd.DataFrame()
        
        st.info(f"ğŸ“Š Äang phÃ¢n tÃ­ch {len(self.all_articles)} bÃ i viáº¿t...")
        
        # Xá»­ lÃ½ vÃ  phÃ¢n tÃ­ch
        df = self.process_articles()
        
        return df

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STREAMLIT APP
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def main():
    # Header
    st.markdown('<div class="main-header">ğŸ“° Stock News Scraper & Risk Analyzer</div>', unsafe_allow_html=True)
    st.markdown("### ğŸŒ Dá»¯ liá»‡u thá»±c tá»« CafeF & VietStock")
    st.markdown("---")
    
    # Sidebar
    with st.sidebar:
        st.header("âš™ï¸ Cáº¥u hÃ¬nh")
        
        # Time filter
        time_option = st.selectbox(
            "Khoáº£ng thá»i gian",
            ["6 giá»", "12 giá»", "24 giá» (1 ngÃ y)", "48 giá» (2 ngÃ y)", "72 giá» (3 ngÃ y)", "168 giá» (1 tuáº§n)"],
            index=2
        )
        time_hours = int(time_option.split()[0])
        
        # Articles limit
        max_articles = st.slider(
            "Sá»‘ bÃ i tá»‘i Ä‘a má»—i nguá»“n",
            min_value=10,
            max_value=50,
            value=30,
            step=5
        )
        
        # Risk filter
        risk_filter = st.multiselect(
            "Lá»c theo má»©c Ä‘á»™ rá»§i ro",
            ["ğŸ”´ NghiÃªm trá»ng", "ğŸŸ¡ Cáº£nh bÃ¡o", "âšª BÃ¬nh thÆ°á»ng"],
            default=["ğŸ”´ NghiÃªm trá»ng", "ğŸŸ¡ Cáº£nh bÃ¡o", "âšª BÃ¬nh thÆ°á»ng"]
        )
        
        st.markdown("---")
        
        # Run button
        if st.button("ğŸš€ Báº®T Äáº¦U CÃ€O TIN", type="primary", use_container_width=True):
            st.session_state['run_scraper'] = True
        
        st.markdown("---")
        
        # Info
        with st.expander("â„¹ï¸ ThÃ´ng tin"):
            st.markdown("""
            **Nguá»“n dá»¯ liá»‡u:**
            - âœ… CafeF (Real-time)
            - âœ… VietStock (Real-time)
            
            **TÃ­nh nÄƒng:**
            - Lá»c tin tá»•ng quan thá»‹ trÆ°á»ng
            - PhÃ¢n tÃ­ch 6 nhÃ³m keyword nguy cÆ¡
            - TÃ­nh Ä‘iá»ƒm rá»§i ro tá»± Ä‘á»™ng
            - Xuáº¥t Excel
            
            **Version:** 2.2  
            **Updated:** 15/10/2025
            """)
    
    # Main content
    if 'run_scraper' not in st.session_state:
        st.session_state['run_scraper'] = False
    
    if st.session_state['run_scraper']:
        scraper = StockScraperV2(time_filter_hours=time_hours)
        df = scraper.run(max_articles_per_source=max_articles)
        
        if not df.empty:
            # Filter by risk level
            if risk_filter:
                df = df[df['Má»©c Ä‘á»™'].isin(risk_filter)]
            
            # Sort by risk score descending
            df = df.sort_values('Äiá»ƒm rá»§i ro', ascending=False)
            
            st.session_state['df'] = df
            st.session_state['stats'] = scraper.stats
            st.session_state['scraper'] = scraper
        
        st.success('âœ… HoÃ n táº¥t!')
        st.session_state['run_scraper'] = False
    
    if 'df' in st.session_state:
        df = st.session_state['df']
        stats = st.session_state['stats']
        scraper = st.session_state['scraper']
        
        # Stats
        st.subheader("ğŸ“Š Thá»‘ng kÃª")
        col1, col2, col3, col4, col5 = st.columns(5)
        
        with col1:
            st.markdown('<div class="stats-card">', unsafe_allow_html=True)
            st.metric("Tá»•ng tin", stats['specific_stocks'])
            st.markdown('</div>', unsafe_allow_html=True)
        
        with col2:
            st.markdown('<div class="stats-card">', unsafe_allow_html=True)
            st.metric("ğŸ”´ NghiÃªm trá»ng", stats['high_risk'])
            st.markdown('</div>', unsafe_allow_html=True)
        
        with col3:
            st.markdown('<div class="stats-card">', unsafe_allow_html=True)
            st.metric("ğŸŸ¡ Cáº£nh bÃ¡o", stats['medium_risk'])
            st.markdown('</div>', unsafe_allow_html=True)
        
        with col4:
            st.markdown('<div class="stats-card">', unsafe_allow_html=True)
            st.metric("âšª BÃ¬nh thÆ°á»ng", stats['normal'])
            st.markdown('</div>', unsafe_allow_html=True)
        
        with col5:
            st.markdown('<div class="stats-card">', unsafe_allow_html=True)
            st.metric("ğŸ“° Nguá»“n", f"CafeF: {stats['cafef_articles']}\nVietStock: {stats['vietstock_articles']}")
            st.markdown('</div>', unsafe_allow_html=True)
        
        # Additional stats
        st.markdown("---")
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.info(f"ğŸ“¥ ÄÃ£ cÃ o: {stats['total_crawled']} bÃ i")
        
        with col2:
            st.warning(f"âŒ ÄÃ£ lá»c (tá»•ng quan): {stats['market_general_filtered']} bÃ i")
        
        with col3:
            st.warning(f"â° ÄÃ£ lá»c (thá»i gian): {stats['time_filtered']} bÃ i")
        
        st.markdown("---")
        
        # Data table
        st.subheader("ğŸ“‹ Káº¿t quáº£")
        
        # Display options
        col1, col2 = st.columns([3, 1])
        with col1:
            st.write(f"Hiá»ƒn thá»‹ {len(df)} bÃ i viáº¿t")
        with col2:
            view_mode = st.selectbox("Cháº¿ Ä‘á»™ xem", ["Báº£ng Ä‘áº§y Ä‘á»§", "Chá»‰ tin quan trá»ng"], index=0)
        
        # Filter for important news only
        if view_mode == "Chá»‰ tin quan trá»ng":
            df_display = df[df['Má»©c Ä‘á»™'].isin(['ğŸ”´ NghiÃªm trá»ng', 'ğŸŸ¡ Cáº£nh bÃ¡o'])]
        else:
            df_display = df
        
        # Display dataframe
        st.dataframe(
            df_display,
            use_container_width=True,
            height=500,
            column_config={
                "URL": st.column_config.LinkColumn("URL"),
                "Äiá»ƒm rá»§i ro": st.column_config.ProgressColumn(
                    "Äiá»ƒm rá»§i ro",
                    format="%d",
                    min_value=0,
                    max_value=100,
                ),
            }
        )
        
        # Export section
        st.markdown("---")
        st.subheader("ğŸ’¾ Xuáº¥t dá»¯ liá»‡u")
        
        col1, col2, col3 = st.columns([2, 1, 1])
        
        with col1:
            st.write("Táº£i xuá»‘ng dá»¯ liá»‡u dÆ°á»›i dáº¡ng Excel Ä‘á»ƒ phÃ¢n tÃ­ch thÃªm")
        
        with col2:
            # Excel export
            output = BytesIO()
            with pd.ExcelWriter(output, engine='openpyxl') as writer:
                df.to_excel(writer, index=False, sheet_name='Stock News')
                
                # Add stats sheet
                stats_df = pd.DataFrame([stats])
                stats_df.to_excel(writer, index=False, sheet_name='Statistics')
            
            st.download_button(
                label="ğŸ“¥ Táº£i Excel (Táº¥t cáº£)",
                data=output.getvalue(),
                file_name=f"stock_news_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                use_container_width=True
            )
        
        with col3:
            # CSV export
            csv = df.to_csv(index=False, encoding='utf-8-sig')
            st.download_button(
                label="ğŸ“¥ Táº£i CSV",
                data=csv,
                file_name=f"stock_news_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                mime="text/csv",
                use_container_width=True
            )
        
        # Detailed analysis
        st.markdown("---")
        
        with st.expander("ğŸ“Š PhÃ¢n tÃ­ch chi tiáº¿t"):
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("### Top 10 mÃ£ CK Ä‘Æ°á»£c nháº¯c nhiá»u nháº¥t")
                # Count stock codes
                all_codes = []
                for codes in df['MÃ£ CK']:
                    all_codes.extend([c.strip() for c in codes.split(',')])
                
                if all_codes:
                    code_counts = pd.Series(all_codes).value_counts().head(10)
                    st.bar_chart(code_counts)
                else:
                    st.info("KhÃ´ng cÃ³ dá»¯ liá»‡u")
            
            with col2:
                st.markdown("### PhÃ¢n bá»‘ theo má»©c Ä‘á»™ rá»§i ro")
                risk_counts = df['Má»©c Ä‘á»™'].value_counts()
                st.bar_chart(risk_counts)
        
        # Keyword table
        with st.expander("ğŸ“‹ Báº£ng 6 nhÃ³m keyword nguy cÆ¡"):
            for category, data in scraper.risk_categories.items():
                st.markdown(f"### {data['icon']} {category}")
                keywords = [kw for kw, _ in data['keywords']]
                # Display in columns for better readability
                cols = st.columns(3)
                for i, keyword in enumerate(keywords):
                    with cols[i % 3]:
                        st.write(f"â€¢ {keyword}")
                st.markdown("---")
        
        # Refresh button
        st.markdown("---")
        if st.button("ğŸ”„ CÃ o láº¡i tin má»›i", use_container_width=True):
            st.session_state.clear()
            st.rerun()
    
    else:
        # Welcome screen
        st.info("ğŸ‘ˆ Chá»n cáº¥u hÃ¬nh vÃ  nháº¥n **Báº®T Äáº¦U CÃ€O TIN** Ä‘á»ƒ báº¯t Ä‘áº§u")
        
        st.markdown("### ğŸ¯ TÃ­nh nÄƒng chÃ­nh")
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("""
            **ğŸ” Lá»c thÃ´ng minh**
            - Loáº¡i bá» tin tá»•ng quan thá»‹ trÆ°á»ng
            - Chá»‰ giá»¯ tin vá» cá»• phiáº¿u cá»¥ thá»ƒ
            - Lá»c theo khoáº£ng thá»i gian
            
            **ğŸ“Š PhÃ¢n tÃ­ch rá»§i ro**
            - 6 nhÃ³m keyword nguy cÆ¡
            - TÃ­nh Ä‘iá»ƒm rá»§i ro tá»± Ä‘á»™ng (0-100)
            - PhÃ¢n loáº¡i 3 má»©c Ä‘á»™
            """)
        
        with col2:
            st.markdown("""
            **ğŸ“° Nguá»“n tin THá»°C**
            - âœ… CafeF (Real-time)
            - âœ… VietStock (Real-time)
            
            **ğŸ’¾ Xuáº¥t dá»¯ liá»‡u**
            - File Excel Ä‘áº§y Ä‘á»§
            - File CSV
            - Báº£ng thá»‘ng kÃª chi tiáº¿t
            """)
        
        st.markdown("---")
        
        # Usage guide
        with st.expander("ğŸ“– HÆ°á»›ng dáº«n sá»­ dá»¥ng"):
            st.markdown("""
            ### CÃ¡c bÆ°á»›c sá»­ dá»¥ng:
            
            1. **Chá»n khoáº£ng thá»i gian**: Tá»« 6 giá» Ä‘áº¿n 1 tuáº§n
            2. **Äiá»u chá»‰nh sá»‘ bÃ i**: 10-50 bÃ i má»—i nguá»“n
            3. **Chá»n má»©c Ä‘á»™ rá»§i ro**: Lá»c tin cáº§n xem
            4. **Nháº¥n "Báº®T Äáº¦U CÃ€O TIN"**: Äá»£i 30-60 giÃ¢y
            5. **Xem káº¿t quáº£**: Báº£ng tin vá»›i Ä‘iá»ƒm rá»§i ro
            6. **Táº£i Excel/CSV**: PhÃ¢n tÃ­ch thÃªm náº¿u cáº§n
            
            ### Giáº£i thÃ­ch má»©c Ä‘á»™ rá»§i ro:
            
            - ğŸ”´ **NghiÃªm trá»ng** (Ä‘iá»ƒm â‰¥ 50): Tin cÃ³ nhiá»u keyword nguy hiá»ƒm nhÆ° "lÃ£nh Ä‘áº¡o bá»‹ báº¯t", "gian láº­n tÃ i chÃ­nh", "Ä‘á»™i lÃ¡i"...
            - ğŸŸ¡ **Cáº£nh bÃ¡o** (Ä‘iá»ƒm 25-49): Tin cÃ³ keyword cáº§n lÆ°u Ã½ nhÆ° "bÃ¡o lá»—", "M&A", "sá»± cá»‘"...
            - âšª **BÃ¬nh thÆ°á»ng** (Ä‘iá»ƒm < 25): Tin thÃ´ng thÆ°á»ng vá» hoáº¡t Ä‘á»™ng doanh nghiá»‡p
            
            ### Tips:
            
            - Cháº¡y vÃ o **buá»•i sÃ¡ng** Ä‘á»ƒ cÃ³ tin má»›i nháº¥t
            - Lá»c chá»‰ xem tin **NghiÃªm trá»ng** Ä‘á»ƒ tiáº¿t kiá»‡m thá»i gian
            - **Táº£i Excel** Ä‘á»ƒ lÆ°u trá»¯ vÃ  theo dÃµi dÃ i háº¡n
            - Cháº¡y láº¡i **má»—i vÃ i giá»** Ä‘á»ƒ cáº­p nháº­t tin má»›i
            """)
        
        # Sample data preview
        with st.expander("ğŸ‘ï¸ Xem dá»¯ liá»‡u máº«u"):
            sample_data = pd.DataFrame([
                {
                    'Má»©c Ä‘á»™': 'ğŸ”´ NghiÃªm trá»ng',
                    'Äiá»ƒm rá»§i ro': 85,
                    'TiÃªu Ä‘á»': 'FPT: Chá»§ tá»‹ch báº¥t ngá» thoÃ¡i vá»‘n, cá»• Ä‘Ã´ng ná»™i bá»™ bÃ¡n chui',
                    'MÃ£ CK': 'FPT',
                    'NgÃ y': '15/10/2025 08:30',
                    'Keyword nguy cÆ¡': 'ğŸ‘¥ chá»§ tá»‹ch thoÃ¡i vá»‘n; ğŸ‘¥ bÃ¡n chui',
                    'Nguá»“n': 'CafeF'
                },
                {
                    'Má»©c Ä‘á»™': 'ğŸŸ¡ Cáº£nh bÃ¡o',
                    'Äiá»ƒm rá»§i ro': 35,
                    'TiÃªu Ä‘á»': 'HPG: HÃ²a PhÃ¡t báº¥t ngá» bÃ¡o lá»— quÃ½ 3/2025',
                    'MÃ£ CK': 'HPG',
                    'NgÃ y': '15/10/2025 07:15',
                    'Keyword nguy cÆ¡': 'ğŸ’° báº¥t ngá» bÃ¡o lá»—',
                    'Nguá»“n': 'VietStock'
                },
                {
                    'Má»©c Ä‘á»™': 'âšª BÃ¬nh thÆ°á»ng',
                    'Äiá»ƒm rá»§i ro': 0,
                    'TiÃªu Ä‘á»': 'VNM: Vinamilk cÃ´ng bá»‘ káº¿ hoáº¡ch má»Ÿ rá»™ng thá»‹ trÆ°á»ng',
                    'MÃ£ CK': 'VNM',
                    'NgÃ y': '15/10/2025 06:45',
                    'Keyword nguy cÆ¡': '',
                    'Nguá»“n': 'CafeF'
                }
            ])
            
            st.dataframe(sample_data, use_container_width=True)

if __name__ == "__main__":
    main()
