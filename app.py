# ============================================================
# üéØ STREAMLIT WEB APP V2.2 - UPLOAD STOCK LIST
# ============================================================
# T√≠nh nƒÉng m·ªõi: Upload Excel/CSV danh s√°ch m√£ CK
# ============================================================

import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime, timedelta, timezone
import time
import re
from urllib.parse import urljoin
import io

# ============================================================
# CONFIG
# ============================================================

st.set_page_config(
    page_title="C√†o Tin Ch·ª©ng Kho√°n V2.2",
    page_icon="üìà",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ============================================================
# CSS CUSTOM
# ============================================================

st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 1rem;
    }
    .sub-header {
        font-size: 1.2rem;
        color: #666;
        text-align: center;
        margin-bottom: 2rem;
    }
    .upload-box {
        background-color: #e8f4f8;
        padding: 1.5rem;
        border-radius: 0.5rem;
        border: 2px dashed #1f77b4;
        margin: 1rem 0;
    }
    .severe-card {
        background-color: #ffe6e6;
        border-left: 5px solid #ff4444;
        padding: 1rem;
        margin: 0.5rem 0;
        border-radius: 0.3rem;
    }
    .warning-card {
        background-color: #fff8e6;
        border-left: 5px solid #ffaa00;
        padding: 1rem;
        margin: 0.5rem 0;
        border-radius: 0.3rem;
    }
    .positive-card {
        background-color: #e6ffe6;
        border-left: 5px solid #44ff44;
        padding: 1rem;
        margin: 0.5rem 0;
        border-radius: 0.3rem;
    }
    .metric-card {
        background-color: #f0f2f6;
        padding: 1rem;
        margin: 0.5rem 0;
        border-radius: 0.3rem;
    }
</style>
""", unsafe_allow_html=True)

# ============================================================
# HELPER FUNCTIONS
# ============================================================

def load_default_stock_list():
    """Danh s√°ch m√£ m·∫∑c ƒë·ªãnh (backup)"""
    default_data = {
        'M√£ CK': ['SHS', 'PVS', 'NVB', 'VCS', 'BVS', 'APS', 'MBS', 'CEO', 'VGC', 'PVC',
                  'LPB', 'EIB', 'BAB', 'OCB', 'BMI', 'HDG', 'PAN', 'NTL'],
        'S√†n': ['HNX']*10 + ['UPCoM']*8,
        'T√™n c√¥ng ty': ['Ch·ª©ng kho√°n SHS', 'Ch·ª©ng kho√°n PVS', 'Ng√¢n h√†ng NVB', 'Ch·ª©ng kho√°n VCS',
                        'Ch·ª©ng kho√°n BVS', 'Ch·ª©ng kho√°n APS', 'Ch·ª©ng kho√°n MBS', 'T·∫≠p ƒëo√†n CEO',
                        'Viglacera', 'PVC', 'Ng√¢n h√†ng LPB', 'Ng√¢n h√†ng EIB', 'Ng√¢n h√†ng BAB',
                        'Ng√¢n h√†ng OCB', 'B·∫£o hi·ªÉm BMI', 'T·∫≠p ƒëo√†n HDG', 'PAN Group', 'NTL Logistics']
    }
    return pd.DataFrame(default_data)

def parse_stock_file(uploaded_file):
    """Parse Excel/CSV file"""
    try:
        # ƒê·ªçc file
        if uploaded_file.name.endswith('.csv'):
            df = pd.read_csv(uploaded_file)
        else:
            df = pd.read_excel(uploaded_file)
        
        # Chu·∫©n h√≥a t√™n c·ªôt
        df.columns = df.columns.str.strip().str.lower()
        
        # Map c√°c t√™n c·ªôt c√≥ th·ªÉ c√≥
        column_mapping = {
            'm√£ ck': 'M√£ CK',
            'ma ck': 'M√£ CK',
            'm√£': 'M√£ CK',
            'ma': 'M√£ CK',
            'code': 'M√£ CK',
            'stock_code': 'M√£ CK',
            'ticker': 'M√£ CK',
            
            's√†n': 'S√†n',
            'san': 'S√†n',
            'exchange': 'S√†n',
            'm√£ s√†n': 'S√†n',
            'ma san': 'S√†n',
            
            't√™n c√¥ng ty': 'T√™n c√¥ng ty',
            'ten cong ty': 'T√™n c√¥ng ty',
            't√™n': 'T√™n c√¥ng ty',
            'ten': 'T√™n c√¥ng ty',
            'name': 'T√™n c√¥ng ty',
            'company': 'T√™n c√¥ng ty',
            'company_name': 'T√™n c√¥ng ty',
        }
        
        # Rename columns
        for old_col, new_col in column_mapping.items():
            if old_col in df.columns:
                df.rename(columns={old_col: new_col}, inplace=True)
        
        # Ki·ªÉm tra c√°c c·ªôt b·∫Øt bu·ªôc
        required_cols = ['M√£ CK', 'S√†n']
        missing_cols = [col for col in required_cols if col not in df.columns]
        
        if missing_cols:
            return None, f"Thi·∫øu c√°c c·ªôt: {', '.join(missing_cols)}"
        
        # Th√™m c·ªôt T√™n c√¥ng ty n·∫øu kh√¥ng c√≥
        if 'T√™n c√¥ng ty' not in df.columns:
            df['T√™n c√¥ng ty'] = ''
        
        # L√†m s·∫°ch d·ªØ li·ªáu
        df['M√£ CK'] = df['M√£ CK'].astype(str).str.strip().str.upper()
        df['S√†n'] = df['S√†n'].astype(str).str.strip().str.upper()
        df['T√™n c√¥ng ty'] = df['T√™n c√¥ng ty'].astype(str).str.strip()
        
        # L·ªçc ch·ªâ HNX v√† UPCoM
        df = df[df['S√†n'].isin(['HNX', 'UPCOM'])]
        
        # Chu·∫©n h√≥a UPCoM
        df['S√†n'] = df['S√†n'].replace('UPCOM', 'UPCoM')
        
        # B·ªè tr√πng
        df = df.drop_duplicates(subset=['M√£ CK'])
        
        return df, None
        
    except Exception as e:
        return None, f"L·ªói ƒë·ªçc file: {str(e)}"

def create_sample_excel():
    """T·∫°o file Excel m·∫´u"""
    sample_data = {
        'M√£ CK': ['SHS', 'PVS', 'NVB', 'LPB', 'EIB', 'CEO'],
        'S√†n': ['HNX', 'HNX', 'HNX', 'UPCoM', 'UPCoM', 'HNX'],
        'T√™n c√¥ng ty': ['Ch·ª©ng kho√°n S√†i G√≤n - H√† N·ªôi', 'Ch·ª©ng kho√°n D·∫ßu kh√≠', 
                        'Ng√¢n h√†ng Qu·ªëc d√¢n', 'Ng√¢n h√†ng L·ªôc Ph√°t', 
                        'Ng√¢n h√†ng Xu·∫•t nh·∫≠p kh·∫©u', 'T·∫≠p ƒëo√†n CEO']
    }
    df = pd.DataFrame(sample_data)
    
    buffer = io.BytesIO()
    with pd.ExcelWriter(buffer, engine='openpyxl') as writer:
        df.to_excel(writer, index=False, sheet_name='Danh s√°ch m√£')
    
    return buffer.getvalue()

# ============================================================
# SCRAPER CLASSES
# ============================================================

class KeywordRiskDetector:
    """Ph√°t hi·ªán keywords r·ªßi ro"""
    
    def __init__(self):
        self.keywords_db = {
            "l√£nh ƒë·∫°o b·ªã b·∫Øt": {"category": "A. N·ªôi b·ªô", "severity": "severe", "score": -95, "violation": "I.2, II.A"},
            "l√£nh ƒë·∫°o b·ªè tr·ªën": {"category": "A. N·ªôi b·ªô", "severity": "severe", "score": -95, "violation": "I.2, II.A"},
            "c·ªï ƒë√¥ng l·ªõn b√°n chui": {"category": "A. N·ªôi b·ªô", "severity": "severe", "score": -85, "violation": "I.1, II.A"},
            "ch·ªß t·ªãch b·∫•t ng·ªù tho√°i h·∫øt v·ªën": {"category": "A. N·ªôi b·ªô", "severity": "severe", "score": -85, "violation": "I.1, II.A"},
            "b·∫•t ng·ªù b√°o l·ªó": {"category": "B. T√†i ch√≠nh", "severity": "severe", "score": -80, "violation": "I.4, II.B"},
            "√¢m v·ªën ch·ªß": {"category": "B. T√†i ch√≠nh", "severity": "severe", "score": -90, "violation": "II.B"},
            "ƒë·ªôi l√°i l√†m gi√°": {"category": "C. Thao t√∫ng", "severity": "severe", "score": -95, "violation": "I.3, II.C"},
            "tƒÉng tr·∫ßn li√™n ti·∫øp": {"category": "C. Thao t√∫ng", "severity": "warning", "score": -60, "violation": "I.2, II.C"},
            "c√¥ng an ƒëi·ªÅu tra": {"category": "E. Ph√°p l√Ω", "severity": "severe", "score": -90, "violation": "II.E"},
            "l·ª£i nhu·∫≠n tƒÉng": {"category": "T√≠ch c·ª±c", "severity": "positive", "score": 70, "violation": ""},
            "tƒÉng tr∆∞·ªüng m·∫°nh": {"category": "T√≠ch c·ª±c", "severity": "positive", "score": 65, "violation": ""},
        }
    
    def analyze(self, text):
        text_lower = text.lower()
        found_keywords = []
        total_score = 0
        categories = set()
        violations = set()
        max_severity = "normal"
        
        for keyword, info in self.keywords_db.items():
            if keyword in text_lower:
                found_keywords.append({
                    "keyword": keyword,
                    "category": info["category"],
                    "severity": info["severity"],
                    "score": info["score"],
                    "violation": info["violation"]
                })
                total_score += info["score"]
                categories.add(info["category"])
                if info["violation"]:
                    violations.add(info["violation"])
                
                if info["severity"] == "severe":
                    max_severity = "severe"
                elif info["severity"] == "warning" and max_severity != "severe":
                    max_severity = "warning"
                elif info["severity"] == "positive" and max_severity == "normal":
                    max_severity = "positive"
        
        return {
            "keywords": found_keywords,
            "total_score": total_score,
            "severity": max_severity,
            "categories": list(categories),
            "violations": ", ".join(sorted(violations))
        }

class SimpleSentimentAnalyzer:
    """Ph√¢n t√≠ch sentiment"""
    
    def __init__(self):
        self.keyword_detector = KeywordRiskDetector()
        self.positive_words = ['tƒÉng', 'tƒÉng tr∆∞·ªüng', 'l·ª£i nhu·∫≠n', 'th√†nh c√¥ng', 't·ªët', 'cao', 'm·∫°nh', 'v∆∞·ª£t']
        self.negative_words = ['gi·∫£m', 's·ª•t gi·∫£m', 'l·ªó', 'thua l·ªó', 'kh√≥ khƒÉn', 'ti√™u c·ª±c', 'suy gi·∫£m']
    
    def analyze_sentiment(self, title, content):
        text = (title + " " + content).lower()
        keyword_analysis = self.keyword_detector.analyze(title + " " + content)
        
        pos_count = sum(1 for word in self.positive_words if word in text)
        neg_count = sum(1 for word in self.negative_words if word in text)
        
        base_score = 50 + (pos_count * 5) - (neg_count * 5)
        
        if keyword_analysis["severity"] == "severe":
            final_score = min(20, base_score + keyword_analysis["total_score"])
        elif keyword_analysis["severity"] == "warning":
            final_score = min(40, base_score + keyword_analysis["total_score"] * 0.7)
        elif keyword_analysis["severity"] == "positive":
            final_score = max(60, base_score + keyword_analysis["total_score"])
        else:
            final_score = base_score
        
        final_score = max(0, min(100, final_score))
        
        if final_score >= 60:
            label = "T√≠ch c·ª±c"
        elif final_score >= 40:
            label = "Trung l·∫≠p"
        else:
            label = "Ti√™u c·ª±c"
        
        if keyword_analysis["severity"] == "severe":
            risk_level = "Nghi√™m tr·ªçng"
        elif keyword_analysis["severity"] == "warning":
            risk_level = "C·∫£nh b√°o"
        elif keyword_analysis["severity"] == "positive":
            risk_level = "T√≠ch c·ª±c"
        else:
            risk_level = "B√¨nh th∆∞·ªùng"
        
        return {
            "sentiment_score": round(final_score, 1),
            "sentiment_label": label,
            "risk_level": risk_level,
            "keywords": keyword_analysis["keywords"],
            "categories": ", ".join(keyword_analysis["categories"]) if keyword_analysis["categories"] else "",
            "violations": keyword_analysis["violations"]
        }

class StockScraperWeb:
    """Scraper v·ªõi stock list t·ª´ file"""
    
    def __init__(self, stock_df, time_filter_hours=24):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept-Language': 'vi-VN,vi;q=0.9,en;q=0.8',
        }
        self.all_articles = []
        self.session = requests.Session()
        self.time_filter_hours = time_filter_hours
        
        self.vietnam_tz = timezone(timedelta(hours=7))
        self.cutoff_time = datetime.now(self.vietnam_tz) - timedelta(hours=time_filter_hours)
        
        self.sentiment_analyzer = SimpleSentimentAnalyzer()
        
        # Load stock list t·ª´ DataFrame
        self.stock_df = stock_df
        self.hnx_stocks = set(stock_df[stock_df['S√†n'] == 'HNX']['M√£ CK'].tolist())
        self.upcom_stocks = set(stock_df[stock_df['S√†n'] == 'UPCoM']['M√£ CK'].tolist())
        
        # T·∫°o dict: m√£ ‚Üí t√™n c√¥ng ty (cho t√¨m ki·∫øm theo t√™n)
        self.code_to_name = dict(zip(stock_df['M√£ CK'], stock_df['T√™n c√¥ng ty']))
        
        # T·∫°o dict: t√™n ‚Üí m√£ (lowercase ƒë·ªÉ search)
        self.name_to_code = {}
        for code, name in self.code_to_name.items():
            if name:
                # T√°ch t·ª´ trong t√™n c√¥ng ty
                words = name.lower().split()
                for word in words:
                    if len(word) > 3:  # B·ªè qua t·ª´ qu√° ng·∫Øn
                        if word not in self.name_to_code:
                            self.name_to_code[word] = []
                        self.name_to_code[word].append(code)
        
        self.stock_to_exchange = {}
        for code in self.hnx_stocks:
            self.stock_to_exchange[code] = 'HNX'
        for code in self.upcom_stocks:
            self.stock_to_exchange[code] = 'UPCoM'
        
        self.stats = {
            'total_crawled': 0,
            'hnx_found': 0,
            'upcom_found': 0,
            'severe_risk': 0,
            'warning_risk': 0,
            'found_by_code': 0,
            'found_by_name': 0
        }
    
    def clean_text(self, text):
        if not text:
            return ""
        text = re.sub(r'[^\w\s.,;:!?()%\-\+\/\"\'√†√°·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√®√©·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±·ª≥√Ω·ª∑·ªπ·ªµƒë√Ä√Å·∫¢√É·∫†ƒÇ·∫Æ·∫∞·∫≤·∫¥·∫∂√Ç·∫§·∫¶·∫®·∫™·∫¨√à√â·∫∫·∫º·∫∏√ä·∫æ·ªÄ·ªÇ·ªÑ·ªÜ√å√ç·ªàƒ®·ªä√í√ì·ªé√ï·ªå√î·ªê·ªí·ªî·ªñ·ªò∆†·ªö·ªú·ªû·ª†·ª¢√ô√ö·ª¶≈®·ª§∆Ø·ª®·ª™·ª¨·ªÆ·ª∞·ª≤√ù·ª∂·ª∏·ª¥ƒê]', ' ', text)
        text = re.sub(r'\s+', ' ', text)
        return text.strip()
    
    def extract_stock(self, text):
        """
        Tr√≠ch xu·∫•t m√£ CK - T√åM THEO M√É V√Ä T√äN
        Priority:
        1. T√¨m theo m√£ CK
        2. N·∫øu kh√¥ng c√≥, t√¨m theo t√™n c√¥ng ty
        """
        text_upper = text.upper()
        text_lower = text.lower()
        
        # Blacklist
        blacklist_patterns = [
            r'CH·ª®NG KHO√ÅN\s+\w+\s+C√ì\s+NH·∫¨N ƒê·ªäNH',
            r'CH·ª®NG KHO√ÅN\s+\w+\s+D·ª∞ B√ÅO',
            r'CH·ª®NG KHO√ÅN\s+\w+\s+PH√ÇN T√çCH',
            r'C√îNG TY\s+CH·ª®NG KHO√ÅN',
            r'CTCK\s+\w+',
            r'VN-INDEX',
            r'HNX-INDEX',
            r'UPCOM-INDEX',
        ]
        
        for pattern in blacklist_patterns:
            if re.search(pattern, text_upper):
                return None, None, None
        
        # METHOD 1: T√¨m theo M√É CK
        for code in self.hnx_stocks:
            match = re.search(r'\b' + code + r'\b', text_upper)
            if match:
                context = text_upper[max(0, match.start()-10):match.end()+10]
                
                if re.search(r'CH·ª®NG KHO√ÅN\s+' + code, context):
                    continue
                if re.search(r'CTCK\s+' + code, context):
                    continue
                
                return code, 'HNX', 'code'
        
        for code in self.upcom_stocks:
            match = re.search(r'\b' + code + r'\b', text_upper)
            if match:
                context = text_upper[max(0, match.start()-10):match.end()+10]
                
                if re.search(r'CH·ª®NG KHO√ÅN\s+' + code, context):
                    continue
                if re.search(r'CTCK\s+' + code, context):
                    continue
                
                return code, 'UPCoM', 'code'
        
        # METHOD 2: T√¨m theo T√äN C√îNG TY
        # T√°ch t·ª´ trong text
        words = text_lower.split()
        matched_codes = []
        
        for word in words:
            if len(word) > 3 and word in self.name_to_code:
                matched_codes.extend(self.name_to_code[word])
        
        if matched_codes:
            # L·∫•y m√£ xu·∫•t hi·ªán nhi·ªÅu nh·∫•t
            from collections import Counter
            most_common = Counter(matched_codes).most_common(1)[0][0]
            exchange = self.stock_to_exchange.get(most_common)
            return most_common, exchange, 'name'
        
        return None, None, None
    
    def fetch_url(self, url, max_retries=2):
        for attempt in range(max_retries):
            try:
                response = self.session.get(url, headers=self.headers, timeout=15)
                response.raise_for_status()
                return response
            except:
                if attempt < max_retries - 1:
                    time.sleep(1)
                return None
    
    def scrape_source(self, url, source_name, pattern, max_articles=20, progress_callback=None):
        """C√†o t·ª´ m·ªôt ngu·ªìn"""
        try:
            response = self.fetch_url(url)
            if not response:
                return 0
            
            response.encoding = 'utf-8'
            soup = BeautifulSoup(response.text, 'html.parser')
            
            count = 0
            seen = set()
            
            links = soup.find_all('a', href=True)
            total_links = len(links)
            
            for idx, link_tag in enumerate(links):
                if progress_callback:
                    progress = (idx + 1) / total_links
                    progress_callback(f"{source_name}: {idx+1}/{total_links}", progress)
                
                href = link_tag.get('href', '')
                
                if pattern(href) and href not in seen:
                    title = link_tag.get_text(strip=True)
                    
                    if title and len(title) > 30:
                        self.stats['total_crawled'] += 1
                        seen.add(href)
                        
                        stock_code, exchange, match_method = self.extract_stock(title)
                        
                        if stock_code and exchange in ['HNX', 'UPCoM']:
                            full_link = urljoin(url, href)
                            
                            # Track method
                            if match_method == 'code':
                                self.stats['found_by_code'] += 1
                            else:
                                self.stats['found_by_name'] += 1
                            
                            # L·∫•y t√™n c√¥ng ty
                            company_name = self.code_to_name.get(stock_code, '')
                            
                            # Sentiment analysis
                            sentiment_result = self.sentiment_analyzer.analyze_sentiment(title, "")
                            
                            if exchange == 'HNX':
                                self.stats['hnx_found'] += 1
                            else:
                                self.stats['upcom_found'] += 1
                            
                            if sentiment_result['risk_level'] == 'Nghi√™m tr·ªçng':
                                self.stats['severe_risk'] += 1
                            elif sentiment_result['risk_level'] == 'C·∫£nh b√°o':
                                self.stats['warning_risk'] += 1
                            
                            self.all_articles.append({
                                'Ti√™u ƒë·ªÅ': title,
                                'Link': full_link,
                                'Ng√†y': datetime.now(self.vietnam_tz).strftime('%d/%m/%Y'),
                                'M√£ CK': stock_code,
                                'T√™n c√¥ng ty': company_name,
                                'S√†n': exchange,
                                'Sentiment': sentiment_result['sentiment_label'],
                                'ƒêi·ªÉm': sentiment_result['sentiment_score'],
                                'Risk': sentiment_result['risk_level'],
                                'Vi ph·∫°m': sentiment_result['violations'],
                                'Keywords': "; ".join([k['keyword'] for k in sentiment_result['keywords'][:3]]),
                                'T√¨m theo': 'M√£ CK' if match_method == 'code' else 'T√™n c√¥ng ty'
                            })
                            
                            count += 1
                            
                            if count >= max_articles:
                                break
            
            return count
        
        except Exception as e:
            st.error(f"L·ªói {source_name}: {str(e)}")
            return 0
    
    def run(self, max_articles_per_source=20, progress_callback=None):
        """Ch·∫°y scraper"""
        sources = [
            ("https://cafef.vn/thi-truong-chung-khoan.chn", "CafeF", lambda h: '.chn' in h),
            ("https://vietstock.vn/chung-khoan.htm", "VietStock", lambda h: re.search(r'/\d{4}/\d{2}/.+\.htm', h)),
        ]
        
        for url, name, pattern in sources:
            self.scrape_source(url, name, pattern, max_articles_per_source, progress_callback)
            time.sleep(1)
        
        if len(self.all_articles) == 0:
            return None
        
        df = pd.DataFrame(self.all_articles)
        df = df.drop_duplicates(subset=['Ti√™u ƒë·ªÅ'], keep='first')
        df.insert(0, 'STT', range(1, len(df) + 1))
        
        return df

# ============================================================
# STREAMLIT APP
# ============================================================

def main():
    # Header
    st.markdown('<div class="main-header">üìà TOOL C√ÄO TIN V2.2</div>', unsafe_allow_html=True)
    st.markdown('<div class="sub-header">HNX & UPCoM - Upload Danh S√°ch M√£ CK</div>', unsafe_allow_html=True)
    
    # Sidebar
    with st.sidebar:
        st.header("‚öôÔ∏è C√ÄI ƒê·∫∂T")
        
        # UPLOAD STOCK LIST
        st.subheader("üìÇ DANH S√ÅCH M√É CK")
        
        st.markdown('<div class="upload-box">', unsafe_allow_html=True)
        st.write("**Upload file Excel/CSV**")
        st.caption("G·ªìm 3 c·ªôt: M√£ CK | S√†n | T√™n c√¥ng ty")
        
        uploaded_file = st.file_uploader(
            "Ch·ªçn file",
            type=['xlsx', 'xls', 'csv'],
            help="File ph·∫£i c√≥ c√°c c·ªôt: M√£ CK, S√†n (HNX/UPCoM), T√™n c√¥ng ty"
        )
        
        # Download sample
        sample_excel = create_sample_excel()
        st.download_button(
            label="üì• T·∫£i file m·∫´u",
            data=sample_excel,
            file_name="mau_danh_sach_ma_ck.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )
        st.markdown('</div>', unsafe_allow_html=True)
        
        # Parse uploaded file
        if uploaded_file is not None:
            stock_df, error = parse_stock_file(uploaded_file)
            
            if error:
                st.error(f"‚ùå {error}")
                st.session_state['stock_df'] = load_default_stock_list()
            else:
                st.success(f"‚úÖ ƒê√£ load {len(stock_df)} m√£ CK")
                st.session_state['stock_df'] = stock_df
                
                # Hi·ªÉn th·ªã th·ªëng k√™
                hnx_count = len(stock_df[stock_df['S√†n'] == 'HNX'])
                upcom_count = len(stock_df[stock_df['S√†n'] == 'UPCoM'])
                st.info(f"HNX: {hnx_count} | UPCoM: {upcom_count}")
        else:
            if 'stock_df' not in st.session_state:
                st.session_state['stock_df'] = load_default_stock_list()
                st.warning("‚ö†Ô∏è ƒêang d√πng danh s√°ch m·∫∑c ƒë·ªãnh")
        
        st.markdown("---")
        
        # C√†i ƒë·∫∑t c√†o tin
        st.subheader("üîß T√ôY CH·ªàNH")
        
        time_filter = st.selectbox(
            "‚è∞ Kho·∫£ng th·ªùi gian",
            options=[6, 12, 24, 48, 72, 168],
            format_func=lambda x: f"{x} gi·ªù" if x < 168 else "1 tu·∫ßn",
            index=2
        )
        
        max_articles = st.slider(
            "üìä S·ªë b√†i t·ªëi ƒëa/ngu·ªìn",
            min_value=5,
            max_value=50,
            value=20,
            step=5
        )
        
        st.markdown("---")
        st.info("üí° **H∆∞·ªõng d·∫´n:**\n1. Upload danh s√°ch m√£\n2. Ch·ªçn th·ªùi gian\n3. B·∫•m 'B·∫Øt ƒë·∫ßu'\n4. Download Excel")
    
    # Main content
    if st.button("üöÄ B·∫ÆT ƒê·∫¶U C√ÄO TIN", type="primary"):
        stock_df = st.session_state.get('stock_df')
        
        if stock_df is None or len(stock_df) == 0:
            st.error("‚ùå Ch∆∞a c√≥ danh s√°ch m√£ CK! Vui l√≤ng upload file.")
            return
        
        with st.spinner("ƒêang c√†o tin..."):
            # Progress
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            def update_progress(message, progress):
                status_text.text(message)
                progress_bar.progress(progress)
            
            # Run scraper
            scraper = StockScraperWeb(stock_df, time_filter_hours=time_filter)
            df = scraper.run(max_articles_per_source=max_articles, progress_callback=update_progress)
            
            progress_bar.empty()
            status_text.empty()
            
            if df is not None:
                st.success(f"‚úÖ Ho√†n t·∫•t! T√¨m th·∫•y {len(df)} b√†i vi·∫øt")
                
                # Hi·ªÉn th·ªã th·ªëng k√™ matching method
                st.info(f"üîç T√¨m theo m√£ CK: {scraper.stats['found_by_code']} | T√¨m theo t√™n: {scraper.stats['found_by_name']}")
                
                # Store in session
                st.session_state['df'] = df
                st.session_state['stats'] = scraper.stats
            else:
                st.error("Kh√¥ng t√¨m th·∫•y b√†i vi·∫øt n√†o!")
    
    # Display results
    if 'df' in st.session_state:
        df = st.session_state['df']
        stats = st.session_state['stats']
        
        # Metrics
        col1, col2, col3, col4, col5 = st.columns(5)
        with col1:
            st.metric("üìä T·ªïng b√†i", len(df))
        with col2:
            st.metric("‚ö†Ô∏è Nghi√™m tr·ªçng", stats['severe_risk'])
        with col3:
            st.metric("‚ö†Ô∏è C·∫£nh b√°o", stats['warning_risk'])
        with col4:
            st.metric("üî§ T√¨m theo m√£", stats['found_by_code'])
        with col5:
            st.metric("üìù T√¨m theo t√™n", stats['found_by_name'])
        
        # Download button
        buffer = io.BytesIO()
        with pd.ExcelWriter(buffer, engine='openpyxl') as writer:
            # Sheet 1: T·∫•t c·∫£
            df.to_excel(writer, index=False, sheet_name='T·∫•t c·∫£')
            
            # Sheet 2: Nghi√™m tr·ªçng
            df_severe = df[df['Risk'] == 'Nghi√™m tr·ªçng']
            if len(df_severe) > 0:
                df_severe.to_excel(writer, index=False, sheet_name='Nghi√™m tr·ªçng')
            
            # Sheet 3: C·∫£nh b√°o
            df_warning = df[df['Risk'] == 'C·∫£nh b√°o']
            if len(df_warning) > 0:
                df_warning.to_excel(writer, index=False, sheet_name='C·∫£nh b√°o')
        
        st.download_button(
            label="‚¨áÔ∏è Download Excel",
            data=buffer.getvalue(),
            file_name=f"Tin_CK_{datetime.now().strftime('%d%m%Y_%H%M')}.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )
        
        st.markdown("---")
        
        # Filters
        st.subheader("üîç L·ªåC & T√åM KI·∫æM")
        
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            search_code = st.text_input("M√£ CK", placeholder="VD: SHS")
        with col2:
            filter_san = st.selectbox("S√†n", ["T·∫•t c·∫£", "HNX", "UPCoM"])
        with col3:
            filter_risk = st.selectbox("Risk Level", ["T·∫•t c·∫£", "Nghi√™m tr·ªçng", "C·∫£nh b√°o", "B√¨nh th∆∞·ªùng", "T√≠ch c·ª±c"])
        with col4:
            filter_method = st.selectbox("T√¨m theo", ["T·∫•t c·∫£", "M√£ CK", "T√™n c√¥ng ty"])
        
        # Apply filters
        df_filtered = df.copy()
        
        if search_code:
            df_filtered = df_filtered[
                df_filtered['M√£ CK'].str.contains(search_code.upper(), case=False, na=False) |
                df_filtered['T√™n c√¥ng ty'].str.contains(search_code, case=False, na=False)
            ]
        
        if filter_san != "T·∫•t c·∫£":
            df_filtered = df_filtered[df_filtered['S√†n'] == filter_san]
        
        if filter_risk != "T·∫•t c·∫£":
            df_filtered = df_filtered[df_filtered['Risk'] == filter_risk]
        
        if filter_method != "T·∫•t c·∫£":
            df_filtered = df_filtered[df_filtered['T√¨m theo'] == filter_method]
        
        st.info(f"Hi·ªÉn th·ªã {len(df_filtered)} / {len(df)} b√†i")
        
        # Display articles
        st.subheader("üì∞ DANH S√ÅCH B√ÄI VI·∫æT")
        
        for idx, row in df_filtered.iterrows():
            # Card color
            if row['Risk'] == 'Nghi√™m tr·ªçng':
                card_class = "severe-card"
                icon = "‚ö†Ô∏è"
            elif row['Risk'] == 'C·∫£nh b√°o':
                card_class = "warning-card"
                icon = "‚ö†Ô∏è"
            elif row['Risk'] == 'T√≠ch c·ª±c':
                card_class = "positive-card"
                icon = "‚úÖ"
            else:
                card_class = "metric-card"
                icon = "üìÑ"
            
            with st.container():
                st.markdown(f'<div class="{card_class}">', unsafe_allow_html=True)
                
                col1, col2 = st.columns([4, 1])
                
                with col1:
                    # Ti√™u ƒë·ªÅ v·ªõi m√£ v√† t√™n c√¥ng ty
                    if row['T√™n c√¥ng ty']:
                        st.markdown(f"**{icon} {row['M√£ CK']} - {row['T√™n c√¥ng ty']} ({row['S√†n']})**")
                    else:
                        st.markdown(f"**{icon} {row['M√£ CK']} ({row['S√†n']})**")
                    
                    st.markdown(f"{row['Ti√™u ƒë·ªÅ']}")
                    
                    caption_text = f"üìÖ {row['Ng√†y']} | "
                    caption_text += f"Sentiment: {row['Sentiment']} ({row['ƒêi·ªÉm']}) | "
                    caption_text += f"Risk: {row['Risk']} | "
                    caption_text += f"üîç {row['T√¨m theo']}"
                    
                    if row['Vi ph·∫°m']:
                        caption_text += f" | ‚öñÔ∏è {row['Vi ph·∫°m']}"
                    
                    st.caption(caption_text)
                
                with col2:
                    if st.button("üîó Xem", key=f"view_{idx}"):
                        st.markdown(f"[M·ªü b√†i vi·∫øt]({row['Link']})")
                
                if row['Keywords']:
                    st.info(f"üîë Keywords: {row['Keywords']}")
                
                st.markdown('</div>', unsafe_allow_html=True)
                st.markdown("<br>", unsafe_allow_html=True)
        
        # Dashboard
        st.markdown("---")
        st.subheader("üìä DASHBOARD")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.write("**Ph√¢n b·ªë Sentiment**")
            sentiment_counts = df['Sentiment'].value_counts()
            st.bar_chart(sentiment_counts)
        
        with col2:
            st.write("**Ph√¢n b·ªë Risk Level**")
            risk_counts = df['Risk'].value_counts()
            st.bar_chart(risk_counts)
        
        col3, col4 = st.columns(2)
        
        with col3:
            st.write("**Top 10 M√£ CK**")
            top_ma = df['M√£ CK'].value_counts().head(10)
            st.bar_chart(top_ma)
        
        with col4:
            st.write("**Ph√¢n b·ªë theo S√†n**")
            san_counts = df['S√†n'].value_counts()
            st.bar_chart(san_counts)
        
        # Th·ªëng k√™ chi ti·∫øt
        st.markdown("---")
        st.subheader("üìà CHI TI·∫æT THEO M√É CK")
        
        with st.expander("Xem chi ti·∫øt"):
            # T·∫°o b·∫£ng t·ªïng h·ª£p
            summary = df.groupby('M√£ CK').agg({
                'Ti√™u ƒë·ªÅ': 'count',
                'ƒêi·ªÉm': 'mean',
                'Risk': lambda x: x.mode()[0] if len(x) > 0 else 'N/A'
            }).rename(columns={
                'Ti√™u ƒë·ªÅ': 'S·ªë b√†i',
                'ƒêi·ªÉm': 'Sentiment TB',
                'Risk': 'Risk ch√≠nh'
            }).reset_index()
            
            # Th√™m t√™n c√¥ng ty
            summary = summary.merge(
                df[['M√£ CK', 'T√™n c√¥ng ty', 'S√†n']].drop_duplicates(),
                on='M√£ CK',
                how='left'
            )
            
            summary['Sentiment TB'] = summary['Sentiment TB'].round(1)
            summary = summary.sort_values('S·ªë b√†i', ascending=False)
            
            st.dataframe(
                summary,
                use_container_width=True,
                hide_index=True
            )

if __name__ == "__main__":
    main()
