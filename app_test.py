# ============================================================
# üéØ STREAMLIT WEB APP V2.4 - UPLOAD + SUMMARIZE
# ============================================================
# ‚úÖ Upload danh s√°ch m√£ CK
# ‚úÖ T√≥m t·∫Øt extractive (t·ª´ V1.0)
# ‚úÖ Sentiment analysis
# ‚úÖ Risk detection
# ============================================================

import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime, timedelta, timezone
import time
import re
from urllib.parse import urljoin
import io

# ============================================================
# CONFIG
# ============================================================

st.set_page_config(
    page_title="C√†o Tin Ch·ª©ng Kho√°n V2.4",
    page_icon="üìà",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ============================================================
# CSS
# ============================================================

st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 1rem;
    }
    .upload-box {
        background-color: #e8f4f8;
        padding: 1.5rem;
        border-radius: 0.5rem;
        border: 2px dashed #1f77b4;
        margin: 1rem 0;
    }
    .severe-card {
        background-color: #ffe6e6;
        border-left: 5px solid #ff4444;
        padding: 1rem;
        margin: 0.5rem 0;
        border-radius: 0.3rem;
    }
    .warning-card {
        background-color: #fff8e6;
        border-left: 5px solid #ffaa00;
        padding: 1rem;
        margin: 0.5rem 0;
        border-radius: 0.3rem;
    }
    .positive-card {
        background-color: #e6ffe6;
        border-left: 5px solid #44ff44;
        padding: 1rem;
        margin: 0.5rem 0;
        border-radius: 0.3rem;
    }
</style>
""", unsafe_allow_html=True)

# ============================================================
# HELPER FUNCTIONS
# ============================================================

def load_default_stock_list():
    """Danh s√°ch m√£ m·∫∑c ƒë·ªãnh"""
    default_data = {
        'M√£ CK': ['SHS', 'PVS', 'NVB', 'VCS', 'BVS', 'CEO', 'VGC', 'PVC',
                  'LPB', 'EIB', 'BAB', 'OCB', 'HDG', 'PAN'],
        'S√†n': ['HNX']*8 + ['UPCoM']*6,
        'T√™n c√¥ng ty': ['Ch·ª©ng kho√°n SHS', 'Ch·ª©ng kho√°n PVS', 'Ng√¢n h√†ng NVB',
                        'Ch·ª©ng kho√°n VCS', 'Ch·ª©ng kho√°n BVS', 'T·∫≠p ƒëo√†n CEO',
                        'Viglacera', 'PVC', 'Ng√¢n h√†ng LPB', 'Ng√¢n h√†ng EIB',
                        'Ng√¢n h√†ng BAB', 'Ng√¢n h√†ng OCB', 'T·∫≠p ƒëo√†n HDG', 'PAN Group']
    }
    return pd.DataFrame(default_data)

def parse_stock_file(uploaded_file):
    """Parse Excel/CSV file"""
    try:
        if uploaded_file.name.endswith('.csv'):
            df = pd.read_csv(uploaded_file)
        else:
            df = pd.read_excel(uploaded_file)
        
        df.columns = df.columns.str.strip().str.lower()
        
        column_mapping = {
            'm√£ ck': 'M√£ CK', 'ma ck': 'M√£ CK', 'm√£': 'M√£ CK', 'code': 'M√£ CK',
            's√†n': 'S√†n', 'san': 'S√†n', 'exchange': 'S√†n',
            't√™n c√¥ng ty': 'T√™n c√¥ng ty', 'ten cong ty': 'T√™n c√¥ng ty', 'name': 'T√™n c√¥ng ty',
        }
        
        for old_col, new_col in column_mapping.items():
            if old_col in df.columns:
                df.rename(columns={old_col: new_col}, inplace=True)
        
        required_cols = ['M√£ CK', 'S√†n']
        missing_cols = [col for col in required_cols if col not in df.columns]
        
        if missing_cols:
            return None, f"Thi·∫øu c√°c c·ªôt: {', '.join(missing_cols)}"
        
        if 'T√™n c√¥ng ty' not in df.columns:
            df['T√™n c√¥ng ty'] = ''
        
        df['M√£ CK'] = df['M√£ CK'].astype(str).str.strip().str.upper()
        df['S√†n'] = df['S√†n'].astype(str).str.strip().str.upper()
        df['T√™n c√¥ng ty'] = df['T√™n c√¥ng ty'].astype(str).str.strip()
        
        df = df[df['S√†n'].isin(['HNX', 'UPCOM'])]
        df['S√†n'] = df['S√†n'].replace('UPCOM', 'UPCoM')
        df = df.drop_duplicates(subset=['M√£ CK'])
        
        return df, None
        
    except Exception as e:
        return None, f"L·ªói ƒë·ªçc file: {str(e)}"

def create_sample_excel():
    """T·∫°o file Excel m·∫´u"""
    sample_data = {
        'M√£ CK': ['SHS', 'PVS', 'NVB', 'LPB', 'EIB', 'CEO'],
        'S√†n': ['HNX', 'HNX', 'HNX', 'UPCoM', 'UPCoM', 'HNX'],
        'T√™n c√¥ng ty': ['Ch·ª©ng kho√°n S√†i G√≤n - H√† N·ªôi', 'Ch·ª©ng kho√°n D·∫ßu kh√≠', 
                        'Ng√¢n h√†ng Qu·ªëc d√¢n', 'Ng√¢n h√†ng L·ªôc Ph√°t', 
                        'Ng√¢n h√†ng Xu·∫•t nh·∫≠p kh·∫©u', 'T·∫≠p ƒëo√†n CEO']
    }
    df = pd.DataFrame(sample_data)
    
    buffer = io.BytesIO()
    with pd.ExcelWriter(buffer, engine='openpyxl') as writer:
        df.to_excel(writer, index=False, sheet_name='Danh s√°ch m√£')
    
    return buffer.getvalue()

# ============================================================
# KEYWORD RISK DETECTOR
# ============================================================

class KeywordRiskDetector:
    def __init__(self):
        self.keywords_db = {
            # A. N·ªôi b·ªô & Qu·∫£n tr·ªã
            "l√£nh ƒë·∫°o b·ªã b·∫Øt": {"category": "A. N·ªôi b·ªô", "severity": "severe", "score": -95, "violation": "I.2, II.A"},
            "l√£nh ƒë·∫°o b·ªè tr·ªën": {"category": "A. N·ªôi b·ªô", "severity": "severe", "score": -95, "violation": "I.2, II.A"},
            "c·ªï ƒë√¥ng l·ªõn b√°n chui": {"category": "A. N·ªôi b·ªô", "severity": "severe", "score": -85, "violation": "I.1, II.A"},
            "ch·ªß t·ªãch b·∫•t ng·ªù tho√°i h·∫øt v·ªën": {"category": "A. N·ªôi b·ªô", "severity": "severe", "score": -85, "violation": "I.1, II.A"},
            
            # B. T√†i ch√≠nh
            "b·∫•t ng·ªù b√°o l·ªó": {"category": "B. T√†i ch√≠nh", "severity": "severe", "score": -80, "violation": "I.4, II.B"},
            "√¢m v·ªën ch·ªß": {"category": "B. T√†i ch√≠nh", "severity": "severe", "score": -90, "violation": "II.B"},
            "m·∫•t kh·∫£ nƒÉng thanh to√°n": {"category": "B. T√†i ch√≠nh", "severity": "severe", "score": -90, "violation": "II.B"},
            "n·ª£ x·∫•u b·∫•t th∆∞·ªùng": {"category": "B. T√†i ch√≠nh", "severity": "severe", "score": -80, "violation": "II.B"},
            
            # C. Thao t√∫ng & Bi·∫øn ƒë·ªông gi√° b·∫•t th∆∞·ªùng
            "ƒë·ªôi l√°i l√†m gi√°": {"category": "C. Thao t√∫ng", "severity": "severe", "score": -95, "violation": "I.3, II.C"},
            "tƒÉng tr·∫ßn li√™n ti·∫øp": {"category": "C. Thao t√∫ng", "severity": "warning", "score": -60, "violation": "I.2, II.C"},
            "gi·∫£m s√†n li√™n t·ª•c": {"category": "C. Thao t√∫ng", "severity": "warning", "score": -70, "violation": "I.2, II.C"},
            "b·ªëc ƒë·∫ßu": {"category": "C. Thao t√∫ng", "severity": "warning", "score": -65, "violation": "I.2, I.3, II.C"},
            "k·ªãch tr·∫ßn": {"category": "C. Thao t√∫ng", "severity": "warning", "score": -65, "violation": "I.2, I.3, II.C"},
            "r·ªõt ƒë√°y": {"category": "C. Thao t√∫ng", "severity": "warning", "score": -70, "violation": "I.2, I.3, II.C"},
            "c·ªï phi·∫øu tƒÉng phi m√£": {"category": "C. Thao t√∫ng", "severity": "warning", "score": -65, "violation": "I.2, I.4, II.C"},
            "tƒÉng d·ª±ng ƒë·ª©ng": {"category": "C. Thao t√∫ng", "severity": "warning", "score": -60, "violation": "I.2, II.C"},
            "kh·ªëi l∆∞·ª£ng tƒÉng b·∫•t th∆∞·ªùng": {"category": "C. Thao t√∫ng", "severity": "warning", "score": -65, "violation": "I.6, II.C"},
            "giao d·ªãch n·ªôi gi√°n": {"category": "C. Thao t√∫ng", "severity": "severe", "score": -90, "violation": "I.1, II.C"},
            
            # D. M&A
            "ni√™m y·∫øt c·ª≠a sau": {"category": "D. M&A", "severity": "severe", "score": -85, "violation": "I.5, II.D"},
            "th√¢u t√≥m": {"category": "D. M&A", "severity": "warning", "score": -50, "violation": "I.5, II.D"},
            
            # E. Ph√°p l√Ω
            "c√¥ng an ƒëi·ªÅu tra": {"category": "E. Ph√°p l√Ω", "severity": "severe", "score": -90, "violation": "II.E"},
            "kh·ªüi t·ªë l√£nh ƒë·∫°o": {"category": "E. Ph√°p l√Ω", "severity": "severe", "score": -95, "violation": "II.E"},
            "gian l·∫≠n t√†i ch√≠nh": {"category": "E. Ph√°p l√Ω", "severity": "severe", "score": -95, "violation": "II.E"},
            
            # F. S·ª± ki·ªán b√™n ngo√†i
            "ch√°y nh√† x∆∞·ªüng": {"category": "F. S·ª± ki·ªán ngo√†i", "severity": "severe", "score": -75, "violation": "II.F"},
            "b·ªã thu h·ªìi gi·∫•y ph√©p": {"category": "F. S·ª± ki·ªán ngo√†i", "severity": "severe", "score": -90, "violation": "II.F"},
            
            # T√≠ch c·ª±c
            "l·ª£i nhu·∫≠n tƒÉng": {"category": "T√≠ch c·ª±c", "severity": "positive", "score": 70, "violation": ""},
            "tƒÉng tr∆∞·ªüng m·∫°nh": {"category": "T√≠ch c·ª±c", "severity": "positive", "score": 65, "violation": ""},
            "doanh thu k·ª∑ l·ª•c": {"category": "T√≠ch c·ª±c", "severity": "positive", "score": 75, "violation": ""},
        }
    
    def analyze(self, text):
        text_lower = text.lower()
        found_keywords = []
        total_score = 0
        categories = set()
        violations = set()
        max_severity = "normal"
        
        for keyword, info in self.keywords_db.items():
            if keyword in text_lower:
                found_keywords.append({
                    "keyword": keyword,
                    "category": info["category"],
                    "severity": info["severity"],
                    "score": info["score"],
                    "violation": info["violation"]
                })
                total_score += info["score"]
                categories.add(info["category"])
                if info["violation"]:
                    violations.add(info["violation"])
                
                if info["severity"] == "severe":
                    max_severity = "severe"
                elif info["severity"] == "warning" and max_severity != "severe":
                    max_severity = "warning"
                elif info["severity"] == "positive" and max_severity == "normal":
                    max_severity = "positive"
        
        return {
            "keywords": found_keywords,
            "total_score": total_score,
            "severity": max_severity,
            "categories": list(categories),
            "violations": ", ".join(sorted(violations))
        }

# ============================================================
# SENTIMENT ANALYZER
# ============================================================

class SimpleSentimentAnalyzer:
    def __init__(self):
        self.keyword_detector = KeywordRiskDetector()
        self.positive_words = ['tƒÉng', 'tƒÉng tr∆∞·ªüng', 'l·ª£i nhu·∫≠n', 'th√†nh c√¥ng', 't·ªët', 'cao', 'm·∫°nh', 'v∆∞·ª£t']
        self.negative_words = ['gi·∫£m', 's·ª•t gi·∫£m', 'l·ªó', 'thua l·ªó', 'kh√≥ khƒÉn', 'ti√™u c·ª±c', 'suy gi·∫£m']
    
    def analyze_sentiment(self, title, content):
        text = (title + " " + content).lower()
        keyword_analysis = self.keyword_detector.analyze(title + " " + content)
        
        pos_count = sum(1 for word in self.positive_words if word in text)
        neg_count = sum(1 for word in self.negative_words if word in text)
        
        base_score = 50 + (pos_count * 5) - (neg_count * 5)
        
        if keyword_analysis["severity"] == "severe":
            final_score = min(20, base_score + keyword_analysis["total_score"])
        elif keyword_analysis["severity"] == "warning":
            final_score = min(40, base_score + keyword_analysis["total_score"] * 0.7)
        elif keyword_analysis["severity"] == "positive":
            final_score = max(60, base_score + keyword_analysis["total_score"])
        else:
            final_score = base_score
        
        final_score = max(0, min(100, final_score))
        
        if final_score >= 60:
            label = "T√≠ch c·ª±c"
        elif final_score >= 40:
            label = "Trung l·∫≠p"
        else:
            label = "Ti√™u c·ª±c"
        
        if keyword_analysis["severity"] == "severe":
            risk_level = "Nghi√™m tr·ªçng"
        elif keyword_analysis["severity"] == "warning":
            risk_level = "C·∫£nh b√°o"
        elif keyword_analysis["severity"] == "positive":
            risk_level = "T√≠ch c·ª±c"
        else:
            risk_level = "B√¨nh th∆∞·ªùng"
        
        return {
            "sentiment_score": round(final_score, 1),
            "sentiment_label": label,
            "risk_level": risk_level,
            "keywords": keyword_analysis["keywords"],
            "categories": ", ".join(keyword_analysis["categories"]) if keyword_analysis["categories"] else "",
            "violations": keyword_analysis["violations"]
        }

# ============================================================
# STOCK SCRAPER
# ============================================================

class StockScraperWeb:
    def __init__(self, stock_df, time_filter_hours=24):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept-Language': 'vi-VN,vi;q=0.9,en;q=0.8',
        }
        self.all_articles = []
        self.session = requests.Session()
        self.time_filter_hours = time_filter_hours
        
        self.vietnam_tz = timezone(timedelta(hours=7))
        self.cutoff_time = datetime.now(self.vietnam_tz) - timedelta(hours=time_filter_hours)
        
        self.sentiment_analyzer = SimpleSentimentAnalyzer()
        
        # Load stock list
        self.stock_df = stock_df
        self.hnx_stocks = set(stock_df[stock_df['S√†n'] == 'HNX']['M√£ CK'].tolist())
        self.upcom_stocks = set(stock_df[stock_df['S√†n'] == 'UPCoM']['M√£ CK'].tolist())
        
        self.code_to_name = dict(zip(stock_df['M√£ CK'], stock_df['T√™n c√¥ng ty']))
        
        self.name_to_code = {}
        for code, name in self.code_to_name.items():
            if name:
                words = name.lower().split()
                for word in words:
                    if len(word) > 3:
                        if word not in self.name_to_code:
                            self.name_to_code[word] = []
                        self.name_to_code[word].append(code)
        
        self.stock_to_exchange = {}
        for code in self.hnx_stocks:
            self.stock_to_exchange[code] = 'HNX'
        for code in self.upcom_stocks:
            self.stock_to_exchange[code] = 'UPCoM'
        
        self.stats = {
            'total_crawled': 0,
            'hnx_found': 0,
            'upcom_found': 0,
            'severe_risk': 0,
            'warning_risk': 0,
            'found_by_code': 0,
            'found_by_name': 0
        }
    
    def clean_text(self, text):
        """L√†m s·∫°ch text - t·ª´ V1.0"""
        if not text:
            return ""
        text = re.sub(r'[^\w\s.,;:!?()%\-\+\/\"\'√†√°·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√®√©·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±·ª≥√Ω·ª∑·ªπ·ªµƒë√Ä√Å·∫¢√É·∫†ƒÇ·∫Æ·∫∞·∫≤·∫¥·∫∂√Ç·∫§·∫¶·∫®·∫™·∫¨√à√â·∫∫·∫º·∫∏√ä·∫æ·ªÄ·ªÇ·ªÑ·ªÜ√å√ç·ªàƒ®·ªä√í√ì·ªé√ï·ªå√î·ªê·ªí·ªî·ªñ·ªò∆†·ªö·ªú·ªû·ª†·ª¢√ô√ö·ª¶≈®·ª§∆Ø·ª®·ª™·ª¨·ªÆ·ª∞·ª≤√ù·ª∂·ª∏·ª¥ƒê]', ' ', text)
        text = re.sub(r'\s+', ' ', text)
        return text.strip()
    
    def advanced_summarize(self, content, title, max_sentences=4):
        """T√≥m t·∫Øt EXTRACTIVE - t·ª´ V1.0"""
        content = self.clean_text(content)
        title = self.clean_text(title)
        
        if not content or len(content) < 100:
            return content
        
        full_text = title + ". " + content
        sentences = re.split(r'[.!?]+', full_text)
        sentences = [s.strip() for s in sentences if len(s.strip()) > 30]
        
        if len(sentences) <= max_sentences:
            return '. '.join(sentences) + '.'
        
        important_keywords = {
            'tƒÉng': 3, 'gi·∫£m': 3, 'tƒÉng tr∆∞·ªüng': 3,
            'l·ª£i nhu·∫≠n': 4, 'doanh thu': 4, 'l·ªó': 3,
            't·ª∑ ƒë·ªìng': 3, 'ngh√¨n t·ª∑': 4,
            'c·ªï phi·∫øu': 3, 'ni√™m y·∫øt': 3,
            'giao d·ªãch': 2, 'thanh kho·∫£n': 3,
            'qu√Ω': 3, 'nƒÉm': 2,
            'ph√°t h√†nh': 3, 'tr√°i phi·∫øu': 3,
            'ƒë·∫ßu t∆∞': 2, 'v·ªën': 3,
        }
        
        scored_sentences = []
        for i, sentence in enumerate(sentences):
            score = 0
            sentence_lower = sentence.lower()
            
            if i == 0:
                score += 5
            elif i == 1:
                score += 3
            elif i < 5:
                score += 1
            
            for keyword, weight in important_keywords.items():
                if keyword in sentence_lower:
                    score += weight
            
            numbers = re.findall(r'\d+(?:[.,]\d+)*', sentence)
            if numbers:
                score += len(numbers)
                if any(num for num in numbers if len(num.replace('.', '').replace(',', '')) >= 4):
                    score += 2
            
            if '%' in sentence:
                score += 3
            
            word_count = len(sentence.split())
            if 12 <= word_count <= 35:
                score += 2
            elif word_count < 8 or word_count > 50:
                score -= 1
            
            for code in list(self.hnx_stocks) + list(self.upcom_stocks):
                if code in sentence.upper():
                    score += 3
                    break
            
            scored_sentences.append((sentence, score, i))
        
        scored_sentences.sort(key=lambda x: x[1], reverse=True)
        top_sentences = scored_sentences[:max_sentences]
        top_sentences.sort(key=lambda x: x[2])
        
        summary = '. '.join([s[0] for s in top_sentences])
        if not summary.endswith('.'):
            summary += '.'
        
        summary = self.clean_text(summary)
        return summary
    
    def extract_stock(self, text):
        """Tr√≠ch xu·∫•t m√£ CK - X·ª≠ l√Ω ƒë·∫∑c bi·ªát TOP, TIN, CEO"""
        text_upper = text.upper()
        text_lower = text.lower()
        
        # BLACKLIST - M·ªü r·ªông
        blacklist_patterns = [
            # Tin t·ªïng quan th·ªã tr∆∞·ªùng
            r'CH·ª®NG KHO√ÅN\s+\w+\s+C√ì\s+NH·∫¨N ƒê·ªäNH',
            r'CH·ª®NG KHO√ÅN\s+\w+\s+D·ª∞ B√ÅO',
            r'CH·ª®NG KHO√ÅN\s+\w+\s+PH√ÇN T√çCH',
            r'C√îNG TY\s+CH·ª®NG KHO√ÅN',
            r'CTCK\s+\w+',
            
            # Index
            r'VN-INDEX',
            r'HNX-INDEX',
            r'UPCOM-INDEX',
            
            # Top c·ªï phi·∫øu (tr√°nh nh·∫ßm v·ªõi m√£ TOP)
            r'TOP\s+C·ªî\s+PHI·∫æU',
            r'TOP\s+\d+',
            r'TOP\s+M√É',
            
            # Tin t·ª©c (tr√°nh nh·∫ßm v·ªõi m√£ TIN)
            r'TIN\s+VUI',
            r'TIN\s+T·ªêT',
            r'TIN\s+X·∫§U',
            r'TIN\s+V·∫ÆN',
            r'TIN\s+CH·ª®NG\s+KHO√ÅN',
            r'TIN\s+TH·ªä\s+TR∆Ø·ªúNG',
            r'NH·∫¨N\s+TIN',
            r'THEO\s+TIN',
            r'M·ªòT\s+TIN',
            r'C√ÅC\s+TIN',
            r'NHI·ªÄU\s+TIN',
            
            # CEO (tr√°nh nh·∫ßm v·ªõi m√£ CEO)
            r'CEO\s+C√îNG\s+TY',
            r'CEO\s+C·ª¶A',
            r'CEO\s+M·ªöI',
            r'GI√ÅM\s+ƒê·ªêC\s+CEO',
            r'T·ªîNG\s+GI√ÅM\s+ƒê·ªêC\s+CEO',
            
            # T·ªïng quan
            r'TH·ªä TR∆Ø·ªúNG CHUNG',
            r'DI·ªÑN BI·∫æN TH·ªä TR∆Ø·ªúNG',
            r'T·ªîNG QUAN TH·ªä TR∆Ø·ªúNG',
            r'ƒêI·ªÇM TIN',
            r'B·∫¢N TIN',
        ]
        
        for pattern in blacklist_patterns:
            if re.search(pattern, text_upper):
                return None, None, None
        
        # T√¨m theo m√£
        for code in self.hnx_stocks:
            match = re.search(r'\b' + code + r'\b', text_upper)
            if match:
                context = text_upper[max(0, match.start()-15):match.end()+15]
                
                # Check context xung quanh
                if re.search(r'CH·ª®NG KHO√ÅN\s+' + code, context):
                    continue
                if re.search(r'CTCK\s+' + code, context):
                    continue
                
                # ƒê·∫∂C BI·ªÜT 1: M√£ "TOP"
                if code == 'TOP':
                    if match.start() > 0:
                        prev_char = text_upper[match.start()-1]
                        if prev_char.isalnum():
                            continue
                    
                    if match.end() < len(text_upper) - 1:
                        next_chars = text_upper[match.end():match.end()+15]
                        if re.match(r'\s+\d+', next_chars):
                            continue
                        if re.match(r'\s+(C·ªî|M√É)', next_chars):
                            continue
                
                # ƒê·∫∂C BI·ªÜT 2: M√£ "TIN"
                if code == 'TIN':
                    # Check t·ª´ TR∆Ø·ªöC "TIN"
                    if match.start() >= 5:
                        prev_words = text_upper[match.start()-15:match.start()]
                        if re.search(r'(NH·∫¨N|THEO|M·ªòT|C√ÅC|NHI·ªÄU)\s*$'):
    
   def fetch_url(self, url, max_retries=2):
    for attempt in range(max_retries):
        try:
            response = self.session.get(url, headers=self.headers, timeout=15)
            response.raise_for_status()
            return response
        except Exception:
            if attempt < max_retries - 1:
                time.sleep(1)
                continue
            else:
                return None
    
    def fetch_article_content(self, url):
        """L·∫•y n·ªôi dung b√†i vi·∫øt - t·ª´ V1.0"""
        try:
            response = self.fetch_url(url)
            if not response:
                return None, None, None
            
            response.encoding = 'utf-8'
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # T√¨m ng√†y
            date_text = None
            for pattern in [
                {'class': re.compile(r'date|time|publish', re.I)},
                {'itemprop': 'datePublished'}
            ]:
                date_elem = soup.find(['time', 'span', 'div'], pattern)
                if date_elem:
                    date_text = date_elem.get('datetime') or date_elem.get_text(strip=True)
                    break
            
            # Parse ng√†y (GMT+7)
            article_date_str = datetime.now(self.vietnam_tz).strftime('%d/%m/%Y')
            article_date_obj = datetime.now(self.vietnam_tz)
            
            # T√¨m n·ªôi dung
            content = ""
            for selector in [
                ('article', {}),
                ('div', {'class': re.compile(r'content|article|detail', re.I)}),
            ]:
                content_div = soup.find(selector[0], selector[1])
                if content_div:
                    paragraphs = content_div.find_all('p')
                    content = ' '.join([p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 50])
                    if content:
                        break
            
            if not content:
                paragraphs = soup.find_all('p')
                valid_p = [p.get_text(strip=True) for p in paragraphs if 50 < len(p.get_text(strip=True)) < 1000]
                content = ' '.join(valid_p[:8])
            
            content = self.clean_text(content)
            return content, article_date_str, article_date_obj
        
        except:
            return None, None, None
    
    def scrape_source(self, url, source_name, pattern, max_articles=20, progress_callback=None):
        try:
            response = self.fetch_url(url)
            if not response:
                return 0
            
            response.encoding = 'utf-8'
            soup = BeautifulSoup(response.text, 'html.parser')
            
            count = 0
            seen = set()
            links = soup.find_all('a', href=True)
            total_links = len(links)
            
            for idx, link_tag in enumerate(links):
                if progress_callback:
                    progress = (idx + 1) / total_links
                    progress_callback(f"{source_name}: {idx+1}/{total_links}", progress)
                
                href = link_tag.get('href', '')
                
                if pattern(href) and href not in seen:
                    title = link_tag.get_text(strip=True)
                    
                    if title and len(title) > 30:
                        self.stats['total_crawled'] += 1
                        seen.add(href)
                        
                        stock_code, exchange, match_method = self.extract_stock(title)
                        
                        if stock_code and exchange in ['HNX', 'UPCoM']:
                            full_link = urljoin(url, href)
                            
                            if match_method == 'code':
                                self.stats['found_by_code'] += 1
                            else:
                                self.stats['found_by_name'] += 1
                            
                            company_name = self.code_to_name.get(stock_code, '')
                            
                            # FETCH N·ªòI DUNG ƒê·∫¶Y ƒê·ª¶
                            content, article_date_str, article_date_obj = self.fetch_article_content(full_link)
                            
                            if content:
                                # T√ìM T·∫ÆT
                                summary = self.advanced_summarize(content, title, max_sentences=4)
                            else:
                                content = ""
                                summary = title  # Fallback n·∫øu kh√¥ng l·∫•y ƒë∆∞·ª£c content
                            
                            # SENTIMENT
                            sentiment_result = self.sentiment_analyzer.analyze_sentiment(title, content)
                            
                            if exchange == 'HNX':
                                self.stats['hnx_found'] += 1
                            else:
                                self.stats['upcom_found'] += 1
                            
                            if sentiment_result['risk_level'] == 'Nghi√™m tr·ªçng':
                                self.stats['severe_risk'] += 1
                            elif sentiment_result['risk_level'] == 'C·∫£nh b√°o':
                                self.stats['warning_risk'] += 1
                            
                            self.all_articles.append({
                                'Ti√™u ƒë·ªÅ': title,
                                'Link': full_link,
                                'Ng√†y': article_date_str,
                                'M√£ CK': stock_code,
                                'T√™n c√¥ng ty': company_name,
                                'S√†n': exchange,
                                'Sentiment': sentiment_result['sentiment_label'],
                                'ƒêi·ªÉm': sentiment_result['sentiment_score'],
                                'Risk': sentiment_result['risk_level'],
                                'Vi ph·∫°m': sentiment_result['violations'],
                                'Keywords': "; ".join([k['keyword'] for k in sentiment_result['keywords'][:3]]),
                                'N·ªôi dung t√≥m t·∫Øt': summary,  # ‚Üê C·ªòT M·ªöI
                                'T√¨m theo': 'M√£ CK' if match_method == 'code' else 'T√™n c√¥ng ty'
                            })
                            
                            count += 1
                            time.sleep(0.5)
                            
                            if count >= max_articles:
                                break
            
            return count
        
        except Exception as e:
            st.error(f"L·ªói {source_name}: {str(e)}")
            return 0
    
    def run(self, max_articles_per_source=20, progress_callback=None):
        sources = [
            ("https://cafef.vn/thi-truong-chung-khoan.chn", "CafeF", lambda h: '.chn' in h),
            ("https://vietstock.vn/chung-khoan.htm", "VietStock", lambda h: re.search(r'/\d{4}/\d{2}/.+\.htm', h)),
        ]
        
        for url, name, pattern in sources:
            self.scrape_source(url, name, pattern, max_articles_per_source, progress_callback)
            time.sleep(1)
        
        if len(self.all_articles) == 0:
            return None
        
        df = pd.DataFrame(self.all_articles)
        df = df.drop_duplicates(subset=['Ti√™u ƒë·ªÅ'], keep='first')
        df.insert(0, 'STT', range(1, len(df) + 1))
        
        return df

# ============================================================
# STREAMLIT APP
# ============================================================

def main():
    st.markdown('<div class="main-header">üìà TOOL C√ÄO TIN V2.4</div>', unsafe_allow_html=True)
    st.markdown('<div style="text-align:center;color:#666;margin-bottom:2rem;">HNX & UPCoM - Upload + Summarize + Sentiment</div>', unsafe_allow_html=True)
    
    # Sidebar
    with st.sidebar:
        st.header("‚öôÔ∏è C√ÄI ƒê·∫∂T")
        
        st.subheader("üìÇ DANH S√ÅCH M√É CK")
        st.markdown('<div class="upload-box">', unsafe_allow_html=True)
        st.write("**Upload file Excel/CSV**")
        st.caption("G·ªìm 3 c·ªôt: M√£ CK | S√†n | T√™n c√¥ng ty")
        
        uploaded_file = st.file_uploader(
            "Ch·ªçn file",
            type=['xlsx', 'xls', 'csv'],
            help="File ph·∫£i c√≥ c√°c c·ªôt: M√£ CK, S√†n (HNX/UPCoM), T√™n c√¥ng ty"
        )
        
        sample_excel = create_sample_excel()
        st.download_button(
            label="üì• T·∫£i file m·∫´u",
            data=sample_excel,
            file_name="mau_danh_sach_ma_ck.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )
        st.markdown('</div>', unsafe_allow_html=True)
        
        if uploaded_file is not None:
            stock_df, error = parse_stock_file(uploaded_file)
            
            if error:
                st.error(f"‚ùå {error}")
                st.session_state['stock_df'] = load_default_stock_list()
            else:
                st.success(f"‚úÖ ƒê√£ load {len(stock_df)} m√£ CK")
                st.session_state['stock_df'] = stock_df
                
                hnx_count = len(stock_df[stock_df['S√†n'] == 'HNX'])
                upcom_count = len(stock_df[stock_df['S√†n'] == 'UPCoM'])
                st.info(f"HNX: {hnx_count} | UPCoM: {upcom_count}")
        else:
            if 'stock_df' not in st.session_state:
                st.session_state['stock_df'] = load_default_stock_list()
                st.warning("‚ö†Ô∏è ƒêang d√πng danh s√°ch m·∫∑c ƒë·ªãnh")
        
        st.markdown("---")
        st.subheader("üîß T√ôY CH·ªàNH")
        
        time_filter = st.selectbox(
            "‚è∞ Kho·∫£ng th·ªùi gian",
            options=[6, 12, 24, 48, 72, 168],
            format_func=lambda x: f"{x} gi·ªù" if x < 168 else "1 tu·∫ßn",
            index=2
        )
        
        max_articles = st.slider(
            "üìä S·ªë b√†i t·ªëi ƒëa/ngu·ªìn",
            min_value=5,
            max_value=50,
            value=20,
            step=5
        )
        
        st.markdown("---")
        st.info("üí° **H∆∞·ªõng d·∫´n:**\n1. Upload danh s√°ch m√£\n2. Ch·ªçn th·ªùi gian\n3. B·∫•m 'B·∫Øt ƒë·∫ßu'\n4. Download Excel")
    
    # Main content
    if st.button("üöÄ B·∫ÆT ƒê·∫¶U C√ÄO TIN", type="primary"):
        stock_df = st.session_state.get('stock_df')
        
        if stock_df is None or len(stock_df) == 0:
            st.error("‚ùå Ch∆∞a c√≥ danh s√°ch m√£ CK! Vui l√≤ng upload file.")
            return
        
        with st.spinner("ƒêang c√†o tin..."):
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            def update_progress(message, progress):
                status_text.text(message)
                progress_bar.progress(progress)
            
            scraper = StockScraperWeb(stock_df, time_filter_hours=time_filter)
            df = scraper.run(max_articles_per_source=max_articles, progress_callback=update_progress)
            
            progress_bar.empty()
            status_text.empty()
            
            if df is not None:
                st.success(f"‚úÖ Ho√†n t·∫•t! T√¨m th·∫•y {len(df)} b√†i vi·∫øt")
                st.info(f"üîç T√¨m theo m√£ CK: {scraper.stats['found_by_code']} | T√¨m theo t√™n: {scraper.stats['found_by_name']}")
                
                st.session_state['df'] = df
                st.session_state['stats'] = scraper.stats
            else:
                st.error("Kh√¥ng t√¨m th·∫•y b√†i vi·∫øt n√†o!")
    
    # Display results
    if 'df' in st.session_state:
        df = st.session_state['df']
        stats = st.session_state['stats']
        
        # Metrics
        col1, col2, col3, col4, col5 = st.columns(5)
        with col1:
            st.metric("üìä T·ªïng b√†i", len(df))
        with col2:
            st.metric("‚ö†Ô∏è Nghi√™m tr·ªçng", stats['severe_risk'])
        with col3:
            st.metric("‚ö†Ô∏è C·∫£nh b√°o", stats['warning_risk'])
        with col4:
            st.metric("üî§ T√¨m theo m√£", stats['found_by_code'])
        with col5:
            st.metric("üìù T√¨m theo t√™n", stats['found_by_name'])
        
        # Download button
        buffer = io.BytesIO()
        with pd.ExcelWriter(buffer, engine='openpyxl') as writer:
            df.to_excel(writer, index=False, sheet_name='T·∫•t c·∫£')
            
            df_severe = df[df['Risk'] == 'Nghi√™m tr·ªçng']
            if len(df_severe) > 0:
                df_severe.to_excel(writer, index=False, sheet_name='Nghi√™m tr·ªçng')
            
            df_warning = df[df['Risk'] == 'C·∫£nh b√°o']
            if len(df_warning) > 0:
                df_warning.to_excel(writer, index=False, sheet_name='C·∫£nh b√°o')
        
        st.download_button(
            label="‚¨áÔ∏è Download Excel",
            data=buffer.getvalue(),
            file_name=f"Tin_CK_{datetime.now().strftime('%d%m%Y_%H%M')}.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )
        
        st.markdown("---")
        
        # Filters
        st.subheader("üîç L·ªåC & T√åM KI·∫æM")
        
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            search_code = st.text_input("M√£ CK", placeholder="VD: SHS")
        with col2:
            filter_san = st.selectbox("S√†n", ["T·∫•t c·∫£", "HNX", "UPCoM"])
        with col3:
            filter_risk = st.selectbox("Risk Level", ["T·∫•t c·∫£", "Nghi√™m tr·ªçng", "C·∫£nh b√°o", "B√¨nh th∆∞·ªùng", "T√≠ch c·ª±c"])
        with col4:
            filter_method = st.selectbox("T√¨m theo", ["T·∫•t c·∫£", "M√£ CK", "T√™n c√¥ng ty"])
        
        # Apply filters
        df_filtered = df.copy()
        
        if search_code:
            df_filtered = df_filtered[
                df_filtered['M√£ CK'].str.contains(search_code.upper(), case=False, na=False) |
                df_filtered['T√™n c√¥ng ty'].str.contains(search_code, case=False, na=False)
            ]
        
        if filter_san != "T·∫•t c·∫£":
            df_filtered = df_filtered[df_filtered['S√†n'] == filter_san]
        
        if filter_risk != "T·∫•t c·∫£":
            df_filtered = df_filtered[df_filtered['Risk'] == filter_risk]
        
        if filter_method != "T·∫•t c·∫£":
            df_filtered = df_filtered[df_filtered['T√¨m theo'] == filter_method]
        
        st.info(f"Hi·ªÉn th·ªã {len(df_filtered)} / {len(df)} b√†i")
        
        # Display articles
        st.subheader("üì∞ DANH S√ÅCH B√ÄI VI·∫æT")
        
        for idx, row in df_filtered.iterrows():
            if row['Risk'] == 'Nghi√™m tr·ªçng':
                card_class = "severe-card"
                icon = "‚ö†Ô∏è"
            elif row['Risk'] == 'C·∫£nh b√°o':
                card_class = "warning-card"
                icon = "‚ö†Ô∏è"
            elif row['Risk'] == 'T√≠ch c·ª±c':
                card_class = "positive-card"
                icon = "‚úÖ"
            else:
                card_class = "metric-card"
                icon = "üìÑ"
            
            with st.container():
                st.markdown(f'<div class="{card_class}">', unsafe_allow_html=True)
                
                col1, col2 = st.columns([4, 1])
                
                with col1:
                    if row['T√™n c√¥ng ty']:
                        st.markdown(f"**{icon} {row['M√£ CK']} - {row['T√™n c√¥ng ty']} ({row['S√†n']})**")
                    else:
                        st.markdown(f"**{icon} {row['M√£ CK']} ({row['S√†n']})**")
                    
                    st.markdown(f"{row['Ti√™u ƒë·ªÅ']}")
                    
                    caption_text = f"üìÖ {row['Ng√†y']} | "
                    caption_text += f"Sentiment: {row['Sentiment']} ({row['ƒêi·ªÉm']}) | "
                    caption_text += f"Risk: {row['Risk']} | "
                    caption_text += f"üîç {row['T√¨m theo']}"
                    
                    if row['Vi ph·∫°m']:
                        caption_text += f" | ‚öñÔ∏è {row['Vi ph·∫°m']}"
                    
                    st.caption(caption_text)
                
                with col2:
                    if st.button("üîó Xem", key=f"view_{idx}"):
                        st.markdown(f"[M·ªü b√†i vi·∫øt]({row['Link']})")
                
                # HI·ªÇN TH·ªä T√ìM T·∫ÆT
                if pd.notna(row['N·ªôi dung t√≥m t·∫Øt']) and row['N·ªôi dung t√≥m t·∫Øt']:
                    with st.expander("üìù Xem t√≥m t·∫Øt"):
                        st.write(row['N·ªôi dung t√≥m t·∫Øt'])
                
                if row['Keywords']:
                    st.info(f"üîë Keywords: {row['Keywords']}")
                
                st.markdown('</div>', unsafe_allow_html=True)
                st.markdown("<br>", unsafe_allow_html=True)
        
        # Dashboard
        st.markdown("---")
        st.subheader("üìä DASHBOARD")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.write("**Ph√¢n b·ªë Sentiment**")
            sentiment_counts = df['Sentiment'].value_counts()
            st.bar_chart(sentiment_counts)
        
        with col2:
            st.write("**Ph√¢n b·ªë Risk Level**")
            risk_counts = df['Risk'].value_counts()
            st.bar_chart(risk_counts)
        
        col3, col4 = st.columns(2)
        
        with col3:
            st.write("**Top 10 M√£ CK**")
            top_ma = df['M√£ CK'].value_counts().head(10)
            st.bar_chart(top_ma)
        
        with col4:
            st.write("**Ph√¢n b·ªë theo S√†n**")
            san_counts = df['S√†n'].value_counts()
            st.bar_chart(san_counts)
        
        # Chi ti·∫øt theo m√£
        st.markdown("---")
        st.subheader("üìà CHI TI·∫æT THEO M√É CK")
        
        with st.expander("Xem chi ti·∫øt"):
            summary = df.groupby('M√£ CK').agg({
                'Ti√™u ƒë·ªÅ': 'count',
                'ƒêi·ªÉm': 'mean',
                'Risk': lambda x: x.mode()[0] if len(x) > 0 else 'N/A'
            }).rename(columns={
                'Ti√™u ƒë·ªÅ': 'S·ªë b√†i',
                'ƒêi·ªÉm': 'Sentiment TB',
                'Risk': 'Risk ch√≠nh'
            }).reset_index()
            
            summary = summary.merge(
                df[['M√£ CK', 'T√™n c√¥ng ty', 'S√†n']].drop_duplicates(),
                on='M√£ CK',
                how='left'
            )
            
            summary['Sentiment TB'] = summary['Sentiment TB'].round(1)
            summary = summary.sort_values('S·ªë b√†i', ascending=False)
            
            st.dataframe(
                summary,
                use_container_width=True,
                hide_index=True
            )

if __name__ == "__main__":
    main(), prev_words):
                            continue
                    
                    # Check t·ª´ SAU "TIN"
                    if match.end() < len(text_upper) - 5:
                        next_chars = text_upper[match.end():match.end()+20]
                        if re.match(r'\s+(VUI|T·ªêT|X·∫§U|V·∫ÆN|CH·ª®NG|TH·ªä)', next_chars):
                            continue
                
                # ƒê·∫∂C BI·ªÜT 3: M√£ "CEO"
                if code == 'CEO':
                    # Check t·ª´ SAU "CEO"
                    if match.end() < len(text_upper) - 5:
                        next_chars = text_upper[match.end():match.end()+15]
                        if re.match(r'\s+(C√îNG\s+TY|C·ª¶A|M·ªöI)', next_chars):
                            continue
                    
                    # Check t·ª´ TR∆Ø·ªöC "CEO"
                    if match.start() >= 10:
                        prev_words = text_upper[match.start()-20:match.start()]
                        if re.search(r'(GI√ÅM\s+ƒê·ªêC|T·ªîNG\s+GI√ÅM\s+ƒê·ªêC)\s*$'):
    
    def fetch_url(self, url, max_retries=2):
        for attempt in range(max_retries):
            try:
                response = self.session.get(url, headers=self.headers, timeout=15)
                response.raise_for_status()
                return response
            except:
                if attempt < max_retries - 1:
                    time.sleep(1)
                return None
    
    def fetch_article_content(self, url):
        """L·∫•y n·ªôi dung b√†i vi·∫øt - t·ª´ V1.0"""
        try:
            response = self.fetch_url(url)
            if not response:
                return None, None, None
            
            response.encoding = 'utf-8'
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # T√¨m ng√†y
            date_text = None
            for pattern in [
                {'class': re.compile(r'date|time|publish', re.I)},
                {'itemprop': 'datePublished'}
            ]:
                date_elem = soup.find(['time', 'span', 'div'], pattern)
                if date_elem:
                    date_text = date_elem.get('datetime') or date_elem.get_text(strip=True)
                    break
            
            # Parse ng√†y (GMT+7)
            article_date_str = datetime.now(self.vietnam_tz).strftime('%d/%m/%Y')
            article_date_obj = datetime.now(self.vietnam_tz)
            
            # T√¨m n·ªôi dung
            content = ""
            for selector in [
                ('article', {}),
                ('div', {'class': re.compile(r'content|article|detail', re.I)}),
            ]:
                content_div = soup.find(selector[0], selector[1])
                if content_div:
                    paragraphs = content_div.find_all('p')
                    content = ' '.join([p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 50])
                    if content:
                        break
            
            if not content:
                paragraphs = soup.find_all('p')
                valid_p = [p.get_text(strip=True) for p in paragraphs if 50 < len(p.get_text(strip=True)) < 1000]
                content = ' '.join(valid_p[:8])
            
            content = self.clean_text(content)
            return content, article_date_str, article_date_obj
        
        except:
            return None, None, None
    
    def scrape_source(self, url, source_name, pattern, max_articles=20, progress_callback=None):
        try:
            response = self.fetch_url(url)
            if not response:
                return 0
            
            response.encoding = 'utf-8'
            soup = BeautifulSoup(response.text, 'html.parser')
            
            count = 0
            seen = set()
            links = soup.find_all('a', href=True)
            total_links = len(links)
            
            for idx, link_tag in enumerate(links):
                if progress_callback:
                    progress = (idx + 1) / total_links
                    progress_callback(f"{source_name}: {idx+1}/{total_links}", progress)
                
                href = link_tag.get('href', '')
                
                if pattern(href) and href not in seen:
                    title = link_tag.get_text(strip=True)
                    
                    if title and len(title) > 30:
                        self.stats['total_crawled'] += 1
                        seen.add(href)
                        
                        stock_code, exchange, match_method = self.extract_stock(title)
                        
                        if stock_code and exchange in ['HNX', 'UPCoM']:
                            full_link = urljoin(url, href)
                            
                            if match_method == 'code':
                                self.stats['found_by_code'] += 1
                            else:
                                self.stats['found_by_name'] += 1
                            
                            company_name = self.code_to_name.get(stock_code, '')
                            
                            # FETCH N·ªòI DUNG ƒê·∫¶Y ƒê·ª¶
                            content, article_date_str, article_date_obj = self.fetch_article_content(full_link)
                            
                            if content:
                                # T√ìM T·∫ÆT
                                summary = self.advanced_summarize(content, title, max_sentences=4)
                            else:
                                content = ""
                                summary = title  # Fallback n·∫øu kh√¥ng l·∫•y ƒë∆∞·ª£c content
                            
                            # SENTIMENT
                            sentiment_result = self.sentiment_analyzer.analyze_sentiment(title, content)
                            
                            if exchange == 'HNX':
                                self.stats['hnx_found'] += 1
                            else:
                                self.stats['upcom_found'] += 1
                            
                            if sentiment_result['risk_level'] == 'Nghi√™m tr·ªçng':
                                self.stats['severe_risk'] += 1
                            elif sentiment_result['risk_level'] == 'C·∫£nh b√°o':
                                self.stats['warning_risk'] += 1
                            
                            self.all_articles.append({
                                'Ti√™u ƒë·ªÅ': title,
                                'Link': full_link,
                                'Ng√†y': article_date_str,
                                'M√£ CK': stock_code,
                                'T√™n c√¥ng ty': company_name,
                                'S√†n': exchange,
                                'Sentiment': sentiment_result['sentiment_label'],
                                'ƒêi·ªÉm': sentiment_result['sentiment_score'],
                                'Risk': sentiment_result['risk_level'],
                                'Vi ph·∫°m': sentiment_result['violations'],
                                'Keywords': "; ".join([k['keyword'] for k in sentiment_result['keywords'][:3]]),
                                'N·ªôi dung t√≥m t·∫Øt': summary,  # ‚Üê C·ªòT M·ªöI
                                'T√¨m theo': 'M√£ CK' if match_method == 'code' else 'T√™n c√¥ng ty'
                            })
                            
                            count += 1
                            time.sleep(0.5)
                            
                            if count >= max_articles:
                                break
            
            return count
        
        except Exception as e:
            st.error(f"L·ªói {source_name}: {str(e)}")
            return 0
    
    def run(self, max_articles_per_source=20, progress_callback=None):
        sources = [
            ("https://cafef.vn/thi-truong-chung-khoan.chn", "CafeF", lambda h: '.chn' in h),
            ("https://vietstock.vn/chung-khoan.htm", "VietStock", lambda h: re.search(r'/\d{4}/\d{2}/.+\.htm', h)),
        ]
        
        for url, name, pattern in sources:
            self.scrape_source(url, name, pattern, max_articles_per_source, progress_callback)
            time.sleep(1)
        
        if len(self.all_articles) == 0:
            return None
        
        df = pd.DataFrame(self.all_articles)
        df = df.drop_duplicates(subset=['Ti√™u ƒë·ªÅ'], keep='first')
        df.insert(0, 'STT', range(1, len(df) + 1))
        
        return df

# ============================================================
# STREAMLIT APP
# ============================================================

def main():
    st.markdown('<div class="main-header">üìà TOOL C√ÄO TIN V2.4</div>', unsafe_allow_html=True)
    st.markdown('<div style="text-align:center;color:#666;margin-bottom:2rem;">HNX & UPCoM - Upload + Summarize + Sentiment</div>', unsafe_allow_html=True)
    
    # Sidebar
    with st.sidebar:
        st.header("‚öôÔ∏è C√ÄI ƒê·∫∂T")
        
        st.subheader("üìÇ DANH S√ÅCH M√É CK")
        st.markdown('<div class="upload-box">', unsafe_allow_html=True)
        st.write("**Upload file Excel/CSV**")
        st.caption("G·ªìm 3 c·ªôt: M√£ CK | S√†n | T√™n c√¥ng ty")
        
        uploaded_file = st.file_uploader(
            "Ch·ªçn file",
            type=['xlsx', 'xls', 'csv'],
            help="File ph·∫£i c√≥ c√°c c·ªôt: M√£ CK, S√†n (HNX/UPCoM), T√™n c√¥ng ty"
        )
        
        sample_excel = create_sample_excel()
        st.download_button(
            label="üì• T·∫£i file m·∫´u",
            data=sample_excel,
            file_name="mau_danh_sach_ma_ck.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )
        st.markdown('</div>', unsafe_allow_html=True)
        
        if uploaded_file is not None:
            stock_df, error = parse_stock_file(uploaded_file)
            
            if error:
                st.error(f"‚ùå {error}")
                st.session_state['stock_df'] = load_default_stock_list()
            else:
                st.success(f"‚úÖ ƒê√£ load {len(stock_df)} m√£ CK")
                st.session_state['stock_df'] = stock_df
                
                hnx_count = len(stock_df[stock_df['S√†n'] == 'HNX'])
                upcom_count = len(stock_df[stock_df['S√†n'] == 'UPCoM'])
                st.info(f"HNX: {hnx_count} | UPCoM: {upcom_count}")
        else:
            if 'stock_df' not in st.session_state:
                st.session_state['stock_df'] = load_default_stock_list()
                st.warning("‚ö†Ô∏è ƒêang d√πng danh s√°ch m·∫∑c ƒë·ªãnh")
        
        st.markdown("---")
        st.subheader("üîß T√ôY CH·ªàNH")
        
        time_filter = st.selectbox(
            "‚è∞ Kho·∫£ng th·ªùi gian",
            options=[6, 12, 24, 48, 72, 168],
            format_func=lambda x: f"{x} gi·ªù" if x < 168 else "1 tu·∫ßn",
            index=2
        )
        
        max_articles = st.slider(
            "üìä S·ªë b√†i t·ªëi ƒëa/ngu·ªìn",
            min_value=5,
            max_value=50,
            value=20,
            step=5
        )
        
        st.markdown("---")
        st.info("üí° **H∆∞·ªõng d·∫´n:**\n1. Upload danh s√°ch m√£\n2. Ch·ªçn th·ªùi gian\n3. B·∫•m 'B·∫Øt ƒë·∫ßu'\n4. Download Excel")
    
    # Main content
    if st.button("üöÄ B·∫ÆT ƒê·∫¶U C√ÄO TIN", type="primary"):
        stock_df = st.session_state.get('stock_df')
        
        if stock_df is None or len(stock_df) == 0:
            st.error("‚ùå Ch∆∞a c√≥ danh s√°ch m√£ CK! Vui l√≤ng upload file.")
            return
        
        with st.spinner("ƒêang c√†o tin..."):
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            def update_progress(message, progress):
                status_text.text(message)
                progress_bar.progress(progress)
            
            scraper = StockScraperWeb(stock_df, time_filter_hours=time_filter)
            df = scraper.run(max_articles_per_source=max_articles, progress_callback=update_progress)
            
            progress_bar.empty()
            status_text.empty()
            
            if df is not None:
                st.success(f"‚úÖ Ho√†n t·∫•t! T√¨m th·∫•y {len(df)} b√†i vi·∫øt")
                st.info(f"üîç T√¨m theo m√£ CK: {scraper.stats['found_by_code']} | T√¨m theo t√™n: {scraper.stats['found_by_name']}")
                
                st.session_state['df'] = df
                st.session_state['stats'] = scraper.stats
            else:
                st.error("Kh√¥ng t√¨m th·∫•y b√†i vi·∫øt n√†o!")
    
    # Display results
    if 'df' in st.session_state:
        df = st.session_state['df']
        stats = st.session_state['stats']
        
        # Metrics
        col1, col2, col3, col4, col5 = st.columns(5)
        with col1:
            st.metric("üìä T·ªïng b√†i", len(df))
        with col2:
            st.metric("‚ö†Ô∏è Nghi√™m tr·ªçng", stats['severe_risk'])
        with col3:
            st.metric("‚ö†Ô∏è C·∫£nh b√°o", stats['warning_risk'])
        with col4:
            st.metric("üî§ T√¨m theo m√£", stats['found_by_code'])
        with col5:
            st.metric("üìù T√¨m theo t√™n", stats['found_by_name'])
        
        # Download button
        buffer = io.BytesIO()
        with pd.ExcelWriter(buffer, engine='openpyxl') as writer:
            df.to_excel(writer, index=False, sheet_name='T·∫•t c·∫£')
            
            df_severe = df[df['Risk'] == 'Nghi√™m tr·ªçng']
            if len(df_severe) > 0:
                df_severe.to_excel(writer, index=False, sheet_name='Nghi√™m tr·ªçng')
            
            df_warning = df[df['Risk'] == 'C·∫£nh b√°o']
            if len(df_warning) > 0:
                df_warning.to_excel(writer, index=False, sheet_name='C·∫£nh b√°o')
        
        st.download_button(
            label="‚¨áÔ∏è Download Excel",
            data=buffer.getvalue(),
            file_name=f"Tin_CK_{datetime.now().strftime('%d%m%Y_%H%M')}.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )
        
        st.markdown("---")
        
        # Filters
        st.subheader("üîç L·ªåC & T√åM KI·∫æM")
        
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            search_code = st.text_input("M√£ CK", placeholder="VD: SHS")
        with col2:
            filter_san = st.selectbox("S√†n", ["T·∫•t c·∫£", "HNX", "UPCoM"])
        with col3:
            filter_risk = st.selectbox("Risk Level", ["T·∫•t c·∫£", "Nghi√™m tr·ªçng", "C·∫£nh b√°o", "B√¨nh th∆∞·ªùng", "T√≠ch c·ª±c"])
        with col4:
            filter_method = st.selectbox("T√¨m theo", ["T·∫•t c·∫£", "M√£ CK", "T√™n c√¥ng ty"])
        
        # Apply filters
        df_filtered = df.copy()
        
        if search_code:
            df_filtered = df_filtered[
                df_filtered['M√£ CK'].str.contains(search_code.upper(), case=False, na=False) |
                df_filtered['T√™n c√¥ng ty'].str.contains(search_code, case=False, na=False)
            ]
        
        if filter_san != "T·∫•t c·∫£":
            df_filtered = df_filtered[df_filtered['S√†n'] == filter_san]
        
        if filter_risk != "T·∫•t c·∫£":
            df_filtered = df_filtered[df_filtered['Risk'] == filter_risk]
        
        if filter_method != "T·∫•t c·∫£":
            df_filtered = df_filtered[df_filtered['T√¨m theo'] == filter_method]
        
        st.info(f"Hi·ªÉn th·ªã {len(df_filtered)} / {len(df)} b√†i")
        
        # Display articles
        st.subheader("üì∞ DANH S√ÅCH B√ÄI VI·∫æT")
        
        for idx, row in df_filtered.iterrows():
            if row['Risk'] == 'Nghi√™m tr·ªçng':
                card_class = "severe-card"
                icon = "‚ö†Ô∏è"
            elif row['Risk'] == 'C·∫£nh b√°o':
                card_class = "warning-card"
                icon = "‚ö†Ô∏è"
            elif row['Risk'] == 'T√≠ch c·ª±c':
                card_class = "positive-card"
                icon = "‚úÖ"
            else:
                card_class = "metric-card"
                icon = "üìÑ"
            
            with st.container():
                st.markdown(f'<div class="{card_class}">', unsafe_allow_html=True)
                
                col1, col2 = st.columns([4, 1])
                
                with col1:
                    if row['T√™n c√¥ng ty']:
                        st.markdown(f"**{icon} {row['M√£ CK']} - {row['T√™n c√¥ng ty']} ({row['S√†n']})**")
                    else:
                        st.markdown(f"**{icon} {row['M√£ CK']} ({row['S√†n']})**")
                    
                    st.markdown(f"{row['Ti√™u ƒë·ªÅ']}")
                    
                    caption_text = f"üìÖ {row['Ng√†y']} | "
                    caption_text += f"Sentiment: {row['Sentiment']} ({row['ƒêi·ªÉm']}) | "
                    caption_text += f"Risk: {row['Risk']} | "
                    caption_text += f"üîç {row['T√¨m theo']}"
                    
                    if row['Vi ph·∫°m']:
                        caption_text += f" | ‚öñÔ∏è {row['Vi ph·∫°m']}"
                    
                    st.caption(caption_text)
                
                with col2:
                    if st.button("üîó Xem", key=f"view_{idx}"):
                        st.markdown(f"[M·ªü b√†i vi·∫øt]({row['Link']})")
                
                # HI·ªÇN TH·ªä T√ìM T·∫ÆT
                if pd.notna(row['N·ªôi dung t√≥m t·∫Øt']) and row['N·ªôi dung t√≥m t·∫Øt']:
                    with st.expander("üìù Xem t√≥m t·∫Øt"):
                        st.write(row['N·ªôi dung t√≥m t·∫Øt'])
                
                if row['Keywords']:
                    st.info(f"üîë Keywords: {row['Keywords']}")
                
                st.markdown('</div>', unsafe_allow_html=True)
                st.markdown("<br>", unsafe_allow_html=True)
        
        # Dashboard
        st.markdown("---")
        st.subheader("üìä DASHBOARD")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.write("**Ph√¢n b·ªë Sentiment**")
            sentiment_counts = df['Sentiment'].value_counts()
            st.bar_chart(sentiment_counts)
        
        with col2:
            st.write("**Ph√¢n b·ªë Risk Level**")
            risk_counts = df['Risk'].value_counts()
            st.bar_chart(risk_counts)
        
        col3, col4 = st.columns(2)
        
        with col3:
            st.write("**Top 10 M√£ CK**")
            top_ma = df['M√£ CK'].value_counts().head(10)
            st.bar_chart(top_ma)
        
        with col4:
            st.write("**Ph√¢n b·ªë theo S√†n**")
            san_counts = df['S√†n'].value_counts()
            st.bar_chart(san_counts)
        
        # Chi ti·∫øt theo m√£
        st.markdown("---")
        st.subheader("üìà CHI TI·∫æT THEO M√É CK")
        
        with st.expander("Xem chi ti·∫øt"):
            summary = df.groupby('M√£ CK').agg({
                'Ti√™u ƒë·ªÅ': 'count',
                'ƒêi·ªÉm': 'mean',
                'Risk': lambda x: x.mode()[0] if len(x) > 0 else 'N/A'
            }).rename(columns={
                'Ti√™u ƒë·ªÅ': 'S·ªë b√†i',
                'ƒêi·ªÉm': 'Sentiment TB',
                'Risk': 'Risk ch√≠nh'
            }).reset_index()
            
            summary = summary.merge(
                df[['M√£ CK', 'T√™n c√¥ng ty', 'S√†n']].drop_duplicates(),
                on='M√£ CK',
                how='left'
            )
            
            summary['Sentiment TB'] = summary['Sentiment TB'].round(1)
            summary = summary.sort_values('S·ªë b√†i', ascending=False)
            
            st.dataframe(
                summary,
                use_container_width=True,
                hide_index=True
            )

if __name__ == "__main__":
    main(), prev_words):
                            continue
                
                return code, 'HNX', 'code'
        
        for code in self.upcom_stocks:
            match = re.search(r'\b' + code + r'\b', text_upper)
            if match:
                context = text_upper[max(0, match.start()-15):match.end()+15]
                
                if re.search(r'CH·ª®NG KHO√ÅN\s+' + code, context):
                    continue
                if re.search(r'CTCK\s+' + code, context):
                    continue
                
                # √Åp d·ª•ng logic ƒë·∫∑c bi·ªát cho TOP, TIN, CEO n·∫øu c√≥ trong UPCoM
                if code == 'TOP':
                    if match.start() > 0:
                        prev_char = text_upper[match.start()-1]
                        if prev_char.isalnum():
                            continue
                    if match.end() < len(text_upper) - 1:
                        next_chars = text_upper[match.end():match.end()+15]
                        if re.match(r'\s+\d+', next_chars) or re.match(r'\s+(C·ªî|M√É)', next_chars):
                            continue
                
                if code == 'TIN':
                    if match.start() >= 5:
                        prev_words = text_upper[match.start()-15:match.start()]
                        if re.search(r'(NH·∫¨N|THEO|M·ªòT|C√ÅC|NHI·ªÄU)\s*$'):
    
    def fetch_url(self, url, max_retries=2):
        for attempt in range(max_retries):
            try:
                response = self.session.get(url, headers=self.headers, timeout=15)
                response.raise_for_status()
                return response
            except:
                if attempt < max_retries - 1:
                    time.sleep(1)
                return None
    
    def fetch_article_content(self, url):
        """L·∫•y n·ªôi dung b√†i vi·∫øt - t·ª´ V1.0"""
        try:
            response = self.fetch_url(url)
            if not response:
                return None, None, None
            
            response.encoding = 'utf-8'
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # T√¨m ng√†y
            date_text = None
            for pattern in [
                {'class': re.compile(r'date|time|publish', re.I)},
                {'itemprop': 'datePublished'}
            ]:
                date_elem = soup.find(['time', 'span', 'div'], pattern)
                if date_elem:
                    date_text = date_elem.get('datetime') or date_elem.get_text(strip=True)
                    break
            
            # Parse ng√†y (GMT+7)
            article_date_str = datetime.now(self.vietnam_tz).strftime('%d/%m/%Y')
            article_date_obj = datetime.now(self.vietnam_tz)
            
            # T√¨m n·ªôi dung
            content = ""
            for selector in [
                ('article', {}),
                ('div', {'class': re.compile(r'content|article|detail', re.I)}),
            ]:
                content_div = soup.find(selector[0], selector[1])
                if content_div:
                    paragraphs = content_div.find_all('p')
                    content = ' '.join([p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 50])
                    if content:
                        break
            
            if not content:
                paragraphs = soup.find_all('p')
                valid_p = [p.get_text(strip=True) for p in paragraphs if 50 < len(p.get_text(strip=True)) < 1000]
                content = ' '.join(valid_p[:8])
            
            content = self.clean_text(content)
            return content, article_date_str, article_date_obj
        
        except:
            return None, None, None
    
    def scrape_source(self, url, source_name, pattern, max_articles=20, progress_callback=None):
        try:
            response = self.fetch_url(url)
            if not response:
                return 0
            
            response.encoding = 'utf-8'
            soup = BeautifulSoup(response.text, 'html.parser')
            
            count = 0
            seen = set()
            links = soup.find_all('a', href=True)
            total_links = len(links)
            
            for idx, link_tag in enumerate(links):
                if progress_callback:
                    progress = (idx + 1) / total_links
                    progress_callback(f"{source_name}: {idx+1}/{total_links}", progress)
                
                href = link_tag.get('href', '')
                
                if pattern(href) and href not in seen:
                    title = link_tag.get_text(strip=True)
                    
                    if title and len(title) > 30:
                        self.stats['total_crawled'] += 1
                        seen.add(href)
                        
                        stock_code, exchange, match_method = self.extract_stock(title)
                        
                        if stock_code and exchange in ['HNX', 'UPCoM']:
                            full_link = urljoin(url, href)
                            
                            if match_method == 'code':
                                self.stats['found_by_code'] += 1
                            else:
                                self.stats['found_by_name'] += 1
                            
                            company_name = self.code_to_name.get(stock_code, '')
                            
                            # FETCH N·ªòI DUNG ƒê·∫¶Y ƒê·ª¶
                            content, article_date_str, article_date_obj = self.fetch_article_content(full_link)
                            
                            if content:
                                # T√ìM T·∫ÆT
                                summary = self.advanced_summarize(content, title, max_sentences=4)
                            else:
                                content = ""
                                summary = title  # Fallback n·∫øu kh√¥ng l·∫•y ƒë∆∞·ª£c content
                            
                            # SENTIMENT
                            sentiment_result = self.sentiment_analyzer.analyze_sentiment(title, content)
                            
                            if exchange == 'HNX':
                                self.stats['hnx_found'] += 1
                            else:
                                self.stats['upcom_found'] += 1
                            
                            if sentiment_result['risk_level'] == 'Nghi√™m tr·ªçng':
                                self.stats['severe_risk'] += 1
                            elif sentiment_result['risk_level'] == 'C·∫£nh b√°o':
                                self.stats['warning_risk'] += 1
                            
                            self.all_articles.append({
                                'Ti√™u ƒë·ªÅ': title,
                                'Link': full_link,
                                'Ng√†y': article_date_str,
                                'M√£ CK': stock_code,
                                'T√™n c√¥ng ty': company_name,
                                'S√†n': exchange,
                                'Sentiment': sentiment_result['sentiment_label'],
                                'ƒêi·ªÉm': sentiment_result['sentiment_score'],
                                'Risk': sentiment_result['risk_level'],
                                'Vi ph·∫°m': sentiment_result['violations'],
                                'Keywords': "; ".join([k['keyword'] for k in sentiment_result['keywords'][:3]]),
                                'N·ªôi dung t√≥m t·∫Øt': summary,  # ‚Üê C·ªòT M·ªöI
                                'T√¨m theo': 'M√£ CK' if match_method == 'code' else 'T√™n c√¥ng ty'
                            })
                            
                            count += 1
                            time.sleep(0.5)
                            
                            if count >= max_articles:
                                break
            
            return count
        
        except Exception as e:
            st.error(f"L·ªói {source_name}: {str(e)}")
            return 0
    
    def run(self, max_articles_per_source=20, progress_callback=None):
        sources = [
            ("https://cafef.vn/thi-truong-chung-khoan.chn", "CafeF", lambda h: '.chn' in h),
            ("https://vietstock.vn/chung-khoan.htm", "VietStock", lambda h: re.search(r'/\d{4}/\d{2}/.+\.htm', h)),
        ]
        
        for url, name, pattern in sources:
            self.scrape_source(url, name, pattern, max_articles_per_source, progress_callback)
            time.sleep(1)
        
        if len(self.all_articles) == 0:
            return None
        
        df = pd.DataFrame(self.all_articles)
        df = df.drop_duplicates(subset=['Ti√™u ƒë·ªÅ'], keep='first')
        df.insert(0, 'STT', range(1, len(df) + 1))
        
        return df

# ============================================================
# STREAMLIT APP
# ============================================================

def main():
    st.markdown('<div class="main-header">üìà TOOL C√ÄO TIN V2.4</div>', unsafe_allow_html=True)
    st.markdown('<div style="text-align:center;color:#666;margin-bottom:2rem;">HNX & UPCoM - Upload + Summarize + Sentiment</div>', unsafe_allow_html=True)
    
    # Sidebar
    with st.sidebar:
        st.header("‚öôÔ∏è C√ÄI ƒê·∫∂T")
        
        st.subheader("üìÇ DANH S√ÅCH M√É CK")
        st.markdown('<div class="upload-box">', unsafe_allow_html=True)
        st.write("**Upload file Excel/CSV**")
        st.caption("G·ªìm 3 c·ªôt: M√£ CK | S√†n | T√™n c√¥ng ty")
        
        uploaded_file = st.file_uploader(
            "Ch·ªçn file",
            type=['xlsx', 'xls', 'csv'],
            help="File ph·∫£i c√≥ c√°c c·ªôt: M√£ CK, S√†n (HNX/UPCoM), T√™n c√¥ng ty"
        )
        
        sample_excel = create_sample_excel()
        st.download_button(
            label="üì• T·∫£i file m·∫´u",
            data=sample_excel,
            file_name="mau_danh_sach_ma_ck.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )
        st.markdown('</div>', unsafe_allow_html=True)
        
        if uploaded_file is not None:
            stock_df, error = parse_stock_file(uploaded_file)
            
            if error:
                st.error(f"‚ùå {error}")
                st.session_state['stock_df'] = load_default_stock_list()
            else:
                st.success(f"‚úÖ ƒê√£ load {len(stock_df)} m√£ CK")
                st.session_state['stock_df'] = stock_df
                
                hnx_count = len(stock_df[stock_df['S√†n'] == 'HNX'])
                upcom_count = len(stock_df[stock_df['S√†n'] == 'UPCoM'])
                st.info(f"HNX: {hnx_count} | UPCoM: {upcom_count}")
        else:
            if 'stock_df' not in st.session_state:
                st.session_state['stock_df'] = load_default_stock_list()
                st.warning("‚ö†Ô∏è ƒêang d√πng danh s√°ch m·∫∑c ƒë·ªãnh")
        
        st.markdown("---")
        st.subheader("üîß T√ôY CH·ªàNH")
        
        time_filter = st.selectbox(
            "‚è∞ Kho·∫£ng th·ªùi gian",
            options=[6, 12, 24, 48, 72, 168],
            format_func=lambda x: f"{x} gi·ªù" if x < 168 else "1 tu·∫ßn",
            index=2
        )
        
        max_articles = st.slider(
            "üìä S·ªë b√†i t·ªëi ƒëa/ngu·ªìn",
            min_value=5,
            max_value=50,
            value=20,
            step=5
        )
        
        st.markdown("---")
        st.info("üí° **H∆∞·ªõng d·∫´n:**\n1. Upload danh s√°ch m√£\n2. Ch·ªçn th·ªùi gian\n3. B·∫•m 'B·∫Øt ƒë·∫ßu'\n4. Download Excel")
    
    # Main content
    if st.button("üöÄ B·∫ÆT ƒê·∫¶U C√ÄO TIN", type="primary"):
        stock_df = st.session_state.get('stock_df')
        
        if stock_df is None or len(stock_df) == 0:
            st.error("‚ùå Ch∆∞a c√≥ danh s√°ch m√£ CK! Vui l√≤ng upload file.")
            return
        
        with st.spinner("ƒêang c√†o tin..."):
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            def update_progress(message, progress):
                status_text.text(message)
                progress_bar.progress(progress)
            
            scraper = StockScraperWeb(stock_df, time_filter_hours=time_filter)
            df = scraper.run(max_articles_per_source=max_articles, progress_callback=update_progress)
            
            progress_bar.empty()
            status_text.empty()
            
            if df is not None:
                st.success(f"‚úÖ Ho√†n t·∫•t! T√¨m th·∫•y {len(df)} b√†i vi·∫øt")
                st.info(f"üîç T√¨m theo m√£ CK: {scraper.stats['found_by_code']} | T√¨m theo t√™n: {scraper.stats['found_by_name']}")
                
                st.session_state['df'] = df
                st.session_state['stats'] = scraper.stats
            else:
                st.error("Kh√¥ng t√¨m th·∫•y b√†i vi·∫øt n√†o!")
    
    # Display results
    if 'df' in st.session_state:
        df = st.session_state['df']
        stats = st.session_state['stats']
        
        # Metrics
        col1, col2, col3, col4, col5 = st.columns(5)
        with col1:
            st.metric("üìä T·ªïng b√†i", len(df))
        with col2:
            st.metric("‚ö†Ô∏è Nghi√™m tr·ªçng", stats['severe_risk'])
        with col3:
            st.metric("‚ö†Ô∏è C·∫£nh b√°o", stats['warning_risk'])
        with col4:
            st.metric("üî§ T√¨m theo m√£", stats['found_by_code'])
        with col5:
            st.metric("üìù T√¨m theo t√™n", stats['found_by_name'])
        
        # Download button
        buffer = io.BytesIO()
        with pd.ExcelWriter(buffer, engine='openpyxl') as writer:
            df.to_excel(writer, index=False, sheet_name='T·∫•t c·∫£')
            
            df_severe = df[df['Risk'] == 'Nghi√™m tr·ªçng']
            if len(df_severe) > 0:
                df_severe.to_excel(writer, index=False, sheet_name='Nghi√™m tr·ªçng')
            
            df_warning = df[df['Risk'] == 'C·∫£nh b√°o']
            if len(df_warning) > 0:
                df_warning.to_excel(writer, index=False, sheet_name='C·∫£nh b√°o')
        
        st.download_button(
            label="‚¨áÔ∏è Download Excel",
            data=buffer.getvalue(),
            file_name=f"Tin_CK_{datetime.now().strftime('%d%m%Y_%H%M')}.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )
        
        st.markdown("---")
        
        # Filters
        st.subheader("üîç L·ªåC & T√åM KI·∫æM")
        
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            search_code = st.text_input("M√£ CK", placeholder="VD: SHS")
        with col2:
            filter_san = st.selectbox("S√†n", ["T·∫•t c·∫£", "HNX", "UPCoM"])
        with col3:
            filter_risk = st.selectbox("Risk Level", ["T·∫•t c·∫£", "Nghi√™m tr·ªçng", "C·∫£nh b√°o", "B√¨nh th∆∞·ªùng", "T√≠ch c·ª±c"])
        with col4:
            filter_method = st.selectbox("T√¨m theo", ["T·∫•t c·∫£", "M√£ CK", "T√™n c√¥ng ty"])
        
        # Apply filters
        df_filtered = df.copy()
        
        if search_code:
            df_filtered = df_filtered[
                df_filtered['M√£ CK'].str.contains(search_code.upper(), case=False, na=False) |
                df_filtered['T√™n c√¥ng ty'].str.contains(search_code, case=False, na=False)
            ]
        
        if filter_san != "T·∫•t c·∫£":
            df_filtered = df_filtered[df_filtered['S√†n'] == filter_san]
        
        if filter_risk != "T·∫•t c·∫£":
            df_filtered = df_filtered[df_filtered['Risk'] == filter_risk]
        
        if filter_method != "T·∫•t c·∫£":
            df_filtered = df_filtered[df_filtered['T√¨m theo'] == filter_method]
        
        st.info(f"Hi·ªÉn th·ªã {len(df_filtered)} / {len(df)} b√†i")
        
        # Display articles
        st.subheader("üì∞ DANH S√ÅCH B√ÄI VI·∫æT")
        
        for idx, row in df_filtered.iterrows():
            if row['Risk'] == 'Nghi√™m tr·ªçng':
                card_class = "severe-card"
                icon = "‚ö†Ô∏è"
            elif row['Risk'] == 'C·∫£nh b√°o':
                card_class = "warning-card"
                icon = "‚ö†Ô∏è"
            elif row['Risk'] == 'T√≠ch c·ª±c':
                card_class = "positive-card"
                icon = "‚úÖ"
            else:
                card_class = "metric-card"
                icon = "üìÑ"
            
            with st.container():
                st.markdown(f'<div class="{card_class}">', unsafe_allow_html=True)
                
                col1, col2 = st.columns([4, 1])
                
                with col1:
                    if row['T√™n c√¥ng ty']:
                        st.markdown(f"**{icon} {row['M√£ CK']} - {row['T√™n c√¥ng ty']} ({row['S√†n']})**")
                    else:
                        st.markdown(f"**{icon} {row['M√£ CK']} ({row['S√†n']})**")
                    
                    st.markdown(f"{row['Ti√™u ƒë·ªÅ']}")
                    
                    caption_text = f"üìÖ {row['Ng√†y']} | "
                    caption_text += f"Sentiment: {row['Sentiment']} ({row['ƒêi·ªÉm']}) | "
                    caption_text += f"Risk: {row['Risk']} | "
                    caption_text += f"üîç {row['T√¨m theo']}"
                    
                    if row['Vi ph·∫°m']:
                        caption_text += f" | ‚öñÔ∏è {row['Vi ph·∫°m']}"
                    
                    st.caption(caption_text)
                
                with col2:
                    if st.button("üîó Xem", key=f"view_{idx}"):
                        st.markdown(f"[M·ªü b√†i vi·∫øt]({row['Link']})")
                
                # HI·ªÇN TH·ªä T√ìM T·∫ÆT
                if pd.notna(row['N·ªôi dung t√≥m t·∫Øt']) and row['N·ªôi dung t√≥m t·∫Øt']:
                    with st.expander("üìù Xem t√≥m t·∫Øt"):
                        st.write(row['N·ªôi dung t√≥m t·∫Øt'])
                
                if row['Keywords']:
                    st.info(f"üîë Keywords: {row['Keywords']}")
                
                st.markdown('</div>', unsafe_allow_html=True)
                st.markdown("<br>", unsafe_allow_html=True)
        
        # Dashboard
        st.markdown("---")
        st.subheader("üìä DASHBOARD")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.write("**Ph√¢n b·ªë Sentiment**")
            sentiment_counts = df['Sentiment'].value_counts()
            st.bar_chart(sentiment_counts)
        
        with col2:
            st.write("**Ph√¢n b·ªë Risk Level**")
            risk_counts = df['Risk'].value_counts()
            st.bar_chart(risk_counts)
        
        col3, col4 = st.columns(2)
        
        with col3:
            st.write("**Top 10 M√£ CK**")
            top_ma = df['M√£ CK'].value_counts().head(10)
            st.bar_chart(top_ma)
        
        with col4:
            st.write("**Ph√¢n b·ªë theo S√†n**")
            san_counts = df['S√†n'].value_counts()
            st.bar_chart(san_counts)
        
        # Chi ti·∫øt theo m√£
        st.markdown("---")
        st.subheader("üìà CHI TI·∫æT THEO M√É CK")
        
        with st.expander("Xem chi ti·∫øt"):
            summary = df.groupby('M√£ CK').agg({
                'Ti√™u ƒë·ªÅ': 'count',
                'ƒêi·ªÉm': 'mean',
                'Risk': lambda x: x.mode()[0] if len(x) > 0 else 'N/A'
            }).rename(columns={
                'Ti√™u ƒë·ªÅ': 'S·ªë b√†i',
                'ƒêi·ªÉm': 'Sentiment TB',
                'Risk': 'Risk ch√≠nh'
            }).reset_index()
            
            summary = summary.merge(
                df[['M√£ CK', 'T√™n c√¥ng ty', 'S√†n']].drop_duplicates(),
                on='M√£ CK',
                how='left'
            )
            
            summary['Sentiment TB'] = summary['Sentiment TB'].round(1)
            summary = summary.sort_values('S·ªë b√†i', ascending=False)
            
            st.dataframe(
                summary,
                use_container_width=True,
                hide_index=True
            )

if __name__ == "__main__":
    main(), prev_words):
                            continue
                    if match.end() < len(text_upper) - 5:
                        next_chars = text_upper[match.end():match.end()+20]
                        if re.match(r'\s+(VUI|T·ªêT|X·∫§U|V·∫ÆN|CH·ª®NG|TH·ªä)', next_chars):
                            continue
                
                if code == 'CEO':
                    if match.end() < len(text_upper) - 5:
                        next_chars = text_upper[match.end():match.end()+15]
                        if re.match(r'\s+(C√îNG\s+TY|C·ª¶A|M·ªöI)', next_chars):
                            continue
                    if match.start() >= 10:
                        prev_words = text_upper[match.start()-20:match.start()]
                        if re.search(r'(GI√ÅM\s+ƒê·ªêC|T·ªîNG\s+GI√ÅM\s+ƒê·ªêC)\s*$'):
    
    def fetch_url(self, url, max_retries=2):
        for attempt in range(max_retries):
            try:
                response = self.session.get(url, headers=self.headers, timeout=15)
                response.raise_for_status()
                return response
            except:
                if attempt < max_retries - 1:
                    time.sleep(1)
                return None
    
    def fetch_article_content(self, url):
        """L·∫•y n·ªôi dung b√†i vi·∫øt - t·ª´ V1.0"""
        try:
            response = self.fetch_url(url)
            if not response:
                return None, None, None
            
            response.encoding = 'utf-8'
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # T√¨m ng√†y
            date_text = None
            for pattern in [
                {'class': re.compile(r'date|time|publish', re.I)},
                {'itemprop': 'datePublished'}
            ]:
                date_elem = soup.find(['time', 'span', 'div'], pattern)
                if date_elem:
                    date_text = date_elem.get('datetime') or date_elem.get_text(strip=True)
                    break
            
            # Parse ng√†y (GMT+7)
            article_date_str = datetime.now(self.vietnam_tz).strftime('%d/%m/%Y')
            article_date_obj = datetime.now(self.vietnam_tz)
            
            # T√¨m n·ªôi dung
            content = ""
            for selector in [
                ('article', {}),
                ('div', {'class': re.compile(r'content|article|detail', re.I)}),
            ]:
                content_div = soup.find(selector[0], selector[1])
                if content_div:
                    paragraphs = content_div.find_all('p')
                    content = ' '.join([p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 50])
                    if content:
                        break
            
            if not content:
                paragraphs = soup.find_all('p')
                valid_p = [p.get_text(strip=True) for p in paragraphs if 50 < len(p.get_text(strip=True)) < 1000]
                content = ' '.join(valid_p[:8])
            
            content = self.clean_text(content)
            return content, article_date_str, article_date_obj
        
        except:
            return None, None, None
    
    def scrape_source(self, url, source_name, pattern, max_articles=20, progress_callback=None):
        try:
            response = self.fetch_url(url)
            if not response:
                return 0
            
            response.encoding = 'utf-8'
            soup = BeautifulSoup(response.text, 'html.parser')
            
            count = 0
            seen = set()
            links = soup.find_all('a', href=True)
            total_links = len(links)
            
            for idx, link_tag in enumerate(links):
                if progress_callback:
                    progress = (idx + 1) / total_links
                    progress_callback(f"{source_name}: {idx+1}/{total_links}", progress)
                
                href = link_tag.get('href', '')
                
                if pattern(href) and href not in seen:
                    title = link_tag.get_text(strip=True)
                    
                    if title and len(title) > 30:
                        self.stats['total_crawled'] += 1
                        seen.add(href)
                        
                        stock_code, exchange, match_method = self.extract_stock(title)
                        
                        if stock_code and exchange in ['HNX', 'UPCoM']:
                            full_link = urljoin(url, href)
                            
                            if match_method == 'code':
                                self.stats['found_by_code'] += 1
                            else:
                                self.stats['found_by_name'] += 1
                            
                            company_name = self.code_to_name.get(stock_code, '')
                            
                            # FETCH N·ªòI DUNG ƒê·∫¶Y ƒê·ª¶
                            content, article_date_str, article_date_obj = self.fetch_article_content(full_link)
                            
                            if content:
                                # T√ìM T·∫ÆT
                                summary = self.advanced_summarize(content, title, max_sentences=4)
                            else:
                                content = ""
                                summary = title  # Fallback n·∫øu kh√¥ng l·∫•y ƒë∆∞·ª£c content
                            
                            # SENTIMENT
                            sentiment_result = self.sentiment_analyzer.analyze_sentiment(title, content)
                            
                            if exchange == 'HNX':
                                self.stats['hnx_found'] += 1
                            else:
                                self.stats['upcom_found'] += 1
                            
                            if sentiment_result['risk_level'] == 'Nghi√™m tr·ªçng':
                                self.stats['severe_risk'] += 1
                            elif sentiment_result['risk_level'] == 'C·∫£nh b√°o':
                                self.stats['warning_risk'] += 1
                            
                            self.all_articles.append({
                                'Ti√™u ƒë·ªÅ': title,
                                'Link': full_link,
                                'Ng√†y': article_date_str,
                                'M√£ CK': stock_code,
                                'T√™n c√¥ng ty': company_name,
                                'S√†n': exchange,
                                'Sentiment': sentiment_result['sentiment_label'],
                                'ƒêi·ªÉm': sentiment_result['sentiment_score'],
                                'Risk': sentiment_result['risk_level'],
                                'Vi ph·∫°m': sentiment_result['violations'],
                                'Keywords': "; ".join([k['keyword'] for k in sentiment_result['keywords'][:3]]),
                                'N·ªôi dung t√≥m t·∫Øt': summary,  # ‚Üê C·ªòT M·ªöI
                                'T√¨m theo': 'M√£ CK' if match_method == 'code' else 'T√™n c√¥ng ty'
                            })
                            
                            count += 1
                            time.sleep(0.5)
                            
                            if count >= max_articles:
                                break
            
            return count
        
        except Exception as e:
            st.error(f"L·ªói {source_name}: {str(e)}")
            return 0
    
    def run(self, max_articles_per_source=20, progress_callback=None):
        sources = [
            ("https://cafef.vn/thi-truong-chung-khoan.chn", "CafeF", lambda h: '.chn' in h),
            ("https://vietstock.vn/chung-khoan.htm", "VietStock", lambda h: re.search(r'/\d{4}/\d{2}/.+\.htm', h)),
        ]
        
        for url, name, pattern in sources:
            self.scrape_source(url, name, pattern, max_articles_per_source, progress_callback)
            time.sleep(1)
        
        if len(self.all_articles) == 0:
            return None
        
        df = pd.DataFrame(self.all_articles)
        df = df.drop_duplicates(subset=['Ti√™u ƒë·ªÅ'], keep='first')
        df.insert(0, 'STT', range(1, len(df) + 1))
        
        return df

# ============================================================
# STREAMLIT APP
# ============================================================

def main():
    st.markdown('<div class="main-header">üìà TOOL C√ÄO TIN V2.4</div>', unsafe_allow_html=True)
    st.markdown('<div style="text-align:center;color:#666;margin-bottom:2rem;">HNX & UPCoM - Upload + Summarize + Sentiment</div>', unsafe_allow_html=True)
    
    # Sidebar
    with st.sidebar:
        st.header("‚öôÔ∏è C√ÄI ƒê·∫∂T")
        
        st.subheader("üìÇ DANH S√ÅCH M√É CK")
        st.markdown('<div class="upload-box">', unsafe_allow_html=True)
        st.write("**Upload file Excel/CSV**")
        st.caption("G·ªìm 3 c·ªôt: M√£ CK | S√†n | T√™n c√¥ng ty")
        
        uploaded_file = st.file_uploader(
            "Ch·ªçn file",
            type=['xlsx', 'xls', 'csv'],
            help="File ph·∫£i c√≥ c√°c c·ªôt: M√£ CK, S√†n (HNX/UPCoM), T√™n c√¥ng ty"
        )
        
        sample_excel = create_sample_excel()
        st.download_button(
            label="üì• T·∫£i file m·∫´u",
            data=sample_excel,
            file_name="mau_danh_sach_ma_ck.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )
        st.markdown('</div>', unsafe_allow_html=True)
        
        if uploaded_file is not None:
            stock_df, error = parse_stock_file(uploaded_file)
            
            if error:
                st.error(f"‚ùå {error}")
                st.session_state['stock_df'] = load_default_stock_list()
            else:
                st.success(f"‚úÖ ƒê√£ load {len(stock_df)} m√£ CK")
                st.session_state['stock_df'] = stock_df
                
                hnx_count = len(stock_df[stock_df['S√†n'] == 'HNX'])
                upcom_count = len(stock_df[stock_df['S√†n'] == 'UPCoM'])
                st.info(f"HNX: {hnx_count} | UPCoM: {upcom_count}")
        else:
            if 'stock_df' not in st.session_state:
                st.session_state['stock_df'] = load_default_stock_list()
                st.warning("‚ö†Ô∏è ƒêang d√πng danh s√°ch m·∫∑c ƒë·ªãnh")
        
        st.markdown("---")
        st.subheader("üîß T√ôY CH·ªàNH")
        
        time_filter = st.selectbox(
            "‚è∞ Kho·∫£ng th·ªùi gian",
            options=[6, 12, 24, 48, 72, 168],
            format_func=lambda x: f"{x} gi·ªù" if x < 168 else "1 tu·∫ßn",
            index=2
        )
        
        max_articles = st.slider(
            "üìä S·ªë b√†i t·ªëi ƒëa/ngu·ªìn",
            min_value=5,
            max_value=50,
            value=20,
            step=5
        )
        
        st.markdown("---")
        st.info("üí° **H∆∞·ªõng d·∫´n:**\n1. Upload danh s√°ch m√£\n2. Ch·ªçn th·ªùi gian\n3. B·∫•m 'B·∫Øt ƒë·∫ßu'\n4. Download Excel")
    
    # Main content
    if st.button("üöÄ B·∫ÆT ƒê·∫¶U C√ÄO TIN", type="primary"):
        stock_df = st.session_state.get('stock_df')
        
        if stock_df is None or len(stock_df) == 0:
            st.error("‚ùå Ch∆∞a c√≥ danh s√°ch m√£ CK! Vui l√≤ng upload file.")
            return
        
        with st.spinner("ƒêang c√†o tin..."):
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            def update_progress(message, progress):
                status_text.text(message)
                progress_bar.progress(progress)
            
            scraper = StockScraperWeb(stock_df, time_filter_hours=time_filter)
            df = scraper.run(max_articles_per_source=max_articles, progress_callback=update_progress)
            
            progress_bar.empty()
            status_text.empty()
            
            if df is not None:
                st.success(f"‚úÖ Ho√†n t·∫•t! T√¨m th·∫•y {len(df)} b√†i vi·∫øt")
                st.info(f"üîç T√¨m theo m√£ CK: {scraper.stats['found_by_code']} | T√¨m theo t√™n: {scraper.stats['found_by_name']}")
                
                st.session_state['df'] = df
                st.session_state['stats'] = scraper.stats
            else:
                st.error("Kh√¥ng t√¨m th·∫•y b√†i vi·∫øt n√†o!")
    
    # Display results
    if 'df' in st.session_state:
        df = st.session_state['df']
        stats = st.session_state['stats']
        
        # Metrics
        col1, col2, col3, col4, col5 = st.columns(5)
        with col1:
            st.metric("üìä T·ªïng b√†i", len(df))
        with col2:
            st.metric("‚ö†Ô∏è Nghi√™m tr·ªçng", stats['severe_risk'])
        with col3:
            st.metric("‚ö†Ô∏è C·∫£nh b√°o", stats['warning_risk'])
        with col4:
            st.metric("üî§ T√¨m theo m√£", stats['found_by_code'])
        with col5:
            st.metric("üìù T√¨m theo t√™n", stats['found_by_name'])
        
        # Download button
        buffer = io.BytesIO()
        with pd.ExcelWriter(buffer, engine='openpyxl') as writer:
            df.to_excel(writer, index=False, sheet_name='T·∫•t c·∫£')
            
            df_severe = df[df['Risk'] == 'Nghi√™m tr·ªçng']
            if len(df_severe) > 0:
                df_severe.to_excel(writer, index=False, sheet_name='Nghi√™m tr·ªçng')
            
            df_warning = df[df['Risk'] == 'C·∫£nh b√°o']
            if len(df_warning) > 0:
                df_warning.to_excel(writer, index=False, sheet_name='C·∫£nh b√°o')
        
        st.download_button(
            label="‚¨áÔ∏è Download Excel",
            data=buffer.getvalue(),
            file_name=f"Tin_CK_{datetime.now().strftime('%d%m%Y_%H%M')}.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )
        
        st.markdown("---")
        
        # Filters
        st.subheader("üîç L·ªåC & T√åM KI·∫æM")
        
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            search_code = st.text_input("M√£ CK", placeholder="VD: SHS")
        with col2:
            filter_san = st.selectbox("S√†n", ["T·∫•t c·∫£", "HNX", "UPCoM"])
        with col3:
            filter_risk = st.selectbox("Risk Level", ["T·∫•t c·∫£", "Nghi√™m tr·ªçng", "C·∫£nh b√°o", "B√¨nh th∆∞·ªùng", "T√≠ch c·ª±c"])
        with col4:
            filter_method = st.selectbox("T√¨m theo", ["T·∫•t c·∫£", "M√£ CK", "T√™n c√¥ng ty"])
        
        # Apply filters
        df_filtered = df.copy()
        
        if search_code:
            df_filtered = df_filtered[
                df_filtered['M√£ CK'].str.contains(search_code.upper(), case=False, na=False) |
                df_filtered['T√™n c√¥ng ty'].str.contains(search_code, case=False, na=False)
            ]
        
        if filter_san != "T·∫•t c·∫£":
            df_filtered = df_filtered[df_filtered['S√†n'] == filter_san]
        
        if filter_risk != "T·∫•t c·∫£":
            df_filtered = df_filtered[df_filtered['Risk'] == filter_risk]
        
        if filter_method != "T·∫•t c·∫£":
            df_filtered = df_filtered[df_filtered['T√¨m theo'] == filter_method]
        
        st.info(f"Hi·ªÉn th·ªã {len(df_filtered)} / {len(df)} b√†i")
        
        # Display articles
        st.subheader("üì∞ DANH S√ÅCH B√ÄI VI·∫æT")
        
        for idx, row in df_filtered.iterrows():
            if row['Risk'] == 'Nghi√™m tr·ªçng':
                card_class = "severe-card"
                icon = "‚ö†Ô∏è"
            elif row['Risk'] == 'C·∫£nh b√°o':
                card_class = "warning-card"
                icon = "‚ö†Ô∏è"
            elif row['Risk'] == 'T√≠ch c·ª±c':
                card_class = "positive-card"
                icon = "‚úÖ"
            else:
                card_class = "metric-card"
                icon = "üìÑ"
            
            with st.container():
                st.markdown(f'<div class="{card_class}">', unsafe_allow_html=True)
                
                col1, col2 = st.columns([4, 1])
                
                with col1:
                    if row['T√™n c√¥ng ty']:
                        st.markdown(f"**{icon} {row['M√£ CK']} - {row['T√™n c√¥ng ty']} ({row['S√†n']})**")
                    else:
                        st.markdown(f"**{icon} {row['M√£ CK']} ({row['S√†n']})**")
                    
                    st.markdown(f"{row['Ti√™u ƒë·ªÅ']}")
                    
                    caption_text = f"üìÖ {row['Ng√†y']} | "
                    caption_text += f"Sentiment: {row['Sentiment']} ({row['ƒêi·ªÉm']}) | "
                    caption_text += f"Risk: {row['Risk']} | "
                    caption_text += f"üîç {row['T√¨m theo']}"
                    
                    if row['Vi ph·∫°m']:
                        caption_text += f" | ‚öñÔ∏è {row['Vi ph·∫°m']}"
                    
                    st.caption(caption_text)
                
                with col2:
                    if st.button("üîó Xem", key=f"view_{idx}"):
                        st.markdown(f"[M·ªü b√†i vi·∫øt]({row['Link']})")
                
                # HI·ªÇN TH·ªä T√ìM T·∫ÆT
                if pd.notna(row['N·ªôi dung t√≥m t·∫Øt']) and row['N·ªôi dung t√≥m t·∫Øt']:
                    with st.expander("üìù Xem t√≥m t·∫Øt"):
                        st.write(row['N·ªôi dung t√≥m t·∫Øt'])
                
                if row['Keywords']:
                    st.info(f"üîë Keywords: {row['Keywords']}")
                
                st.markdown('</div>', unsafe_allow_html=True)
                st.markdown("<br>", unsafe_allow_html=True)
        
        # Dashboard
        st.markdown("---")
        st.subheader("üìä DASHBOARD")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.write("**Ph√¢n b·ªë Sentiment**")
            sentiment_counts = df['Sentiment'].value_counts()
            st.bar_chart(sentiment_counts)
        
        with col2:
            st.write("**Ph√¢n b·ªë Risk Level**")
            risk_counts = df['Risk'].value_counts()
            st.bar_chart(risk_counts)
        
        col3, col4 = st.columns(2)
        
        with col3:
            st.write("**Top 10 M√£ CK**")
            top_ma = df['M√£ CK'].value_counts().head(10)
            st.bar_chart(top_ma)
        
        with col4:
            st.write("**Ph√¢n b·ªë theo S√†n**")
            san_counts = df['S√†n'].value_counts()
            st.bar_chart(san_counts)
        
        # Chi ti·∫øt theo m√£
        st.markdown("---")
        st.subheader("üìà CHI TI·∫æT THEO M√É CK")
        
        with st.expander("Xem chi ti·∫øt"):
            summary = df.groupby('M√£ CK').agg({
                'Ti√™u ƒë·ªÅ': 'count',
                'ƒêi·ªÉm': 'mean',
                'Risk': lambda x: x.mode()[0] if len(x) > 0 else 'N/A'
            }).rename(columns={
                'Ti√™u ƒë·ªÅ': 'S·ªë b√†i',
                'ƒêi·ªÉm': 'Sentiment TB',
                'Risk': 'Risk ch√≠nh'
            }).reset_index()
            
            summary = summary.merge(
                df[['M√£ CK', 'T√™n c√¥ng ty', 'S√†n']].drop_duplicates(),
                on='M√£ CK',
                how='left'
            )
            
            summary['Sentiment TB'] = summary['Sentiment TB'].round(1)
            summary = summary.sort_values('S·ªë b√†i', ascending=False)
            
            st.dataframe(
                summary,
                use_container_width=True,
                hide_index=True
            )

if __name__ == "__main__":
    main(), prev_words):
                            continue
                
                return code, 'UPCoM', 'code'
        
        # T√¨m theo t√™n
        words = text_lower.split()
        matched_codes = []
        for word in words:
            if len(word) > 3 and word in self.name_to_code:
                matched_codes.extend(self.name_to_code[word])
        
        if matched_codes:
            from collections import Counter
            most_common = Counter(matched_codes).most_common(1)[0][0]
            exchange = self.stock_to_exchange.get(most_common)
            return most_common, exchange, 'name'
        
        return None, None, None
    
    def fetch_url(self, url, max_retries=2):
        for attempt in range(max_retries):
            try:
                response = self.session.get(url, headers=self.headers, timeout=15)
                response.raise_for_status()
                return response
            except:
                if attempt < max_retries - 1:
                    time.sleep(1)
                return None
    
    def fetch_article_content(self, url):
        """L·∫•y n·ªôi dung b√†i vi·∫øt - t·ª´ V1.0"""
        try:
            response = self.fetch_url(url)
            if not response:
                return None, None, None
            
            response.encoding = 'utf-8'
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # T√¨m ng√†y
            date_text = None
            for pattern in [
                {'class': re.compile(r'date|time|publish', re.I)},
                {'itemprop': 'datePublished'}
            ]:
                date_elem = soup.find(['time', 'span', 'div'], pattern)
                if date_elem:
                    date_text = date_elem.get('datetime') or date_elem.get_text(strip=True)
                    break
            
            # Parse ng√†y (GMT+7)
            article_date_str = datetime.now(self.vietnam_tz).strftime('%d/%m/%Y')
            article_date_obj = datetime.now(self.vietnam_tz)
            
            # T√¨m n·ªôi dung
            content = ""
            for selector in [
                ('article', {}),
                ('div', {'class': re.compile(r'content|article|detail', re.I)}),
            ]:
                content_div = soup.find(selector[0], selector[1])
                if content_div:
                    paragraphs = content_div.find_all('p')
                    content = ' '.join([p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 50])
                    if content:
                        break
            
            if not content:
                paragraphs = soup.find_all('p')
                valid_p = [p.get_text(strip=True) for p in paragraphs if 50 < len(p.get_text(strip=True)) < 1000]
                content = ' '.join(valid_p[:8])
            
            content = self.clean_text(content)
            return content, article_date_str, article_date_obj
        
        except:
            return None, None, None
    
    def scrape_source(self, url, source_name, pattern, max_articles=20, progress_callback=None):
        try:
            response = self.fetch_url(url)
            if not response:
                return 0
            
            response.encoding = 'utf-8'
            soup = BeautifulSoup(response.text, 'html.parser')
            
            count = 0
            seen = set()
            links = soup.find_all('a', href=True)
            total_links = len(links)
            
            for idx, link_tag in enumerate(links):
                if progress_callback:
                    progress = (idx + 1) / total_links
                    progress_callback(f"{source_name}: {idx+1}/{total_links}", progress)
                
                href = link_tag.get('href', '')
                
                if pattern(href) and href not in seen:
                    title = link_tag.get_text(strip=True)
                    
                    if title and len(title) > 30:
                        self.stats['total_crawled'] += 1
                        seen.add(href)
                        
                        stock_code, exchange, match_method = self.extract_stock(title)
                        
                        if stock_code and exchange in ['HNX', 'UPCoM']:
                            full_link = urljoin(url, href)
                            
                            if match_method == 'code':
                                self.stats['found_by_code'] += 1
                            else:
                                self.stats['found_by_name'] += 1
                            
                            company_name = self.code_to_name.get(stock_code, '')
                            
                            # FETCH N·ªòI DUNG ƒê·∫¶Y ƒê·ª¶
                            content, article_date_str, article_date_obj = self.fetch_article_content(full_link)
                            
                            if content:
                                # T√ìM T·∫ÆT
                                summary = self.advanced_summarize(content, title, max_sentences=4)
                            else:
                                content = ""
                                summary = title  # Fallback n·∫øu kh√¥ng l·∫•y ƒë∆∞·ª£c content
                            
                            # SENTIMENT
                            sentiment_result = self.sentiment_analyzer.analyze_sentiment(title, content)
                            
                            if exchange == 'HNX':
                                self.stats['hnx_found'] += 1
                            else:
                                self.stats['upcom_found'] += 1
                            
                            if sentiment_result['risk_level'] == 'Nghi√™m tr·ªçng':
                                self.stats['severe_risk'] += 1
                            elif sentiment_result['risk_level'] == 'C·∫£nh b√°o':
                                self.stats['warning_risk'] += 1
                            
                            self.all_articles.append({
                                'Ti√™u ƒë·ªÅ': title,
                                'Link': full_link,
                                'Ng√†y': article_date_str,
                                'M√£ CK': stock_code,
                                'T√™n c√¥ng ty': company_name,
                                'S√†n': exchange,
                                'Sentiment': sentiment_result['sentiment_label'],
                                'ƒêi·ªÉm': sentiment_result['sentiment_score'],
                                'Risk': sentiment_result['risk_level'],
                                'Vi ph·∫°m': sentiment_result['violations'],
                                'Keywords': "; ".join([k['keyword'] for k in sentiment_result['keywords'][:3]]),
                                'N·ªôi dung t√≥m t·∫Øt': summary,  # ‚Üê C·ªòT M·ªöI
                                'T√¨m theo': 'M√£ CK' if match_method == 'code' else 'T√™n c√¥ng ty'
                            })
                            
                            count += 1
                            time.sleep(0.5)
                            
                            if count >= max_articles:
                                break
            
            return count
        
        except Exception as e:
            st.error(f"L·ªói {source_name}: {str(e)}")
            return 0
    
    def run(self, max_articles_per_source=20, progress_callback=None):
        sources = [
            ("https://cafef.vn/thi-truong-chung-khoan.chn", "CafeF", lambda h: '.chn' in h),
            ("https://vietstock.vn/chung-khoan.htm", "VietStock", lambda h: re.search(r'/\d{4}/\d{2}/.+\.htm', h)),
        ]
        
        for url, name, pattern in sources:
            self.scrape_source(url, name, pattern, max_articles_per_source, progress_callback)
            time.sleep(1)
        
        if len(self.all_articles) == 0:
            return None
        
        df = pd.DataFrame(self.all_articles)
        df = df.drop_duplicates(subset=['Ti√™u ƒë·ªÅ'], keep='first')
        df.insert(0, 'STT', range(1, len(df) + 1))
        
        return df

# ============================================================
# STREAMLIT APP
# ============================================================

def main():
    st.markdown('<div class="main-header">üìà TOOL C√ÄO TIN V2.4</div>', unsafe_allow_html=True)
    st.markdown('<div style="text-align:center;color:#666;margin-bottom:2rem;">HNX & UPCoM - Upload + Summarize + Sentiment</div>', unsafe_allow_html=True)
    
    # Sidebar
    with st.sidebar:
        st.header("‚öôÔ∏è C√ÄI ƒê·∫∂T")
        
        st.subheader("üìÇ DANH S√ÅCH M√É CK")
        st.markdown('<div class="upload-box">', unsafe_allow_html=True)
        st.write("**Upload file Excel/CSV**")
        st.caption("G·ªìm 3 c·ªôt: M√£ CK | S√†n | T√™n c√¥ng ty")
        
        uploaded_file = st.file_uploader(
            "Ch·ªçn file",
            type=['xlsx', 'xls', 'csv'],
            help="File ph·∫£i c√≥ c√°c c·ªôt: M√£ CK, S√†n (HNX/UPCoM), T√™n c√¥ng ty"
        )
        
        sample_excel = create_sample_excel()
        st.download_button(
            label="üì• T·∫£i file m·∫´u",
            data=sample_excel,
            file_name="mau_danh_sach_ma_ck.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )
        st.markdown('</div>', unsafe_allow_html=True)
        
        if uploaded_file is not None:
            stock_df, error = parse_stock_file(uploaded_file)
            
            if error:
                st.error(f"‚ùå {error}")
                st.session_state['stock_df'] = load_default_stock_list()
            else:
                st.success(f"‚úÖ ƒê√£ load {len(stock_df)} m√£ CK")
                st.session_state['stock_df'] = stock_df
                
                hnx_count = len(stock_df[stock_df['S√†n'] == 'HNX'])
                upcom_count = len(stock_df[stock_df['S√†n'] == 'UPCoM'])
                st.info(f"HNX: {hnx_count} | UPCoM: {upcom_count}")
        else:
            if 'stock_df' not in st.session_state:
                st.session_state['stock_df'] = load_default_stock_list()
                st.warning("‚ö†Ô∏è ƒêang d√πng danh s√°ch m·∫∑c ƒë·ªãnh")
        
        st.markdown("---")
        st.subheader("üîß T√ôY CH·ªàNH")
        
        time_filter = st.selectbox(
            "‚è∞ Kho·∫£ng th·ªùi gian",
            options=[6, 12, 24, 48, 72, 168],
            format_func=lambda x: f"{x} gi·ªù" if x < 168 else "1 tu·∫ßn",
            index=2
        )
        
        max_articles = st.slider(
            "üìä S·ªë b√†i t·ªëi ƒëa/ngu·ªìn",
            min_value=5,
            max_value=50,
            value=20,
            step=5
        )
        
        st.markdown("---")
        st.info("üí° **H∆∞·ªõng d·∫´n:**\n1. Upload danh s√°ch m√£\n2. Ch·ªçn th·ªùi gian\n3. B·∫•m 'B·∫Øt ƒë·∫ßu'\n4. Download Excel")
    
    # Main content
    if st.button("üöÄ B·∫ÆT ƒê·∫¶U C√ÄO TIN", type="primary"):
        stock_df = st.session_state.get('stock_df')
        
        if stock_df is None or len(stock_df) == 0:
            st.error("‚ùå Ch∆∞a c√≥ danh s√°ch m√£ CK! Vui l√≤ng upload file.")
            return
        
        with st.spinner("ƒêang c√†o tin..."):
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            def update_progress(message, progress):
                status_text.text(message)
                progress_bar.progress(progress)
            
            scraper = StockScraperWeb(stock_df, time_filter_hours=time_filter)
            df = scraper.run(max_articles_per_source=max_articles, progress_callback=update_progress)
            
            progress_bar.empty()
            status_text.empty()
            
            if df is not None:
                st.success(f"‚úÖ Ho√†n t·∫•t! T√¨m th·∫•y {len(df)} b√†i vi·∫øt")
                st.info(f"üîç T√¨m theo m√£ CK: {scraper.stats['found_by_code']} | T√¨m theo t√™n: {scraper.stats['found_by_name']}")
                
                st.session_state['df'] = df
                st.session_state['stats'] = scraper.stats
            else:
                st.error("Kh√¥ng t√¨m th·∫•y b√†i vi·∫øt n√†o!")
    
    # Display results
    if 'df' in st.session_state:
        df = st.session_state['df']
        stats = st.session_state['stats']
        
        # Metrics
        col1, col2, col3, col4, col5 = st.columns(5)
        with col1:
            st.metric("üìä T·ªïng b√†i", len(df))
        with col2:
            st.metric("‚ö†Ô∏è Nghi√™m tr·ªçng", stats['severe_risk'])
        with col3:
            st.metric("‚ö†Ô∏è C·∫£nh b√°o", stats['warning_risk'])
        with col4:
            st.metric("üî§ T√¨m theo m√£", stats['found_by_code'])
        with col5:
            st.metric("üìù T√¨m theo t√™n", stats['found_by_name'])
        
        # Download button
        buffer = io.BytesIO()
        with pd.ExcelWriter(buffer, engine='openpyxl') as writer:
            df.to_excel(writer, index=False, sheet_name='T·∫•t c·∫£')
            
            df_severe = df[df['Risk'] == 'Nghi√™m tr·ªçng']
            if len(df_severe) > 0:
                df_severe.to_excel(writer, index=False, sheet_name='Nghi√™m tr·ªçng')
            
            df_warning = df[df['Risk'] == 'C·∫£nh b√°o']
            if len(df_warning) > 0:
                df_warning.to_excel(writer, index=False, sheet_name='C·∫£nh b√°o')
        
        st.download_button(
            label="‚¨áÔ∏è Download Excel",
            data=buffer.getvalue(),
            file_name=f"Tin_CK_{datetime.now().strftime('%d%m%Y_%H%M')}.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )
        
        st.markdown("---")
        
        # Filters
        st.subheader("üîç L·ªåC & T√åM KI·∫æM")
        
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            search_code = st.text_input("M√£ CK", placeholder="VD: SHS")
        with col2:
            filter_san = st.selectbox("S√†n", ["T·∫•t c·∫£", "HNX", "UPCoM"])
        with col3:
            filter_risk = st.selectbox("Risk Level", ["T·∫•t c·∫£", "Nghi√™m tr·ªçng", "C·∫£nh b√°o", "B√¨nh th∆∞·ªùng", "T√≠ch c·ª±c"])
        with col4:
            filter_method = st.selectbox("T√¨m theo", ["T·∫•t c·∫£", "M√£ CK", "T√™n c√¥ng ty"])
        
        # Apply filters
        df_filtered = df.copy()
        
        if search_code:
            df_filtered = df_filtered[
                df_filtered['M√£ CK'].str.contains(search_code.upper(), case=False, na=False) |
                df_filtered['T√™n c√¥ng ty'].str.contains(search_code, case=False, na=False)
            ]
        
        if filter_san != "T·∫•t c·∫£":
            df_filtered = df_filtered[df_filtered['S√†n'] == filter_san]
        
        if filter_risk != "T·∫•t c·∫£":
            df_filtered = df_filtered[df_filtered['Risk'] == filter_risk]
        
        if filter_method != "T·∫•t c·∫£":
            df_filtered = df_filtered[df_filtered['T√¨m theo'] == filter_method]
        
        st.info(f"Hi·ªÉn th·ªã {len(df_filtered)} / {len(df)} b√†i")
        
        # Display articles
        st.subheader("üì∞ DANH S√ÅCH B√ÄI VI·∫æT")
        
        for idx, row in df_filtered.iterrows():
            if row['Risk'] == 'Nghi√™m tr·ªçng':
                card_class = "severe-card"
                icon = "‚ö†Ô∏è"
            elif row['Risk'] == 'C·∫£nh b√°o':
                card_class = "warning-card"
                icon = "‚ö†Ô∏è"
            elif row['Risk'] == 'T√≠ch c·ª±c':
                card_class = "positive-card"
                icon = "‚úÖ"
            else:
                card_class = "metric-card"
                icon = "üìÑ"
            
            with st.container():
                st.markdown(f'<div class="{card_class}">', unsafe_allow_html=True)
                
                col1, col2 = st.columns([4, 1])
                
                with col1:
                    if row['T√™n c√¥ng ty']:
                        st.markdown(f"**{icon} {row['M√£ CK']} - {row['T√™n c√¥ng ty']} ({row['S√†n']})**")
                    else:
                        st.markdown(f"**{icon} {row['M√£ CK']} ({row['S√†n']})**")
                    
                    st.markdown(f"{row['Ti√™u ƒë·ªÅ']}")
                    
                    caption_text = f"üìÖ {row['Ng√†y']} | "
                    caption_text += f"Sentiment: {row['Sentiment']} ({row['ƒêi·ªÉm']}) | "
                    caption_text += f"Risk: {row['Risk']} | "
                    caption_text += f"üîç {row['T√¨m theo']}"
                    
                    if row['Vi ph·∫°m']:
                        caption_text += f" | ‚öñÔ∏è {row['Vi ph·∫°m']}"
                    
                    st.caption(caption_text)
                
                with col2:
                    if st.button("üîó Xem", key=f"view_{idx}"):
                        st.markdown(f"[M·ªü b√†i vi·∫øt]({row['Link']})")
                
                # HI·ªÇN TH·ªä T√ìM T·∫ÆT
                if pd.notna(row['N·ªôi dung t√≥m t·∫Øt']) and row['N·ªôi dung t√≥m t·∫Øt']:
                    with st.expander("üìù Xem t√≥m t·∫Øt"):
                        st.write(row['N·ªôi dung t√≥m t·∫Øt'])
                
                if row['Keywords']:
                    st.info(f"üîë Keywords: {row['Keywords']}")
                
                st.markdown('</div>', unsafe_allow_html=True)
                st.markdown("<br>", unsafe_allow_html=True)
        
        # Dashboard
        st.markdown("---")
        st.subheader("üìä DASHBOARD")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.write("**Ph√¢n b·ªë Sentiment**")
            sentiment_counts = df['Sentiment'].value_counts()
            st.bar_chart(sentiment_counts)
        
        with col2:
            st.write("**Ph√¢n b·ªë Risk Level**")
            risk_counts = df['Risk'].value_counts()
            st.bar_chart(risk_counts)
        
        col3, col4 = st.columns(2)
        
        with col3:
            st.write("**Top 10 M√£ CK**")
            top_ma = df['M√£ CK'].value_counts().head(10)
            st.bar_chart(top_ma)
        
        with col4:
            st.write("**Ph√¢n b·ªë theo S√†n**")
            san_counts = df['S√†n'].value_counts()
            st.bar_chart(san_counts)
        
        # Chi ti·∫øt theo m√£
        st.markdown("---")
        st.subheader("üìà CHI TI·∫æT THEO M√É CK")
        
        with st.expander("Xem chi ti·∫øt"):
            summary = df.groupby('M√£ CK').agg({
                'Ti√™u ƒë·ªÅ': 'count',
                'ƒêi·ªÉm': 'mean',
                'Risk': lambda x: x.mode()[0] if len(x) > 0 else 'N/A'
            }).rename(columns={
                'Ti√™u ƒë·ªÅ': 'S·ªë b√†i',
                'ƒêi·ªÉm': 'Sentiment TB',
                'Risk': 'Risk ch√≠nh'
            }).reset_index()
            
            summary = summary.merge(
                df[['M√£ CK', 'T√™n c√¥ng ty', 'S√†n']].drop_duplicates(),
                on='M√£ CK',
                how='left'
            )
            
            summary['Sentiment TB'] = summary['Sentiment TB'].round(1)
            summary = summary.sort_values('S·ªë b√†i', ascending=False)
            
            st.dataframe(
                summary,
                use_container_width=True,
                hide_index=True
            )

if __name__ == "__main__":
    main()
