# ============================================================
# üéØ STREAMLIT WEB APP V2.4 - UPLOAD + SUMMARIZE
# ============================================================
# ‚úÖ Upload danh s√°ch m√£ CK
# ‚úÖ T√≥m t·∫Øt extractive (t·ª´ V1.0)
# ‚úÖ Sentiment analysis
# ‚úÖ Risk detection
# ============================================================

import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime, timedelta, timezone
import time
import re
from urllib.parse import urljoin
import io

# ============================================================
# CONFIG
# ============================================================

st.set_page_config(
    page_title="C√†o Tin Ch·ª©ng Kho√°n V2.4",
    page_icon="üìà",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ============================================================
# CSS
# ============================================================

st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 1rem;
    }
    .upload-box {
        background-color: #e8f4f8;
        padding: 1.5rem;
        border-radius: 0.5rem;
        border: 2px dashed #1f77b4;
        margin: 1rem 0;
    }
    .severe-card {
        background-color: #ffe6e6;
        border-left: 5px solid #ff4444;
        padding: 1rem;
        margin: 0.5rem 0;
        border-radius: 0.3rem;
    }
    .warning-card {
        background-color: #fff8e6;
        border-left: 5px solid #ffaa00;
        padding: 1rem;
        margin: 0.5rem 0;
        border-radius: 0.3rem;
    }
    .positive-card {
        background-color: #e6ffe6;
        border-left: 5px solid #44ff44;
        padding: 1rem;
        margin: 0.5rem 0;
        border-radius: 0.3rem;
    }
</style>
""", unsafe_allow_html=True)

# ============================================================
# HELPER FUNCTIONS
# ============================================================

def load_default_stock_list():
    """Danh s√°ch m√£ m·∫∑c ƒë·ªãnh"""
    default_data = {
        'M√£ CK': ['SHS', 'PVS', 'NVB', 'VCS', 'BVS', 'CEO', 'VGC', 'PVC',
                  'LPB', 'EIB', 'BAB', 'OCB', 'HDG', 'PAN'],
        'S√†n': ['HNX']*8 + ['UPCoM']*6,
        'T√™n c√¥ng ty': ['Ch·ª©ng kho√°n SHS', 'Ch·ª©ng kho√°n PVS', 'Ng√¢n h√†ng NVB',
                        'Ch·ª©ng kho√°n VCS', 'Ch·ª©ng kho√°n BVS', 'T·∫≠p ƒëo√†n CEO',
                        'Viglacera', 'PVC', 'Ng√¢n h√†ng LPB', 'Ng√¢n h√†ng EIB',
                        'Ng√¢n h√†ng BAB', 'Ng√¢n h√†ng OCB', 'T·∫≠p ƒëo√†n HDG', 'PAN Group']
    }
    return pd.DataFrame(default_data)

def parse_stock_file(uploaded_file):
    """Parse Excel/CSV file"""
    try:
        if uploaded_file.name.endswith('.csv'):
            df = pd.read_csv(uploaded_file)
        else:
            df = pd.read_excel(uploaded_file)
        
        df.columns = df.columns.str.strip().str.lower()
        
        column_mapping = {
            'm√£ ck': 'M√£ CK', 'ma ck': 'M√£ CK', 'm√£': 'M√£ CK', 'code': 'M√£ CK',
            's√†n': 'S√†n', 'san': 'S√†n', 'exchange': 'S√†n',
            't√™n c√¥ng ty': 'T√™n c√¥ng ty', 'ten cong ty': 'T√™n c√¥ng ty', 'name': 'T√™n c√¥ng ty',
        }
        
        for old_col, new_col in column_mapping.items():
            if old_col in df.columns:
                df.rename(columns={old_col: new_col}, inplace=True)
        
        required_cols = ['M√£ CK', 'S√†n']
        missing_cols = [col for col in required_cols if col not in df.columns]
        
        if missing_cols:
            return None, f"Thi·∫øu c√°c c·ªôt: {', '.join(missing_cols)}"
        
        if 'T√™n c√¥ng ty' not in df.columns:
            df['T√™n c√¥ng ty'] = ''
        
        df['M√£ CK'] = df['M√£ CK'].astype(str).str.strip().str.upper()
        df['S√†n'] = df['S√†n'].astype(str).str.strip().str.upper()
        df['T√™n c√¥ng ty'] = df['T√™n c√¥ng ty'].astype(str).str.strip()
        
        df = df[df['S√†n'].isin(['HNX', 'UPCOM'])]
        df['S√†n'] = df['S√†n'].replace('UPCOM', 'UPCoM')
        df = df.drop_duplicates(subset=['M√£ CK'])
        
        return df, None
        
    except Exception as e:
        return None, f"L·ªói ƒë·ªçc file: {str(e)}"

def create_sample_excel():
    """T·∫°o file Excel m·∫´u"""
    sample_data = {
        'M√£ CK': ['SHS', 'PVS', 'NVB', 'LPB', 'EIB', 'CEO'],
        'S√†n': ['HNX', 'HNX', 'HNX', 'UPCoM', 'UPCoM', 'HNX'],
        'T√™n c√¥ng ty': ['Ch·ª©ng kho√°n S√†i G√≤n - H√† N·ªôi', 'Ch·ª©ng kho√°n D·∫ßu kh√≠', 
                        'Ng√¢n h√†ng Qu·ªëc d√¢n', 'Ng√¢n h√†ng L·ªôc Ph√°t', 
                        'Ng√¢n h√†ng Xu·∫•t nh·∫≠p kh·∫©u', 'T·∫≠p ƒëo√†n CEO']
    }
    df = pd.DataFrame(sample_data)
    
    buffer = io.BytesIO()
    with pd.ExcelWriter(buffer, engine='openpyxl') as writer:
        df.to_excel(writer, index=False, sheet_name='Danh s√°ch m√£')
    
    return buffer.getvalue()

# ============================================================
# KEYWORD RISK DETECTOR
# ============================================================

class KeywordRiskDetector:
    def __init__(self):
        self.keywords_db = {
            # A. N·ªôi b·ªô & Qu·∫£n tr·ªã
            "l√£nh ƒë·∫°o b·ªã b·∫Øt": {"category": "A. N·ªôi b·ªô", "severity": "severe", "score": -95, "violation": "I.2, II.A"},
            "l√£nh ƒë·∫°o b·ªè tr·ªën": {"category": "A. N·ªôi b·ªô", "severity": "severe", "score": -95, "violation": "I.2, II.A"},
            "c·ªï ƒë√¥ng l·ªõn b√°n chui": {"category": "A. N·ªôi b·ªô", "severity": "severe", "score": -85, "violation": "I.1, II.A"},
            "ch·ªß t·ªãch b·∫•t ng·ªù tho√°i h·∫øt v·ªën": {"category": "A. N·ªôi b·ªô", "severity": "severe", "score": -85, "violation": "I.1, II.A"},
            
            # B. T√†i ch√≠nh
            "b·∫•t ng·ªù b√°o l·ªó": {"category": "B. T√†i ch√≠nh", "severity": "severe", "score": -80, "violation": "I.4, II.B"},
            "√¢m v·ªën ch·ªß": {"category": "B. T√†i ch√≠nh", "severity": "severe", "score": -90, "violation": "II.B"},
            "m·∫•t kh·∫£ nƒÉng thanh to√°n": {"category": "B. T√†i ch√≠nh", "severity": "severe", "score": -90, "violation": "II.B"},
            "n·ª£ x·∫•u b·∫•t th∆∞·ªùng": {"category": "B. T√†i ch√≠nh", "severity": "severe", "score": -80, "violation": "II.B"},
            
            # C. Thao t√∫ng & Bi·∫øn ƒë·ªông gi√° b·∫•t th∆∞·ªùng
            "ƒë·ªôi l√°i l√†m gi√°": {"category": "C. Thao t√∫ng", "severity": "severe", "score": -95, "violation": "I.3, II.C"},
            "tƒÉng tr·∫ßn li√™n ti·∫øp": {"category": "C. Thao t√∫ng", "severity": "warning", "score": -60, "violation": "I.2, II.C"},
            "gi·∫£m s√†n li√™n t·ª•c": {"category": "C. Thao t√∫ng", "severity": "warning", "score": -70, "violation": "I.2, II.C"},
            "b·ªëc ƒë·∫ßu": {"category": "C. Thao t√∫ng", "severity": "warning", "score": -65, "violation": "I.2, I.3, II.C"},
            "k·ªãch tr·∫ßn": {"category": "C. Thao t√∫ng", "severity": "warning", "score": -65, "violation": "I.2, I.3, II.C"},
            "r·ªõt ƒë√°y": {"category": "C. Thao t√∫ng", "severity": "warning", "score": -70, "violation": "I.2, I.3, II.C"},
            "c·ªï phi·∫øu tƒÉng phi m√£": {"category": "C. Thao t√∫ng", "severity": "warning", "score": -65, "violation": "I.2, I.4, II.C"},
            "tƒÉng d·ª±ng ƒë·ª©ng": {"category": "C. Thao t√∫ng", "severity": "warning", "score": -60, "violation": "I.2, II.C"},
            "kh·ªëi l∆∞·ª£ng tƒÉng b·∫•t th∆∞·ªùng": {"category": "C. Thao t√∫ng", "severity": "warning", "score": -65, "violation": "I.6, II.C"},
            "giao d·ªãch n·ªôi gi√°n": {"category": "C. Thao t√∫ng", "severity": "severe", "score": -90, "violation": "I.1, II.C"},
            
            # D. M&A
            "ni√™m y·∫øt c·ª≠a sau": {"category": "D. M&A", "severity": "severe", "score": -85, "violation": "I.5, II.D"},
            "th√¢u t√≥m": {"category": "D. M&A", "severity": "warning", "score": -50, "violation": "I.5, II.D"},
            
            # E. Ph√°p l√Ω
            "c√¥ng an ƒëi·ªÅu tra": {"category": "E. Ph√°p l√Ω", "severity": "severe", "score": -90, "violation": "II.E"},
            "kh·ªüi t·ªë l√£nh ƒë·∫°o": {"category": "E. Ph√°p l√Ω", "severity": "severe", "score": -95, "violation": "II.E"},
            "gian l·∫≠n t√†i ch√≠nh": {"category": "E. Ph√°p l√Ω", "severity": "severe", "score": -95, "violation": "II.E"},
            
            # F. S·ª± ki·ªán b√™n ngo√†i
            "ch√°y nh√† x∆∞·ªüng": {"category": "F. S·ª± ki·ªán ngo√†i", "severity": "severe", "score": -75, "violation": "II.F"},
            "b·ªã thu h·ªìi gi·∫•y ph√©p": {"category": "F. S·ª± ki·ªán ngo√†i", "severity": "severe", "score": -90, "violation": "II.F"},
            
            # T√≠ch c·ª±c
            "l·ª£i nhu·∫≠n tƒÉng": {"category": "T√≠ch c·ª±c", "severity": "positive", "score": 70, "violation": ""},
            "tƒÉng tr∆∞·ªüng m·∫°nh": {"category": "T√≠ch c·ª±c", "severity": "positive", "score": 65, "violation": ""},
            "doanh thu k·ª∑ l·ª•c": {"category": "T√≠ch c·ª±c", "severity": "positive", "score": 75, "violation": ""},
        }
    
    def analyze(self, text):
        text_lower = text.lower()
        found_keywords = []
        total_score = 0
        categories = set()
        violations = set()
        max_severity = "normal"
        
        for keyword, info in self.keywords_db.items():
            if keyword in text_lower:
                found_keywords.append({
                    "keyword": keyword,
                    "category": info["category"],
                    "severity": info["severity"],
                    "score": info["score"],
                    "violation": info["violation"]
                })
                total_score += info["score"]
                categories.add(info["category"])
                if info["violation"]:
                    violations.add(info["violation"])
                
                if info["severity"] == "severe":
                    max_severity = "severe"
                elif info["severity"] == "warning" and max_severity != "severe":
                    max_severity = "warning"
                elif info["severity"] == "positive" and max_severity == "normal":
                    max_severity = "positive"
        
        return {
            "keywords": found_keywords,
            "total_score": total_score,
            "severity": max_severity,
            "categories": list(categories),
            "violations": ", ".join(sorted(violations))
        }

# ============================================================
# SENTIMENT ANALYZER
# ============================================================

class SimpleSentimentAnalyzer:
    def __init__(self):
        self.keyword_detector = KeywordRiskDetector()
        self.positive_words = ['tƒÉng', 'tƒÉng tr∆∞·ªüng', 'l·ª£i nhu·∫≠n', 'th√†nh c√¥ng', 't·ªët', 'cao', 'm·∫°nh', 'v∆∞·ª£t']
        self.negative_words = ['gi·∫£m', 's·ª•t gi·∫£m', 'l·ªó', 'thua l·ªó', 'kh√≥ khƒÉn', 'ti√™u c·ª±c', 'suy gi·∫£m']
    
    def analyze_sentiment(self, title, content):
        text = (title + " " + content).lower()
        keyword_analysis = self.keyword_detector.analyze(title + " " + content)
        
        pos_count = sum(1 for word in self.positive_words if word in text)
        neg_count = sum(1 for word in self.negative_words if word in text)
        
        base_score = 50 + (pos_count * 5) - (neg_count * 5)
        
        if keyword_analysis["severity"] == "severe":
            final_score = min(20, base_score + keyword_analysis["total_score"])
        elif keyword_analysis["severity"] == "warning":
            final_score = min(40, base_score + keyword_analysis["total_score"] * 0.7)
        elif keyword_analysis["severity"] == "positive":
            final_score = max(60, base_score + keyword_analysis["total_score"])
        else:
            final_score = base_score
        
        final_score = max(0, min(100, final_score))
        
        if final_score >= 60:
            label = "T√≠ch c·ª±c"
        elif final_score >= 40:
            label = "Trung l·∫≠p"
        else:
            label = "Ti√™u c·ª±c"
        
        if keyword_analysis["severity"] == "severe":
            risk_level = "Nghi√™m tr·ªçng"
        elif keyword_analysis["severity"] == "warning":
            risk_level = "C·∫£nh b√°o"
        elif keyword_analysis["severity"] == "positive":
            risk_level = "T√≠ch c·ª±c"
        else:
            risk_level = "B√¨nh th∆∞·ªùng"
        
        return {
            "sentiment_score": round(final_score, 1),
            "sentiment_label": label,
            "risk_level": risk_level,
            "keywords": keyword_analysis["keywords"],
            "categories": ", ".join(keyword_analysis["categories"]) if keyword_analysis["categories"] else "",
            "violations": keyword_analysis["violations"]
        }

# ============================================================
# STOCK SCRAPER - FIXED VERSION
# ============================================================

class StockScraperWeb:
    def __init__(self, stock_df, time_filter_hours=24):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept-Language': 'vi-VN,vi;q=0.9,en;q=0.8',
        }
        self.all_articles = []
        self.session = requests.Session()
        self.time_filter_hours = time_filter_hours
        
        self.vietnam_tz = timezone(timedelta(hours=7))
        self.cutoff_time = datetime.now(self.vietnam_tz) - timedelta(hours=time_filter_hours)
        
        self.sentiment_analyzer = SimpleSentimentAnalyzer()
        
        # Load stock list
        self.stock_df = stock_df
        self.hose_stocks = set(stock_df[stock_df['S√†n'] == 'HOSE']['M√£ CK'].tolist())
        self.hnx_stocks = set(stock_df[stock_df['S√†n'] == 'HNX']['M√£ CK'].tolist())
        self.upcom_stocks = set(stock_df[stock_df['S√†n'] == 'UPCoM']['M√£ CK'].tolist())
        
        # T·∫°o set t·ªïng h·ª£p T·∫§T C·∫¢ m√£ CK
        self.all_stock_codes = self.hose_stocks | self.hnx_stocks | self.upcom_stocks
        
        self.code_to_name = dict(zip(stock_df['M√£ CK'], stock_df['T√™n c√¥ng ty']))
        
        self.name_to_code = {}
        for code, name in self.code_to_name.items():
            if name:
                words = name.lower().split()
                for word in words:
                    if len(word) > 3:
                        if word not in self.name_to_code:
                            self.name_to_code[word] = []
                        self.name_to_code[word].append(code)
        
        self.stock_to_exchange = {}
        for code in self.hose_stocks:
            self.stock_to_exchange[code] = 'HOSE'
        for code in self.hnx_stocks:
            self.stock_to_exchange[code] = 'HNX'
        for code in self.upcom_stocks:
            self.stock_to_exchange[code] = 'UPCoM'
        
        self.stats = {
            'total_crawled': 0,
            'hnx_found': 0,
            'upcom_found': 0,
            'hose_only_filtered': 0,  # Th√™m stat m·ªõi
            'severe_risk': 0,
            'warning_risk': 0,
            'found_by_code': 0,
            'found_by_name': 0
        }
    
    def clean_text(self, text):
        """L√†m s·∫°ch text - t·ª´ V1.0"""
        if not text:
            return ""
        text = re.sub(r'[^\w\s.,;:!?()%\-\+\/\"\'√†√°·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√®√©·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±·ª≥√Ω·ª∑·ªπ·ªµƒë√Ä√Å·∫¢√É·∫†ƒÇ·∫Æ·∫∞·∫≤·∫¥·∫∂√Ç·∫§·∫¶·∫®·∫™·∫¨√à√â·∫∫·∫º·∫∏√ä·∫æ·ªÄ·ªÇ·ªÑ·ªÜ√å√ç·ªàƒ®·ªä√í√ì·ªé√ï·ªå√î·ªê·ªí·ªî·ªñ·ªò∆†·ªö·ªú·ªû·ª†·ª¢√ô√ö·ª¶≈®·ª§∆Ø·ª®·ª™·ª¨·ªÆ·ª∞·ª≤√ù·ª∂·ª∏·ª¥ƒê]', ' ', text)
        text = re.sub(r'\s+', ' ', text)
        return text.strip()
    
    def advanced_summarize(self, content, title, max_sentences=4):
        """T√≥m t·∫Øt EXTRACTIVE - t·ª´ V1.0"""
        content = self.clean_text(content)
        title = self.clean_text(title)
        
        if not content or len(content) < 100:
            return content
        
        full_text = title + ". " + content
        sentences = re.split(r'[.!?]+', full_text)
        sentences = [s.strip() for s in sentences if len(s.strip()) > 30]
        
        if len(sentences) <= max_sentences:
            return '. '.join(sentences) + '.'
        
        important_keywords = {
            'tƒÉng': 3, 'gi·∫£m': 3, 'tƒÉng tr∆∞·ªüng': 3,
            'l·ª£i nhu·∫≠n': 4, 'doanh thu': 4, 'l·ªó': 3,
            't·ª∑ ƒë·ªìng': 3, 'ngh√¨n t·ª∑': 4,
            'c·ªï phi·∫øu': 3, 'ni√™m y·∫øt': 3,
            'giao d·ªãch': 2, 'thanh kho·∫£n': 3,
            'qu√Ω': 3, 'nƒÉm': 2,
            'ph√°t h√†nh': 3, 'tr√°i phi·∫øu': 3,
            'ƒë·∫ßu t∆∞': 2, 'v·ªën': 3,
        }
        
        scored_sentences = []
        for i, sentence in enumerate(sentences):
            score = 0
            sentence_lower = sentence.lower()
            
            if i == 0:
                score += 5
            elif i == 1:
                score += 3
            elif i < 5:
                score += 1
            
            for keyword, weight in important_keywords.items():
                if keyword in sentence_lower:
                    score += weight
            
            numbers = re.findall(r'\d+(?:[.,]\d+)*', sentence)
            if numbers:
                score += len(numbers)
                if any(num for num in numbers if len(num.replace('.', '').replace(',', '')) >= 4):
                    score += 2
            
            if '%' in sentence:
                score += 3
            
            word_count = len(sentence.split())
            if 12 <= word_count <= 35:
                score += 2
            elif word_count < 8 or word_count > 50:
                score -= 1
            
            # T√¨m m√£ CK trong c√¢u
            for code in self.all_stock_codes:
                if re.search(r'\b' + code + r'\b', sentence):  # Ch·ªâ match whole word
                    score += 3
                    break
            
            scored_sentences.append((sentence, score, i))
        
        scored_sentences.sort(key=lambda x: x[1], reverse=True)
        top_sentences = scored_sentences[:max_sentences]
        top_sentences.sort(key=lambda x: x[2])
        
        summary = '. '.join([s[0] for s in top_sentences])
        if not summary.endswith('.'):
            summary += '.'
        
        summary = self.clean_text(summary)
        return summary
    
    def is_generic_news(self, title):
        """Ki·ªÉm tra xem c√≥ ph·∫£i tin t·ª©c chung kh√¥ng"""
        title_lower = title.lower()
        
        generic_patterns = [
            r'l·ªãch\s+s·ª±\s+ki·ªán',
            r'tin\s+v·∫Øn',
            r't·ªïng\s+h·ª£p',
            r'ƒëi·ªÉm\s+tin',
            r'nh·ªãp\s+ƒë·∫≠p',
            r'th·ªã\s+tr∆∞·ªùng\s+ng√†y',
            r'ch·ª©ng\s+kho√°n\s+ng√†y',
            r'phi√™n\s+giao\s+d·ªãch',
            r'c√°c\s+tin\s+t·ª©c',
            r'tin\s+nhanh',
            r'c·∫≠p\s+nh·∫≠t',
            r'ƒëi·ªÉm\s+l·∫°i',
        ]
        
        for pattern in generic_patterns:
            if re.search(pattern, title_lower):
                return True
        
        return False
    
    # ============================================================
    # H√ÄM M·ªöI: QU√âT T·∫§T C·∫¢ M√É CK TRONG B√ÄI (KH√îNG UPPER CASE)
    # ============================================================
    def extract_all_stocks_from_article(self, text):
        """
        Qu√©t to√†n b·ªô b√†i vi·∫øt 1 l∆∞·ª£t ƒë·ªÉ t√¨m T·∫§T C·∫¢ m√£ CK xu·∫•t hi·ªán
        CH·ªà NH·∫¨N DI·ªÜN M√É VI·∫æT HOA TRONG B√ÄI G·ªêC (kh√¥ng upper case text)
        
        Returns:
            dict: {
                'all_codes': set(),  # T·∫•t c·∫£ m√£ t√¨m th·∫•y
                'hose_codes': set(), 
                'hnx_codes': set(),
                'upcom_codes': set(),
                'has_hnx_upcom': bool,  # C√≥ HNX/UPCoM kh√¥ng?
                'has_only_hose': bool   # Ch·ªâ c√≥ HOSE kh√¥ng?
            }
        """
        result = {
            'all_codes': set(),
            'hose_codes': set(),
            'hnx_codes': set(),
            'upcom_codes': set(),
            'has_hnx_upcom': False,
            'has_only_hose': False
        }
        
        # KH√îNG upper case text - gi·ªØ nguy√™n ƒë·ªÉ detect ch·ªâ m√£ vi·∫øt hoa
        RISKY_CODES = {'THU', 'TIN', 'TOP', 'HAI', 'LAI', 'CEO', 'CCP'}
        
        # ============================================================
        # PATTERN 1: M√É TRONG NGO·∫∂C V·ªöI S√ÄN
        # ============================================================
        patterns_with_exchange = [
            r'\((?:UPCOM|HNX|HOSE):\s*([A-Z]{3})\)',
            r'\(([A-Z]{3})\s*[-‚Äì]\s*(?:UPCOM|HNX|HOSE)\)',
            r'\(([A-Z]{3})\s*,\s*(?:UPCOM|HNX|HOSE)\)',
        ]
        
        for pattern in patterns_with_exchange:
            for match in re.finditer(pattern, text):
                code = match.group(1)
                if code in self.all_stock_codes:
                    result['all_codes'].add(code)
        
        # ============================================================
        # PATTERN 2: M√É SAU C·ª§M T·ª™ NH·∫¨N DI·ªÜN
        # ============================================================
        signal_patterns = [
            r'(?:c·ªï\s+phi·∫øu|m√£|cp)\s+([A-Z]{3})\b',
            r'\bc√¥ng\s+ty\s+([A-Z]{3})\b',
            r'\b([A-Z]{3})\s+(?:tƒÉng|gi·∫£m|tƒÉng|gi·∫£m)\b',
        ]
        
        for pattern in signal_patterns:
            for match in re.finditer(pattern, text, re.IGNORECASE):
                code = match.group(1).upper()
                if code in self.all_stock_codes:
                    result['all_codes'].add(code)
        
        # ============================================================
        # PATTERN 3: M√É VI·∫æT HOA ƒê·ª®NG ƒê·ªòC L·∫¨P (CH·ªà NH·ªÆNG M√É AN TO√ÄN)
        # ============================================================
        # T√¨m t·∫•t c·∫£ c√°c t·ª´ vi·∫øt hoa 3 ch·ªØ c√°i ƒë·ª©ng ri√™ng
        standalone_pattern = r'\b([A-Z]{3})\b'
        
        for match in re.finditer(standalone_pattern, text):
            code = match.group(1)
            
            # Ch·ªâ nh·∫≠n n·∫øu:
            # 1. L√† m√£ CK h·ª£p l·ªá
            # 2. KH√îNG thu·ªôc nh√≥m nguy hi·ªÉm (tr·ª´ khi c√≥ t√≠n hi·ªáu r√µ r√†ng ·ªü tr√™n)
            if code in self.all_stock_codes:
                if code not in RISKY_CODES:
                    result['all_codes'].add(code)
                elif code in result['all_codes']:  # ƒê√£ t√¨m th·∫•y ·ªü pattern tr√™n
                    pass  # Gi·ªØ l·∫°i
        
        # ============================================================
        # PH√ÇN LO·∫†I THEO S√ÄN
        # ============================================================
        for code in result['all_codes']:
            exchange = self.stock_to_exchange.get(code)
            if exchange == 'HOSE':
                result['hose_codes'].add(code)
            elif exchange == 'HNX':
                result['hnx_codes'].add(code)
            elif exchange == 'UPCoM':
                result['upcom_codes'].add(code)
        
        # ============================================================
        # X√ÅC ƒê·ªäNH ƒêI·ªÄU KI·ªÜN L·ªåC
        # ============================================================
        result['has_hnx_upcom'] = len(result['hnx_codes']) > 0 or len(result['upcom_codes']) > 0
        result['has_only_hose'] = len(result['hose_codes']) > 0 and not result['has_hnx_upcom']
        
        return result
    
    # Gi·ªØ l·∫°i h√†m extract_stock c≈© cho vi·ªác l·∫•y m√£ ch√≠nh c·ªßa b√†i
    def extract_stock(self, text):
        """
        Tr√≠ch xu·∫•t M√É CH√çNH c·ªßa b√†i vi·∫øt (d√πng cho display)
        ∆Øu ti√™n: HNX/UPCoM > HOSE
        """
        stock_analysis = self.extract_all_stocks_from_article(text)
        
        # ∆Øu ti√™n HNX/UPCoM
        if stock_analysis['hnx_codes']:
            code = list(stock_analysis['hnx_codes'])[0]
            return code, 'HNX', 'code'
        elif stock_analysis['upcom_codes']:
            code = list(stock_analysis['upcom_codes'])[0]
            return code, 'UPCoM', 'code'
        elif stock_analysis['hose_codes']:
            code = list(stock_analysis['hose_codes'])[0]
            return code, 'HOSE', 'code'
        
        return None, None, None
    
    def parse_date(self, date_text):
        """Parse date t·ª´ text"""
        try:
            for fmt in [
                '%Y-%m-%dT%H:%M:%S%z',
                '%Y-%m-%d %H:%M:%S',
                '%d/%m/%Y %H:%M',
                '%Y-%m-%d',
                '%d/%m/%Y',
            ]:
                try:
                    dt = datetime.strptime(date_text[:19], fmt[:19])
                    if dt.tzinfo is None:
                        dt = dt.replace(tzinfo=self.vietnam_tz)
                    return dt
                except:
                    continue
            return None
        except:
            return None
    
    def fetch_url(self, url, max_retries=3):
        """Fetch URL v·ªõi retry"""
        for attempt in range(max_retries):
            try:
                response = self.session.get(url, headers=self.headers, timeout=15)
                if response.status_code == 200:
                    return response
            except:
                if attempt == max_retries - 1:
                    return None
                time.sleep(1)
        return None
    
    def fetch_article_content(self, url):
        """Fetch n·ªôi dung chi ti·∫øt b√†i vi·∫øt"""
        try:
            response = self.fetch_url(url)
            if not response:
                return None, None, None
            
            response.encoding = 'utf-8'
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # T√¨m ng√†y th√°ng
            article_date_obj = None
            for pattern in [
                {'itemprop': 'datePublished'},
                {'property': 'article:published_time'},
                {'name': 'pubdate'},
                {'class': re.compile(r'meta.*time', re.I)}
            ]:
                date_elem = soup.find(['time', 'span', 'div', 'meta'], pattern)
                if date_elem:
                    date_text = date_elem.get('datetime') or date_elem.get('content') or date_elem.get_text(strip=True)
                    if date_text:
                        article_date_obj = self.parse_date(date_text)
                        if article_date_obj:
                            break
            
            if not article_date_obj:
                article_date_obj = datetime.now(self.vietnam_tz)
            
            article_date_str = article_date_obj.strftime('%d/%m/%Y %H:%M')
            
            # T√¨m n·ªôi dung
            content = ""
            for selector in [
                ('article', {}),
                ('div', {'class': re.compile(r'content|article|detail|body', re.I)}),
            ]:
                content_div = soup.find(selector[0], selector[1])
                if content_div:
                    paragraphs = content_div.find_all('p')
                    content = ' '.join([p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 50])
                    if content:
                        break
            
            if not content:
                paragraphs = soup.find_all('p')
                valid_p = [p.get_text(strip=True) for p in paragraphs if 50 < len(p.get_text(strip=True)) < 1000]
                content = ' '.join(valid_p[:8])
            
            content = self.clean_text(content)
            return content, article_date_str, article_date_obj
        
        except:
            return None, None, None
    
    def scrape_source(self, url, source_name, pattern, max_articles=20, progress_callback=None):
        try:
            response = self.fetch_url(url)
            if not response:
                return 0
            
            response.encoding = 'utf-8'
            soup = BeautifulSoup(response.text, 'html.parser')
            
            count = 0
            seen = set()
            links = soup.find_all('a', href=True)
            total_links = len(links)
            
            # ============================================================
            # B∆Ø·ªöC 1: C√ÄO TO√ÄN B·ªò B√ÄI VI·∫æT
            # ============================================================
            all_crawled_articles = []
            
            for idx, link_tag in enumerate(links):
                if progress_callback:
                    progress = (idx + 1) / total_links * 0.5
                    progress_callback(f"{source_name} - ƒêang c√†o: {idx+1}/{total_links}", progress)
                
                href = link_tag.get('href', '')
                
                if pattern(href) and href not in seen:
                    title = link_tag.get_text(strip=True)
                    
                    if title and len(title) > 30 and not self.is_generic_news(title):
                        seen.add(href)
                        full_link = urljoin(url, href)
                        
                        content, article_date_str, article_date_obj = self.fetch_article_content(full_link)
                        
                        if content and article_date_obj:
                            if article_date_obj >= self.cutoff_time:
                                all_crawled_articles.append({
                                    'title': title,
                                    'link': full_link,
                                    'date': article_date_str,
                                    'date_obj': article_date_obj,
                                    'content': content
                                })
                            
                            time.sleep(0.3)
                            
                            if len(all_crawled_articles) >= max_articles * 3:
                                break
            
            self.stats['total_crawled'] = len(all_crawled_articles)
            
            # ============================================================
            # B∆Ø·ªöC 2: QU√âT T·∫§T C·∫¢ M√É CK & L·ªåC THEO ƒêI·ªÄU KI·ªÜN
            # ============================================================
            for idx, article in enumerate(all_crawled_articles):
                if progress_callback:
                    progress = 0.5 + (idx + 1) / len(all_crawled_articles) * 0.5
                    progress_callback(f"{source_name} - ƒêang ph√¢n t√≠ch: {idx+1}/{len(all_crawled_articles)}", progress)
                
                # QU√âT TO√ÄN B·ªò B√ÄI 1 L∆Ø·ª¢T
                full_text = article['title'] + " " + article['content']
                stock_analysis = self.extract_all_stocks_from_article(full_text)
                
                # ============================================================
                # ƒêI·ªÄU KI·ªÜN L·ªåC: CH·ªà GI·ªÆ B√ÄI C√ì HNX/UPCoM
                # B·ªé QUA B√ÄI CH·ªà C√ì HOSE
                # ============================================================
                if stock_analysis['has_only_hose']:
                    # B·ªè qua b√†i ch·ªâ c√≥ HOSE
                    self.stats['hose_only_filtered'] += 1
                    continue
                
                if not stock_analysis['has_hnx_upcom']:
                    # Kh√¥ng c√≥ m√£ n√†o ho·∫∑c kh√¥ng c√≥ HNX/UPCoM -> b·ªè qua
                    continue
                
                # ============================================================
                # B√ÄI ƒê·∫†T ƒêI·ªÄU KI·ªÜN -> X·ª¨ L√ù
                # ============================================================
                
                # L·∫•y m√£ ch√≠nh ƒë·ªÉ hi·ªÉn th·ªã (∆∞u ti√™n HNX/UPCoM)
                stock_code, exchange, match_method = self.extract_stock(full_text)
                
                if stock_code:
                    if match_method == 'code':
                        self.stats['found_by_code'] += 1
                    else:
                        self.stats['found_by_name'] += 1
                    
                    company_name = self.code_to_name.get(stock_code, '')
                    
                    # T√ìM T·∫ÆT
                    summary = self.advanced_summarize(article['content'], article['title'], max_sentences=4)
                    
                    # SENTIMENT
                    sentiment_result = self.sentiment_analyzer.analyze_sentiment(article['title'], article['content'])
                    
                    if exchange == 'HNX':
                        self.stats['hnx_found'] += 1
                    elif exchange == 'UPCoM':
                        self.stats['upcom_found'] += 1
                    
                    if sentiment_result['risk_level'] == 'Nghi√™m tr·ªçng':
                        self.stats['severe_risk'] += 1
                    elif sentiment_result['risk_level'] == 'C·∫£nh b√°o':
                        self.stats['warning_risk'] += 1
                    
                    # T·∫°o danh s√°ch t·∫•t c·∫£ m√£ t√¨m th·∫•y
                    all_codes_str = ', '.join(sorted(stock_analysis['all_codes']))
                    
                    self.all_articles.append({
                        'Ti√™u ƒë·ªÅ': article['title'],
                        'Link': article['link'],
                        'Ng√†y': article['date'],
                        'M√£ CK ch√≠nh': stock_code,
                        'T√™n c√¥ng ty': company_name,
                        'S√†n': exchange,
                        'T·∫•t c·∫£ m√£': all_codes_str,  # TH√äM TR∆Ø·ªúNG M·ªöI
                        'Sentiment': sentiment_result['sentiment_label'],
                        'ƒêi·ªÉm': sentiment_result['sentiment_score'],
                        'Risk': sentiment_result['risk_level'],
                        'Vi ph·∫°m': sentiment_result['violations'],
                        'Keywords': "; ".join([k['keyword'] for k in sentiment_result['keywords'][:3]]),
                        'N·ªôi dung t√≥m t·∫Øt': summary,
                        'T√¨m theo': 'M√£ CK' if match_method == 'code' else 'T√™n c√¥ng ty'
                    })
                    
                    count += 1
                    
                    if count >= max_articles:
                        break
            
            return count
        
        except Exception as e:
            st.error(f"L·ªói {source_name}: {str(e)}")
            return 0
    
    def run(self, max_articles_per_source=20, progress_callback=None):
        sources = [
            ("https://cafef.vn/thi-truong-chung-khoan.chn", "CafeF", lambda h: '.chn' in h),
            ("https://vietstock.vn/chung-khoan.htm", "VietStock", lambda h: re.search(r'/\d{4}/\d{2}/.+\.htm', h)),
            ("https://nguoiquansat.vn/chung-khoan", "Ng∆∞·ªùi Quan S√°t", lambda h: '/chung-khoan/' in h and h.startswith('/')),
            ("https://baomoi.com/chung-khoan.epi", "B√°o M·ªõi", lambda h: h.startswith('/') and any(x in h for x in ['.epi', '-c111'])),
            ("https://www.tinnhanhchungkhoan.vn/chung-khoan/", "Tin Nhanh CK (CK)", lambda h: '/chung-khoan/' in h or '/doanh-nghiep/' in h),
            ("https://www.tinnhanhchungkhoan.vn/doanh-nghiep/", "Tin Nhanh CK (DN)", lambda h: '/doanh-nghiep/' in h or '/chung-khoan/' in h),
        ]
        
        for url, name, pattern in sources:
            self.scrape_source(url, name, pattern, max_articles_per_source, progress_callback)
            time.sleep(1)
        
        if len(self.all_articles) == 0:
            return None
        
        df = pd.DataFrame(self.all_articles)
        df = df.drop_duplicates(subset=['Ti√™u ƒë·ªÅ'], keep='first')
        df.insert(0, 'STT', range(1, len(df) + 1))
        
        return df
# ============================================================
# STREAMLIT APP
# ============================================================

def main():
    st.markdown('<div class="main-header">üìà TOOL THU TH·∫¨P TIN ƒê·ªíN 2.0</div>', unsafe_allow_html=True)
    st.markdown('<div style="text-align:center;color:#666;margin-bottom:2rem;">HNX & UPCoM </div>', unsafe_allow_html=True)
    
    # Sidebar
    with st.sidebar:
        st.header("‚öôÔ∏è C√ÄI ƒê·∫∂T")
        
        st.subheader("üìÇ DANH S√ÅCH M√É CK")
        st.markdown('<div class="upload-box">', unsafe_allow_html=True)
        st.write("**Upload file Excel/CSV**")
        st.caption("G·ªìm 3 c·ªôt: M√£ CK | S√†n | T√™n c√¥ng ty")
        
        uploaded_file = st.file_uploader(
            "Ch·ªçn file",
            type=['xlsx', 'xls', 'csv'],
            help="File ph·∫£i c√≥ c√°c c·ªôt: M√£ CK, S√†n (HNX/UPCoM), T√™n c√¥ng ty"
        )
        
        sample_excel = create_sample_excel()
        st.download_button(
            label="üì• T·∫£i file m·∫´u",
            data=sample_excel,
            file_name="mau_danh_sach_ma_ck.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )
        st.markdown('</div>', unsafe_allow_html=True)
        
        if uploaded_file is not None:
            stock_df, error = parse_stock_file(uploaded_file)
            
            if error:
                st.error(f"‚ùå {error}")
                st.session_state['stock_df'] = load_default_stock_list()
            else:
                st.success(f"‚úÖ ƒê√£ load {len(stock_df)} m√£ CK")
                st.session_state['stock_df'] = stock_df
                
                hnx_count = len(stock_df[stock_df['S√†n'] == 'HNX'])
                upcom_count = len(stock_df[stock_df['S√†n'] == 'UPCoM'])
                st.info(f"HNX: {hnx_count} | UPCoM: {upcom_count}")
        else:
            if 'stock_df' not in st.session_state:
                st.session_state['stock_df'] = load_default_stock_list()
                st.warning("‚ö†Ô∏è ƒêang d√πng danh s√°ch m·∫∑c ƒë·ªãnh")
        
        st.markdown("---")
        st.subheader("üîß T√ôY CH·ªàNH")
        
        time_filter = st.selectbox(
            "‚è∞ Kho·∫£ng th·ªùi gian",
            options=[6, 12, 24, 48, 72, 168],
            format_func=lambda x: f"{x} gi·ªù" if x < 168 else "1 tu·∫ßn",
            index=2
        )
        
        max_articles = st.slider(
            "üìä S·ªë b√†i t·ªëi ƒëa/ngu·ªìn",
            min_value=5,
            max_value=50,
            value=20,
            step=5
        )
        
        st.markdown("---")
        st.info("üí° **H∆∞·ªõng d·∫´n:**\n1. Upload danh s√°ch m√£\n2. Ch·ªçn th·ªùi gian\n3. B·∫•m 'B·∫Øt ƒë·∫ßu'\n4. Download Excel")
    
    # Main content
    if st.button("üöÄ B·∫ÆT ƒê·∫¶U C√ÄO TIN", type="primary"):
        stock_df = st.session_state.get('stock_df')
        
        if stock_df is None or len(stock_df) == 0:
            st.error("‚ùå Ch∆∞a c√≥ danh s√°ch m√£ CK! Vui l√≤ng upload file.")
            return
        
        with st.spinner("ƒêang c√†o tin..."):
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            def update_progress(message, progress):
                status_text.text(message)
                progress_bar.progress(progress)
            
            scraper = StockScraperWeb(stock_df, time_filter_hours=time_filter)
            df = scraper.run(max_articles_per_source=max_articles, progress_callback=update_progress)
            
            progress_bar.empty()
            status_text.empty()
            
            if df is not None:
                st.success(f"‚úÖ Ho√†n t·∫•t! T√¨m th·∫•y {len(df)} b√†i vi·∫øt")
                st.info(f"üîç T√¨m theo m√£ CK: {scraper.stats['found_by_code']} | T√¨m theo t√™n: {scraper.stats['found_by_name']}")
                
                st.session_state['df'] = df
                st.session_state['stats'] = scraper.stats
            else:
                st.error("Kh√¥ng t√¨m th·∫•y b√†i vi·∫øt n√†o!")
    
    # Display results
    if 'df' in st.session_state:
        df = st.session_state['df']
        stats = st.session_state['stats']
        
        # Metrics
        col1, col2, col3, col4, col5 = st.columns(5)
        with col1:
            st.metric("üìä T·ªïng b√†i", len(df))
        with col2:
            st.metric("‚ö†Ô∏è Nghi√™m tr·ªçng", stats['severe_risk'])
        with col3:
            st.metric("‚ö†Ô∏è C·∫£nh b√°o", stats['warning_risk'])
        with col4:
            st.metric("üî§ T√¨m theo m√£", stats['found_by_code'])
        with col5:
            st.metric("üìù T√¨m theo t√™n", stats['found_by_name'])
        
        # Download button
        buffer = io.BytesIO()
        with pd.ExcelWriter(buffer, engine='openpyxl') as writer:
            df.to_excel(writer, index=False, sheet_name='T·∫•t c·∫£')
            
            df_severe = df[df['Risk'] == 'Nghi√™m tr·ªçng']
            if len(df_severe) > 0:
                df_severe.to_excel(writer, index=False, sheet_name='Nghi√™m tr·ªçng')
            
            df_warning = df[df['Risk'] == 'C·∫£nh b√°o']
            if len(df_warning) > 0:
                df_warning.to_excel(writer, index=False, sheet_name='C·∫£nh b√°o')
        
        st.download_button(
            label="‚¨áÔ∏è Download Excel",
            data=buffer.getvalue(),
            file_name=f"Tin_CK_{datetime.now().strftime('%d%m%Y_%H%M')}.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )
        
        st.markdown("---")
        
        # Filters
        st.subheader("üîç L·ªåC & T√åM KI·∫æM")
        
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            search_code = st.text_input("M√£ CK", placeholder="VD: SHS")
        with col2:
            filter_san = st.selectbox("S√†n", ["T·∫•t c·∫£", "HNX", "UPCoM"])
        with col3:
            filter_risk = st.selectbox("Risk Level", ["T·∫•t c·∫£", "Nghi√™m tr·ªçng", "C·∫£nh b√°o", "B√¨nh th∆∞·ªùng", "T√≠ch c·ª±c"])
        with col4:
            filter_method = st.selectbox("T√¨m theo", ["T·∫•t c·∫£", "M√£ CK", "T√™n c√¥ng ty"])
        
        # Apply filters
        df_filtered = df.copy()
        
        if search_code:
            df_filtered = df_filtered[
                df_filtered['M√£ CK'].str.contains(search_code.upper(), case=False, na=False) |
                df_filtered['T√™n c√¥ng ty'].str.contains(search_code, case=False, na=False)
            ]
        
        if filter_san != "T·∫•t c·∫£":
            df_filtered = df_filtered[df_filtered['S√†n'] == filter_san]
        
        if filter_risk != "T·∫•t c·∫£":
            df_filtered = df_filtered[df_filtered['Risk'] == filter_risk]
        
        if filter_method != "T·∫•t c·∫£":
            df_filtered = df_filtered[df_filtered['T√¨m theo'] == filter_method]
        
        st.info(f"Hi·ªÉn th·ªã {len(df_filtered)} / {len(df)} b√†i")
        
        # Display articles
        st.subheader("üì∞ DANH S√ÅCH B√ÄI VI·∫æT")
        
        for idx, row in df_filtered.iterrows():
            if row['Risk'] == 'Nghi√™m tr·ªçng':
                card_class = "severe-card"
                icon = "‚ö†Ô∏è"
            elif row['Risk'] == 'C·∫£nh b√°o':
                card_class = "warning-card"
                icon = "‚ö†Ô∏è"
            elif row['Risk'] == 'T√≠ch c·ª±c':
                card_class = "positive-card"
                icon = "‚úÖ"
            else:
                card_class = "metric-card"
                icon = "üìÑ"
            
            with st.container():
                st.markdown(f'<div class="{card_class}">', unsafe_allow_html=True)
                
                col1, col2 = st.columns([4, 1])
                
                with col1:
                    if row['T√™n c√¥ng ty']:
                        st.markdown(f"**{icon} {row['M√£ CK']} - {row['T√™n c√¥ng ty']} ({row['S√†n']})**")
                    else:
                        st.markdown(f"**{icon} {row['M√£ CK']} ({row['S√†n']})**")
                    
                    st.markdown(f"{row['Ti√™u ƒë·ªÅ']}")
                    
                    caption_text = f"üìÖ {row['Ng√†y']} | "
                    caption_text += f"Sentiment: {row['Sentiment']} ({row['ƒêi·ªÉm']}) | "
                    caption_text += f"Risk: {row['Risk']} | "
                    caption_text += f"üîç {row['T√¨m theo']}"
                    
                    if row['Vi ph·∫°m']:
                        caption_text += f" | ‚öñÔ∏è {row['Vi ph·∫°m']}"
                    
                    st.caption(caption_text)
                
                with col2:
                    if st.button("üîó Xem", key=f"view_{idx}"):
                        st.markdown(f"[M·ªü b√†i vi·∫øt]({row['Link']})")
                
                # HI·ªÇN TH·ªä T√ìM T·∫ÆT
                if pd.notna(row['N·ªôi dung t√≥m t·∫Øt']) and row['N·ªôi dung t√≥m t·∫Øt']:
                    with st.expander("üìù Xem t√≥m t·∫Øt"):
                        st.write(row['N·ªôi dung t√≥m t·∫Øt'])
                
                if row['Keywords']:
                    st.info(f"üîë Keywords: {row['Keywords']}")
                
                st.markdown('</div>', unsafe_allow_html=True)
                st.markdown("<br>", unsafe_allow_html=True)
        
        # Dashboard
        st.markdown("---")
        st.subheader("üìä DASHBOARD")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.write("**Ph√¢n b·ªë Sentiment**")
            sentiment_counts = df['Sentiment'].value_counts()
            st.bar_chart(sentiment_counts)
        
        with col2:
            st.write("**Ph√¢n b·ªë Risk Level**")
            risk_counts = df['Risk'].value_counts()
            st.bar_chart(risk_counts)
        
        col3, col4 = st.columns(2)
        
        with col3:
            st.write("**Top 10 M√£ CK**")
            top_ma = df['M√£ CK'].value_counts().head(10)
            st.bar_chart(top_ma)
        
        with col4:
            st.write("**Ph√¢n b·ªë theo S√†n**")
            san_counts = df['S√†n'].value_counts()
            st.bar_chart(san_counts)
        
        # Chi ti·∫øt theo m√£
        st.markdown("---")
        st.subheader("üìà CHI TI·∫æT THEO M√É CK")
        
        with st.expander("Xem chi ti·∫øt"):
            summary = df.groupby('M√£ CK').agg({
                'Ti√™u ƒë·ªÅ': 'count',
                'ƒêi·ªÉm': 'mean',
                'Risk': lambda x: x.mode()[0] if len(x) > 0 else 'N/A'
            }).rename(columns={
                'Ti√™u ƒë·ªÅ': 'S·ªë b√†i',
                'ƒêi·ªÉm': 'Sentiment TB',
                'Risk': 'Risk ch√≠nh'
            }).reset_index()
            
            summary = summary.merge(
                df[['M√£ CK', 'T√™n c√¥ng ty', 'S√†n']].drop_duplicates(),
                on='M√£ CK',
                how='left'
            )
            
            summary['Sentiment TB'] = summary['Sentiment TB'].round(1)
            summary = summary.sort_values('S·ªë b√†i', ascending=False)
            
            st.dataframe(
                summary,
                use_container_width=True,
                hide_index=True
            )

if __name__ == "__main__":
    main()
